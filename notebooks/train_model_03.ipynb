{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8cc119a1-28bd-47af-8a5d-38204c005bdf",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Imports & Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ba165936-c2d9-4dd5-b388-62912b63f069",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-09-23 02:00:22,535] WARNING model_trainer: dataset_config.yaml NON trovato: uso fallback di default.\n",
            "[2025-09-23 02:00:22,547] INFO model_trainer: Setup OK | seed=42 | outputs_dir=c:/Users/Utente/Desktop/Projects/ai_oracle_rwa/notebooks/outputs\n"
          ]
        }
      ],
      "source": [
        "# 00) Import & setup — esecuzione da dentro `notebooks/`\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Siamo già in notebooks/, quindi `shared/` è un pacchetto sibling\n",
        "NB_ROOT = Path.cwd()                 # .../notebooks\n",
        "PROJ_ROOT = NB_ROOT.parent           # project root\n",
        "\n",
        "if str(NB_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(NB_ROOT))\n",
        "if str(PROJ_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJ_ROOT))\n",
        "\n",
        "# ── shared imports (coerenti con i tuoi notebook)\n",
        "from shared.common.utils import (\n",
        "    NumpyJSONEncoder,\n",
        "    optimize_dtypes,\n",
        "    log_basic_diagnostics,\n",
        "    set_global_seed,\n",
        ")\n",
        "from shared.common.config import load_config, configure_logger\n",
        "from shared.common.constants import (\n",
        "    VALUATION_K,\n",
        "    LAST_VERIFIED_TS, PREDICTION_TS, LAG_HOURS,\n",
        "    CONDITION_SCORE, RISK_SCORE,\n",
        ")\n",
        "\n",
        "# sklearn (alcuni import usati in celle successive)\n",
        "from sklearn.ensemble import RandomForestRegressor  # noqa: F401\n",
        "\n",
        "# ── Logger\n",
        "LOG_LEVEL = os.getenv(\"NB_LOG_LEVEL\", \"INFO\")\n",
        "logger = configure_logger(name=\"model_trainer\", level=LOG_LEVEL)\n",
        "\n",
        "# ── Config (opzionale): se non esiste, lavoriamo in fallback senza YAML\n",
        "CFG_PATH = NB_ROOT / \"dataset_config.yaml\"\n",
        "if CFG_PATH.exists():\n",
        "    CONFIG = load_config(str(CFG_PATH))\n",
        "    logger.info(\"Config YAML caricato: %s\", CFG_PATH.as_posix())\n",
        "else:\n",
        "    CONFIG = {}\n",
        "    logger.warning(\"dataset_config.yaml NON trovato: uso fallback di default.\")\n",
        "\n",
        "TRAIN_CFG = CONFIG.get(\"training\", {}) or {}\n",
        "\n",
        "# ── Seed globale\n",
        "SEED = int(TRAIN_CFG.get(\"seed\", CONFIG.get(\"seed\", 42)))\n",
        "set_global_seed(SEED)\n",
        "\n",
        "# ── Cartelle output (da usare SEMPRE relative a `notebooks/`)\n",
        "BASE_OUT = NB_ROOT / \"outputs\"                  # <- 👈 corretto: siamo già in notebooks/\n",
        "MODEL_DIR = BASE_OUT / \"modeling\"\n",
        "FIG_DIR   = MODEL_DIR / \"figures\"\n",
        "ART_DIR   = MODEL_DIR / \"artifacts\"\n",
        "PROP_DIR  = MODEL_DIR / \"property\"              # cartella “servita” dal backend/registry\n",
        "\n",
        "for d in (BASE_OUT, MODEL_DIR, FIG_DIR, ART_DIR, PROP_DIR):\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ── Dataset path canonico (override da YAML se presente)\n",
        "DATASET_PATH = Path(TRAIN_CFG.get(\"dataset_path\", BASE_OUT / \"dataset_generated.csv\"))\n",
        "\n",
        "# QoL\n",
        "pd.set_option(\"display.max_columns\", 200)\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "logger.info(\"Setup OK | seed=%s | outputs_dir=%s\", SEED, BASE_OUT.as_posix())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c0f3f96-b7be-48ff-a9b2-2c9b9569cb63",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "38892f35-5a7d-488c-83d2-a99bca1e6fee",
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-09-23 02:00:22,589] INFO model_trainer: Manifest nb01 trovato: c:/Users/Utente/Desktop/Projects/ai_oracle_rwa/notebooks/outputs/snapshots/manifest_20250922T185630Z.json\n",
            "[2025-09-23 02:00:22,592] INFO model_trainer: 📄 Caricamento dataset da: C:/Users/Utente/Desktop/Projects/ai_oracle_rwa/notebooks/outputs/dataset_generated.csv\n",
            "[2025-09-23 02:00:23,049] INFO model_trainer: ✅ Dtypes optimized: 14.95 MB → 12.81 MB (−2.15 MB, 14.4%)\n",
            "[2025-09-23 02:00:23,051] INFO model_trainer: [UTILS] Distribution by location:\n",
            "location\n",
            "Milan       3017\n",
            "Rome        2700\n",
            "Turin       1214\n",
            "Naples      1190\n",
            "Bologna      886\n",
            "Genoa        773\n",
            "Florence     770\n",
            "Palermo      767\n",
            "Venice       596\n",
            "Bari         593\n",
            "Verona       591\n",
            "Padua        581\n",
            "Catania      444\n",
            "Cagliari     442\n",
            "Trieste      436\n",
            "[2025-09-23 02:00:23,052] INFO model_trainer: [UTILS] Valuation min: 54.23k€\n",
            "[2025-09-23 02:00:23,055] INFO model_trainer: [UTILS] Valuation max: 2427.28k€\n",
            "[2025-09-23 02:00:23,057] INFO model_trainer: [UTILS] Valuation mean: 493.76k€\n",
            "[2025-09-23 02:00:23,470] INFO model_trainer: [UTILS] Corr(size_m2, valuation_k): 0.634\n",
            "[2025-09-23 02:00:23,522] INFO model_trainer: ✅ Schema validation passed\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "895"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 01) Carica dataset dal manifest di nb01 (robusto) + ottimizza + valida\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from shared.common.utils import canonical_json_dumps\n",
        "from shared.common.sanity_checks import validate_dataset\n",
        "\n",
        "# --- helper: risolvi path relativo verso posizioni note ---\n",
        "def _resolve_path(p: str | Path) -> Path | None:\n",
        "    cand = Path(p)\n",
        "    if cand.exists():\n",
        "        return cand\n",
        "    # se è relativo, prova sotto base output e progetto\n",
        "    for base in [BASE_OUT, NB_ROOT, PROJ_ROOT]:\n",
        "        q = (base / str(p)).resolve()\n",
        "        if q.exists():\n",
        "            return q\n",
        "    return None\n",
        "\n",
        "# 1) Trova ultimo manifest di nb01\n",
        "snap_dir = BASE_OUT / \"snapshots\"\n",
        "snap_dir.mkdir(parents=True, exist_ok=True)\n",
        "manifests = sorted(snap_dir.glob(\"manifest_*.json\"))\n",
        "manifest01 = None\n",
        "if manifests:\n",
        "    try:\n",
        "        manifest01 = json.loads(manifests[-1].read_text(encoding=\"utf-8\"))\n",
        "        logger.info(\"Manifest nb01 trovato: %s\", manifests[-1].as_posix())\n",
        "    except Exception as e:\n",
        "        logger.warning(\"Impossibile leggere il manifest più recente: %s\", e)\n",
        "\n",
        "# 2) Determina data_path dal manifest (supporta varie chiavi)\n",
        "data_path: Path | None = None\n",
        "if isinstance(manifest01, dict):\n",
        "    paths = (manifest01.get(\"paths\") or {})  # type: ignore\n",
        "    for k in (\"dataset_path\", \"dataset\", \"output_path\"):\n",
        "        p = paths.get(k)\n",
        "        if p:\n",
        "            rp = _resolve_path(p)\n",
        "            if rp:\n",
        "                data_path = rp\n",
        "                break\n",
        "\n",
        "# 3) Fallback: usa DATASET_PATH (da Cella 01) o cerca in BASE_OUT\n",
        "if data_path is None or not data_path.exists():\n",
        "    candidates = [\n",
        "        Path(DATASET_PATH) if isinstance(DATASET_PATH, (str, Path)) else None,\n",
        "        BASE_OUT / \"dataset_generated.parquet\",\n",
        "        BASE_OUT / \"dataset_generated.csv\",\n",
        "    ]\n",
        "    # estendi con eventuali file simili\n",
        "    candidates += sorted(BASE_OUT.glob(\"dataset_*.parquet\"))\n",
        "    candidates += sorted(BASE_OUT.glob(\"dataset_*.csv\"))\n",
        "    data_path = next((c for c in candidates if c and c.exists()), None)\n",
        "\n",
        "if not data_path or not data_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        \"Dataset non trovato. Verifica manifest nb01 in notebooks/outputs/snapshots \"\n",
        "        \"oppure che esista notebooks/outputs/dataset_generated.(csv|parquet).\"\n",
        "    )\n",
        "\n",
        "logger.info(\"📄 Caricamento dataset da: %s\", data_path.as_posix())\n",
        "\n",
        "# 4) Caricamento parquet/csv\n",
        "if data_path.suffix.lower() in {\".parquet\", \".pq\"}:\n",
        "    df = pd.read_parquet(data_path)\n",
        "else:\n",
        "    df = pd.read_csv(data_path)\n",
        "\n",
        "# 5) Ottimizzazione dtypes (log risparmio)\n",
        "mem_before = df.memory_usage(deep=True).sum() / 1024**2\n",
        "df = optimize_dtypes(df)\n",
        "mem_after = df.memory_usage(deep=True).sum() / 1024**2\n",
        "logger.info(\n",
        "    \"✅ Dtypes optimized: %.2f MB → %.2f MB (−%.2f MB, %.1f%%)\",\n",
        "    mem_before, mem_after, mem_before - mem_after,\n",
        "    0.0 if mem_before == 0 else (mem_before - mem_after) / mem_before * 100.0\n",
        ")\n",
        "\n",
        "# 6) Diagnostica rapida\n",
        "log_basic_diagnostics(df, logger)\n",
        "\n",
        "# 7) Validazione schema (asset_type da config, come nb01)\n",
        "asset_type = str(CONFIG.get(\"generation\", {}).get(\"asset_type\", \"property\"))\n",
        "try:\n",
        "    val_report = validate_dataset(df, asset_type=asset_type, raise_on_failure=True)\n",
        "    logger.info(\"✅ Schema validation passed\")\n",
        "except Exception as e:\n",
        "    logger.warning(\"Schema validation warning: %s\", e)\n",
        "    val_report = {\"overall_passed\": False, \"error\": str(e)}\n",
        "\n",
        "# 8) Persistenza report vicino ai modeling outputs\n",
        "(MODEL_DIR / \"validation_nb03.json\").write_text(\n",
        "    canonical_json_dumps(val_report),\n",
        "    encoding=\"utf-8\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f92a3f27",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-09-23 02:00:23,562] WARNING model_trainer: 🔴 RIMOZIONE FEATURES LEAKY: ['price_per_sqm', 'strongly_incoherent']\n",
            "[2025-09-23 02:00:23,570] INFO model_trainer: ✅ Dataset pulito: 43 colonne rimanenti\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape dopo pulizia: (15000, 43)\n",
            "Colonne numeriche: ['valuation_k', 'listing_month', 'size_m2', 'rooms', 'bathrooms', 'year_built', 'age_years', 'floor', 'building_floors', 'is_top_floor', 'is_ground_floor', 'has_elevator', 'has_garden', 'has_balcony', 'garage', 'owner_occupied', 'public_transport_nearby', 'distance_to_center_km', 'parking_spot', 'cellar', 'attic', 'concierge', 'humidity_level', 'temperature_avg', 'noise_level', 'air_quality_index', 'condition_score', 'risk_score', 'luxury_score', 'env_score', 'confidence_score']\n"
          ]
        }
      ],
      "source": [
        "# 02) PULIZIA LEAKAGE IMMEDIATA (robusta)\n",
        "from __future__ import annotations\n",
        "\n",
        "import re\n",
        "\n",
        "from shared.common.constants import VALUATION_K\n",
        "from shared.common.constants import PRICE_PER_SQM, PRICE_PER_SQM_CAPPED_VIOLATED\n",
        "from shared.n03_train_model.preprocessing import ML_LEAKY_FEATURES as _ML_LEAKY\n",
        "\n",
        "# --- 1) Rimozione esplicita (case-insensitive) ---\n",
        "explicit_leaky = {\n",
        "    PRICE_PER_SQM,\n",
        "    \"price_per_sqm\",\n",
        "    \"price_per_sqm_vs_region_avg\",\n",
        "    \"price_per_sqm_capped\",\n",
        "    \"valuation_k_log\",\n",
        "    PRICE_PER_SQM_CAPPED_VIOLATED,\n",
        "    \"strongly_incoherent\",\n",
        "    \"valuation_k_decile\",\n",
        "    \"valuation_rank\",\n",
        "    \"is_top_valuation\",\n",
        "}\n",
        "explicit_leaky |= set(map(str, _ML_LEAKY))\n",
        "\n",
        "# mappa lowercase -> originale\n",
        "lower_map = {c.lower(): c for c in df.columns}\n",
        "present_explicit = [lower_map[n.lower()] for n in explicit_leaky if n and n.lower() in lower_map]\n",
        "\n",
        "# --- 2) Rimozione pattern-based (regex, case-insensitive) ---\n",
        "regex_patterns = [\n",
        "    r\"price_per_sqm\",       # qualunque col contenga price_per_sqm\n",
        "    r\"^valuation_k_.+$\",    # derivate del target\n",
        "]\n",
        "present_regex = []\n",
        "for col in df.columns:\n",
        "    if col == VALUATION_K:\n",
        "        continue\n",
        "    if any(re.search(pat, col, flags=re.IGNORECASE) for pat in regex_patterns):\n",
        "        present_regex.append(col)\n",
        "\n",
        "# --- 3) Applica rimozione ---\n",
        "to_drop = sorted(set(present_explicit) | set(present_regex))\n",
        "if to_drop:\n",
        "    logger.warning(\"🔴 RIMOZIONE FEATURES LEAKY: %s\", to_drop)\n",
        "    df.drop(columns=to_drop, inplace=True, errors=\"ignore\")\n",
        "    logger.info(\"✅ Dataset pulito: %d colonne rimanenti\", df.shape[1])\n",
        "else:\n",
        "    logger.info(\"✅ Nessuna feature leaky trovata nel dataset\")\n",
        "\n",
        "# --- 4) Verifiche finali ---\n",
        "assert not any(\"price_per_sqm\" in c.lower() for c in df.columns), \"ERRORE: colonne 'price_per_sqm*' ancora presenti!\"\n",
        "assert not any(c.lower().startswith(\"valuation_k_\") for c in df.columns if c.lower() != VALUATION_K.lower()), \\\n",
        "       \"ERRORE: derivate 'valuation_k_*' ancora presenti!\"\n",
        "\n",
        "# Debug essenziale\n",
        "logger.debug(\"Colonne rimanenti: %s\", list(df.columns))\n",
        "print(f\"Shape dopo pulizia: {df.shape}\")\n",
        "print(f\"Colonne numeriche: {df.select_dtypes(include='number').columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c08c1d08-0b0e-4ed0-bbd9-94c27b0909e3",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Preparation (derivations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8e773326-72f1-480d-8473-b685af965df1",
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-09-23 02:00:23,601] WARNING model_trainer: Impossibile derivare lag_hours: mancano last_verified_ts o prediction_ts\n",
            "[2025-09-23 02:00:23,623] INFO model_trainer: Creato feature derivata: condition_minus_risk\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "DATASET PULITO - PRIME 3 RIGHE\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>asset_id</th>\n",
              "      <th>asset_type</th>\n",
              "      <th>location</th>\n",
              "      <th>valuation_k</th>\n",
              "      <th>last_verified_ts</th>\n",
              "      <th>listing_month</th>\n",
              "      <th>region</th>\n",
              "      <th>urban_type</th>\n",
              "      <th>zone</th>\n",
              "      <th>size_m2</th>\n",
              "      <th>rooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>year_built</th>\n",
              "      <th>age_years</th>\n",
              "      <th>floor</th>\n",
              "      <th>building_floors</th>\n",
              "      <th>is_top_floor</th>\n",
              "      <th>is_ground_floor</th>\n",
              "      <th>has_elevator</th>\n",
              "      <th>has_garden</th>\n",
              "      <th>has_balcony</th>\n",
              "      <th>garage</th>\n",
              "      <th>owner_occupied</th>\n",
              "      <th>public_transport_nearby</th>\n",
              "      <th>distance_to_center_km</th>\n",
              "      <th>parking_spot</th>\n",
              "      <th>cellar</th>\n",
              "      <th>attic</th>\n",
              "      <th>concierge</th>\n",
              "      <th>energy_class</th>\n",
              "      <th>humidity_level</th>\n",
              "      <th>temperature_avg</th>\n",
              "      <th>noise_level</th>\n",
              "      <th>air_quality_index</th>\n",
              "      <th>condition_score</th>\n",
              "      <th>risk_score</th>\n",
              "      <th>luxury_score</th>\n",
              "      <th>env_score</th>\n",
              "      <th>orientation</th>\n",
              "      <th>view</th>\n",
              "      <th>condition</th>\n",
              "      <th>heating</th>\n",
              "      <th>confidence_score</th>\n",
              "      <th>condition_minus_risk</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>asset_000000</td>\n",
              "      <td>property</td>\n",
              "      <td>Venice</td>\n",
              "      <td>254.320007</td>\n",
              "      <td>2025-09-22 18:55:01+00:00</td>\n",
              "      <td>9</td>\n",
              "      <td>north</td>\n",
              "      <td>urban</td>\n",
              "      <td>periphery</td>\n",
              "      <td>77</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1975</td>\n",
              "      <td>50</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5.98</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>C</td>\n",
              "      <td>36.099998</td>\n",
              "      <td>16.0</td>\n",
              "      <td>46</td>\n",
              "      <td>87</td>\n",
              "      <td>0.845</td>\n",
              "      <td>0.112</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.6</td>\n",
              "      <td>North-West</td>\n",
              "      <td>inner courtyard</td>\n",
              "      <td>renovated</td>\n",
              "      <td>autonomous</td>\n",
              "      <td>0.6625</td>\n",
              "      <td>0.733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>asset_000001</td>\n",
              "      <td>property</td>\n",
              "      <td>Turin</td>\n",
              "      <td>970.830017</td>\n",
              "      <td>2025-09-22 18:55:01+00:00</td>\n",
              "      <td>9</td>\n",
              "      <td>north</td>\n",
              "      <td>urban</td>\n",
              "      <td>center</td>\n",
              "      <td>170</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2014</td>\n",
              "      <td>11</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>E</td>\n",
              "      <td>49.000000</td>\n",
              "      <td>14.3</td>\n",
              "      <td>79</td>\n",
              "      <td>55</td>\n",
              "      <td>0.801</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.7</td>\n",
              "      <td>South-East</td>\n",
              "      <td>street</td>\n",
              "      <td>renovated</td>\n",
              "      <td>autonomous</td>\n",
              "      <td>0.7205</td>\n",
              "      <td>0.622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>asset_000002</td>\n",
              "      <td>property</td>\n",
              "      <td>Bari</td>\n",
              "      <td>378.429993</td>\n",
              "      <td>2025-09-22 18:55:01+00:00</td>\n",
              "      <td>9</td>\n",
              "      <td>south</td>\n",
              "      <td>urban</td>\n",
              "      <td>center</td>\n",
              "      <td>66</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2015</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>A</td>\n",
              "      <td>64.800003</td>\n",
              "      <td>22.4</td>\n",
              "      <td>71</td>\n",
              "      <td>73</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.4</td>\n",
              "      <td>South-East</td>\n",
              "      <td>street</td>\n",
              "      <td>new</td>\n",
              "      <td>heat pump</td>\n",
              "      <td>0.5650</td>\n",
              "      <td>0.7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       asset_id asset_type location  valuation_k           last_verified_ts  \\\n",
              "0  asset_000000   property   Venice   254.320007  2025-09-22 18:55:01+00:00   \n",
              "1  asset_000001   property    Turin   970.830017  2025-09-22 18:55:01+00:00   \n",
              "2  asset_000002   property     Bari   378.429993  2025-09-22 18:55:01+00:00   \n",
              "\n",
              "   listing_month region urban_type       zone  size_m2  rooms  bathrooms  \\\n",
              "0              9  north      urban  periphery       77      3          1   \n",
              "1              9  north      urban     center      170      3          2   \n",
              "2              9  south      urban     center       66      2          1   \n",
              "\n",
              "   year_built  age_years  floor  building_floors  is_top_floor  \\\n",
              "0        1975         50      3                9             0   \n",
              "1        2014         11      4                7             0   \n",
              "2        2015         10      1                8             0   \n",
              "\n",
              "   is_ground_floor  has_elevator  has_garden  has_balcony  garage  \\\n",
              "0                0             1           0            1       1   \n",
              "1                0             1           1            1       0   \n",
              "2                0             1           0            0       0   \n",
              "\n",
              "   owner_occupied  public_transport_nearby  distance_to_center_km  \\\n",
              "0               0                        1                   5.98   \n",
              "1               1                        0                   1.40   \n",
              "2               1                        1                   0.74   \n",
              "\n",
              "   parking_spot  cellar  attic  concierge energy_class  humidity_level  \\\n",
              "0             0       0      0          0            C       36.099998   \n",
              "1             1       0      0          0            E       49.000000   \n",
              "2             0       0      0          0            A       64.800003   \n",
              "\n",
              "   temperature_avg  noise_level  air_quality_index  condition_score  \\\n",
              "0             16.0           46                 87            0.845   \n",
              "1             14.3           79                 55            0.801   \n",
              "2             22.4           71                 73             0.85   \n",
              "\n",
              "   risk_score  luxury_score  env_score orientation             view  \\\n",
              "0       0.112           0.4        0.6  North-West  inner courtyard   \n",
              "1       0.179           0.6        0.7  South-East           street   \n",
              "2        0.15           0.2        0.4  South-East           street   \n",
              "\n",
              "   condition     heating  confidence_score  condition_minus_risk  \n",
              "0  renovated  autonomous            0.6625                 0.733  \n",
              "1  renovated  autonomous            0.7205                 0.622  \n",
              "2        new   heat pump            0.5650                   0.7  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Shape: (15000, 44)\n",
            "Target (valuation_k) range: [54.23, 2427.28]\n"
          ]
        }
      ],
      "source": [
        "# 04) FEATURE DERIVATE\n",
        "from __future__ import annotations\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    from shared.common.constants import PRICE_PER_SQM  # type: ignore\n",
        "except Exception:\n",
        "    PRICE_PER_SQM = \"price_per_sqm\"\n",
        "\n",
        "# 1) LAG_HOURS se mancante (da timestamp UTC)\n",
        "if (LAG_HOURS not in df.columns) and ({LAST_VERIFIED_TS, PREDICTION_TS} <= set(df.columns)):\n",
        "    # parse tollerante (accetta naive → le rende UTC)\n",
        "    df[LAST_VERIFIED_TS] = pd.to_datetime(df[LAST_VERIFIED_TS], utc=True, errors=\"coerce\")\n",
        "    df[PREDICTION_TS]   = pd.to_datetime(df[PREDICTION_TS],   utc=True, errors=\"coerce\")\n",
        "\n",
        "    lag = (df[PREDICTION_TS] - df[LAST_VERIFIED_TS]).dt.total_seconds().div(3600)\n",
        "    # valori negativi o assurdi → NaN; poi cast a float32\n",
        "    lag = lag.where(lag >= 0, other=pd.NA)\n",
        "    df[LAG_HOURS] = lag.astype(\"Float32\")\n",
        "    logger.info(\"Creato %s da %s & %s\", LAG_HOURS, LAST_VERIFIED_TS, PREDICTION_TS)\n",
        "\n",
        "elif LAG_HOURS in df.columns:\n",
        "    df[LAG_HOURS] = pd.to_numeric(df[LAG_HOURS], errors=\"coerce\").astype(\"Float32\")\n",
        "else:\n",
        "    logger.warning(\"Impossibile derivare %s: mancano %s o %s\", LAG_HOURS, LAST_VERIFIED_TS, PREDICTION_TS)\n",
        "\n",
        "# 2) condition_minus_risk (utile e non-leaky)\n",
        "if (CONDITION_SCORE in df.columns) and (RISK_SCORE in df.columns):\n",
        "    df[CONDITION_SCORE] = pd.to_numeric(df[CONDITION_SCORE], errors=\"coerce\").astype(\"Float32\")\n",
        "    df[RISK_SCORE]      = pd.to_numeric(df[RISK_SCORE],      errors=\"coerce\").astype(\"Float32\")\n",
        "    df[\"condition_minus_risk\"] = (df[CONDITION_SCORE] - df[RISK_SCORE]).astype(\"Float32\")\n",
        "    logger.info(\"Creato feature derivata: condition_minus_risk\")\n",
        "else:\n",
        "    logger.debug(\"condition_minus_risk non creato: mancano %s o %s\", CONDITION_SCORE, RISK_SCORE)\n",
        "\n",
        "if (\"listing_month\" not in df.columns) and (PREDICTION_TS in df.columns):\n",
        "    if pd.api.types.is_datetime64_any_dtype(df[PREDICTION_TS]) or pd.api.types.is_object_dtype(df[PREDICTION_TS]):\n",
        "        try:\n",
        "            ts = pd.to_datetime(df[PREDICTION_TS], utc=True, errors=\"coerce\")\n",
        "            df[\"listing_month\"] = ts.dt.month.astype(\"Int16\")\n",
        "            logger.info(\"Creato listing_month da %s\", PREDICTION_TS)\n",
        "        except Exception:\n",
        "            logger.debug(\"listing_month non creato (parse fallita)\")\n",
        "\n",
        "# 4) Target: check + coercizione numerica\n",
        "if VALUATION_K not in df.columns:\n",
        "    raise ValueError(f\"{VALUATION_K} mancante: impossibile allenare.\")\n",
        "df[VALUATION_K] = pd.to_numeric(df[VALUATION_K], errors=\"coerce\").astype(\"Float32\")\n",
        "\n",
        "# 5) Verifica finale assenza leakage\n",
        "assert not any(\"price_per_sqm\" in c.lower() for c in df.columns), \"LEAKAGE: colonne 'price_per_sqm*' presenti!\"\n",
        "assert not any(c.lower().startswith(\"valuation_k_\") for c in df.columns if c != VALUATION_K), \\\n",
        "       \"LEAKAGE: derivate 'valuation_k_*' ancora presenti!\"\n",
        "\n",
        "# 6) Snapshot\n",
        "print(\"=\" * 60)\n",
        "print(\"DATASET PULITO - PRIME 3 RIGHE\")\n",
        "print(\"=\" * 60)\n",
        "display(df.head(3))\n",
        "print(f\"\\nShape: {df.shape}\")\n",
        "print(f\"Target (valuation_k) range: [{df[VALUATION_K].min():.2f}, {df[VALUATION_K].max():.2f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fca06807",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "TOP 15 CORRELAZIONI POSITIVE (Pearson) CON IL TARGET\n",
            "============================================================\n",
            "size_m2                 0.634026\n",
            "rooms                   0.421034\n",
            "luxury_score            0.386606\n",
            "confidence_score        0.274871\n",
            "concierge               0.185886\n",
            "year_built              0.178846\n",
            "condition_score         0.101366\n",
            "condition_minus_risk    0.099963\n",
            "garage                  0.070989\n",
            "floor                   0.042733\n",
            "is_top_floor            0.033071\n",
            "has_garden              0.032491\n",
            "has_balcony             0.032208\n",
            "attic                   0.011993\n",
            "cellar                  0.010292\n",
            "Name: valuation_k, dtype: float64\n",
            "\n",
            "============================================================\n",
            "TOP 15 CORRELAZIONI NEGATIVE (Pearson) CON IL TARGET\n",
            "============================================================\n",
            "humidity_level             0.002854\n",
            "building_floors            0.000684\n",
            "temperature_avg           -0.000825\n",
            "has_elevator              -0.001213\n",
            "noise_level               -0.003757\n",
            "env_score                 -0.010018\n",
            "public_transport_nearby   -0.010339\n",
            "owner_occupied            -0.010597\n",
            "bathrooms                 -0.011268\n",
            "parking_spot              -0.029194\n",
            "is_ground_floor           -0.050425\n",
            "risk_score                -0.096957\n",
            "age_years                 -0.178846\n",
            "distance_to_center_km     -0.353575\n",
            "listing_month                   NaN\n",
            "Name: valuation_k, dtype: float64\n",
            "\n",
            "✅ Nessuna correlazione sospetta (>|r| > 0.95 )\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-09-23 02:00:23,914] INFO model_trainer: Correlations saved: c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\artifacts\\target_correlations.json (JSON) | c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\artifacts\\target_correlations.csv (CSV)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "TOP 10 CORRELAZIONI SPEARMAN (assolute) CON IL TARGET\n",
            "============================================================\n",
            "size_m2                  0.699167\n",
            "rooms                    0.458088\n",
            "distance_to_center_km   -0.419562\n",
            "luxury_score             0.399617\n",
            "confidence_score         0.285346\n",
            "concierge                0.221838\n",
            "year_built               0.170086\n",
            "age_years               -0.170086\n",
            "condition_score          0.107864\n",
            "condition_minus_risk     0.106188\n",
            "Name: valuation_k, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# 05) ANALISI CORRELAZIONI CON IL TARGET (no-leakage, robusta)\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "SUSPICIOUS_THR = 0.95\n",
        "corr_json_path = ART_DIR / \"target_correlations.json\"\n",
        "corr_csv_path  = ART_DIR / \"target_correlations.csv\"\n",
        "\n",
        "# 0) Safety: il target deve essere numerico (coercizzato in cella 04)\n",
        "if VALUATION_K not in df.columns:\n",
        "    logger.error(\"Target %s non trovato nel dataset\", VALUATION_K)\n",
        "    corr_json_path.write_text(canonical_json_dumps({\"error\": \"target missing\"}), encoding=\"utf-8\")\n",
        "else:\n",
        "    # 1) Colonne numeriche (post-pulizia) + sanity\n",
        "    numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
        "    if VALUATION_K not in numeric_cols:\n",
        "        logger.warning(\"Il target non risulta numerico: provo a forzare il cast.\")\n",
        "        df[VALUATION_K] = pd.to_numeric(df[VALUATION_K], errors=\"coerce\")\n",
        "        if not pd.api.types.is_numeric_dtype(df[VALUATION_K]):\n",
        "            raise TypeError(f\"{VALUATION_K} non numerico; impossibile calcolare correlazioni.\")\n",
        "\n",
        "    if len(numeric_cols) < 2:\n",
        "        logger.warning(\"Poche colonne numeriche per calcolare correlazioni.\")\n",
        "        corr_json_path.write_text(canonical_json_dumps({\"error\": \"not enough numeric columns\"}), encoding=\"utf-8\")\n",
        "    else:\n",
        "        # 2) Pearson\n",
        "        corr_mat = df[numeric_cols].corr(method=\"pearson\")\n",
        "        if VALUATION_K not in corr_mat.columns:\n",
        "            raise RuntimeError(\"Correlazione Pearson non calcolabile sul target (tutti NaN?).\")\n",
        "\n",
        "        correlations = corr_mat[VALUATION_K].drop(labels=[VALUATION_K], errors=\"ignore\").sort_values(ascending=False)\n",
        "\n",
        "        print(\"=\" * 60)\n",
        "        print(\"TOP 15 CORRELAZIONI POSITIVE (Pearson) CON IL TARGET\")\n",
        "        print(\"=\" * 60)\n",
        "        print(correlations.head(15))\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"TOP 15 CORRELAZIONI NEGATIVE (Pearson) CON IL TARGET\")\n",
        "        print(\"=\" * 60)\n",
        "        print(correlations.tail(15))\n",
        "\n",
        "        suspicious = correlations[correlations.abs() > SUSPICIOUS_THR]\n",
        "        if not suspicious.empty:\n",
        "            print(\"\\n🔴 ATTENZIONE: Correlazioni sospette |r| >\", SUSPICIOUS_THR)\n",
        "            for feat, corr in suspicious.items():\n",
        "                print(f\"  - {feat}: {corr:.4f}\")\n",
        "            logger.warning(\"Possibile leakage o duplicati semantici: %s\", list(suspicious.index))\n",
        "        else:\n",
        "            print(\"\\n✅ Nessuna correlazione sospetta (>|r| >\", SUSPICIOUS_THR, \")\")\n",
        "\n",
        "        # 3) Report strutturato + CSV\n",
        "        corr_df = pd.DataFrame(\n",
        "            {\"feature\": correlations.index, \"correlation_pearson\": correlations.values}\n",
        "        )\n",
        "        payload = {\n",
        "            \"meta\": {\n",
        "                \"method\": \"pearson\",\n",
        "                \"n_numeric_features\": int(len(numeric_cols) - 1),\n",
        "                \"target\": VALUATION_K,\n",
        "                \"suspicious_threshold\": SUSPICIOUS_THR,\n",
        "            },\n",
        "            \"correlations\": corr_df.to_dict(\"records\"),\n",
        "            \"suspicious\": suspicious.to_dict() if not suspicious.empty else {},\n",
        "        }\n",
        "        corr_json_path.write_text(\n",
        "            canonical_json_dumps(payload), encoding=\"utf-8\"\n",
        "        )\n",
        "        corr_df.to_csv(corr_csv_path, index=False)\n",
        "        logger.info(\"Correlations saved: %s (JSON) | %s (CSV)\", corr_json_path, corr_csv_path)\n",
        "\n",
        "        # 4) Spearman (best-effort, robusto a monotonia non lineare)\n",
        "        try:\n",
        "            spearman = df[numeric_cols].corr(method=\"spearman\")[VALUATION_K].drop(labels=[VALUATION_K], errors=\"ignore\")\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(\"TOP 10 CORRELAZIONI SPEARMAN (assolute) CON IL TARGET\")\n",
        "            print(\"=\" * 60)\n",
        "            print(spearman.reindex(spearman.abs().sort_values(ascending=False).index).head(10))\n",
        "        except Exception as e:\n",
        "            logger.debug(\"Spearman correlation failed: %s\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "69b68095-70f4-4b5d-ad0d-7b47ba5f9b7a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape prima dello split: (15000, 44)\n",
            "Colonne totali: 44\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-09-23 02:00:29,358] INFO model_trainer: train: 10500 rows, 44 cols\n",
            "[2025-09-23 02:00:29,359] INFO model_trainer: valid: 2250 rows, 44 cols\n",
            "[2025-09-23 02:00:29,361] INFO model_trainer: test: 2250 rows, 44 cols\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Split completato:\n",
            "  Train: (10500, 44)\n",
            "  Valid: (2250, 44)\n",
            "  Test:  (2250, 44)\n",
            "  Group column: asset_id\n"
          ]
        }
      ],
      "source": [
        "# 06) SPLIT TRAIN/VALID/TEST (strat. su decili) + blocco duplicati per gruppo (default: ASSET_ID)\n",
        "from __future__ import annotations\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# costanti (PRICE_PER_SQM può non essere importato in questo notebook)\n",
        "try:\n",
        "    from shared.common.constants import VALUATION_K, ASSET_ID, PRICE_PER_SQM  # type: ignore\n",
        "except Exception:\n",
        "    from shared.common.constants import VALUATION_K, ASSET_ID  # type: ignore\n",
        "    PRICE_PER_SQM = \"price_per_sqm\"\n",
        "\n",
        "TARGET = VALUATION_K\n",
        "\n",
        "# --- Verifica preliminare\n",
        "print(f\"Dataset shape prima dello split: {df.shape}\")\n",
        "print(f\"Colonne totali: {len(df.columns)}\")\n",
        "\n",
        "# --- 0) Parametri da config (con fallback)\n",
        "if \"TRAIN_CFG\" not in globals() or not isinstance(TRAIN_CFG, dict):\n",
        "    TRAIN_CFG = {}\n",
        "TEST_SIZE = float(TRAIN_CFG.get(\"test_size\", 0.15))\n",
        "VAL_SIZE  = float(TRAIN_CFG.get(\"val_size\",  0.15))\n",
        "N_DECILES = int(TRAIN_CFG.get(\"n_deciles\",   10))\n",
        "GROUP_COL = str(TRAIN_CFG.get(\"group_col\", ASSET_ID))  # puoi mettere 'location' in config se vuoi\n",
        "\n",
        "if not (0.01 <= TEST_SIZE <= 0.9) or not (0.01 <= VAL_SIZE <= 0.9):\n",
        "    logger.warning(\"test_size/val_size fuori range → fallback 0.15/0.15\")\n",
        "    TEST_SIZE, VAL_SIZE = 0.15, 0.15\n",
        "\n",
        "# --- 1) Pulizia target per lo split\n",
        "mask_y = pd.to_numeric(df[TARGET], errors=\"coerce\").notna()\n",
        "if not mask_y.all():\n",
        "    logger.warning(\"Righe senza target rimosse dallo split: %d\", (~mask_y).sum())\n",
        "df_clean = df.loc[mask_y].copy()\n",
        "\n",
        "# --- helper: stratificazione per decili “robusta”\n",
        "def _strat_bins(y: pd.Series, q: int = 10) -> pd.Series:\n",
        "    \"\"\"Decili robusti sul target (usa rank per duplicati). Fallback a singola classe.\"\"\"\n",
        "    y_num = pd.to_numeric(y, errors=\"coerce\")\n",
        "    ranks = y_num.rank(method=\"first\")\n",
        "    unique = int(ranks.nunique())\n",
        "    if unique < 2:\n",
        "        return pd.Series(0, index=y.index, dtype=int)\n",
        "    q_eff = max(2, min(int(q), unique))\n",
        "    try:\n",
        "        bins = pd.qcut(ranks, q=q_eff, labels=False, duplicates=\"drop\")\n",
        "    except Exception:\n",
        "        bins = pd.Series(0, index=y.index, dtype=int)\n",
        "    # riempi eventuali NaN con la moda\n",
        "    if bins.isna().any():\n",
        "        mode_bin = int(bins.dropna().mode().iat[0]) if not bins.dropna().empty else 0\n",
        "        bins = bins.fillna(mode_bin).astype(int)\n",
        "    return bins.astype(int)\n",
        "\n",
        "def _safe_stratify(labels: pd.Series | np.ndarray, min_per_class: int = 2):\n",
        "    \"\"\"Ritorna labels se idonee alla stratificazione, altrimenti None.\"\"\"\n",
        "    lab = pd.Series(labels)\n",
        "    vc = lab.value_counts()\n",
        "    if len(vc) < 2 or (vc < min_per_class).any():\n",
        "        return None\n",
        "    return lab.values\n",
        "\n",
        "# --- 2) Split con blocco duplicati per GROUP_COL (default: ASSET_ID), fallback classico se manca\n",
        "if GROUP_COL in df_clean.columns and df_clean[GROUP_COL].notna().any():\n",
        "    # mediana target per gruppo → decili a livello gruppo\n",
        "    df_clean[GROUP_COL] = df_clean[GROUP_COL].astype(str)\n",
        "    gstats = (\n",
        "        df_clean[[GROUP_COL, TARGET]]\n",
        "        .groupby(GROUP_COL, as_index=False)[TARGET]\n",
        "        .median()\n",
        "        .rename(columns={TARGET: f\"{TARGET}__group_median\"})\n",
        "    )\n",
        "\n",
        "    g_all = gstats[GROUP_COL].values\n",
        "    g_bins_all = _strat_bins(gstats[f\"{TARGET}__group_median\"], q=N_DECILES).values\n",
        "    strat_all = _safe_stratify(g_bins_all)\n",
        "\n",
        "    # primo split: test groups\n",
        "    g_tmp, g_test = train_test_split(\n",
        "        g_all,\n",
        "        test_size=TEST_SIZE,\n",
        "        random_state=SEED,\n",
        "        stratify=strat_all,\n",
        "    )\n",
        "\n",
        "    # secondo split: valid dal residuo\n",
        "    val_rel = VAL_SIZE / max(1e-9, (1.0 - TEST_SIZE))\n",
        "    val_rel = float(min(max(val_rel, 0.05), 0.8))\n",
        "\n",
        "    tmp_mask = np.isin(gstats[GROUP_COL].values, g_tmp)\n",
        "    gstats_tmp = gstats.loc[tmp_mask].copy()\n",
        "    # decili solo sui gruppi rimasti\n",
        "    bins_tmp = _strat_bins(gstats_tmp[f\"{TARGET}__group_median\"], q=N_DECILES).values\n",
        "    # mappa gruppo→bin per stratify\n",
        "    bin_map_tmp = dict(zip(gstats_tmp[GROUP_COL].values, bins_tmp))\n",
        "    y_tmp_bins = np.array([bin_map_tmp.get(g, 0) for g in g_tmp])\n",
        "    strat_tmp = _safe_stratify(y_tmp_bins)\n",
        "\n",
        "    g_train, g_valid = train_test_split(\n",
        "        g_tmp,\n",
        "        test_size=val_rel,\n",
        "        random_state=SEED,\n",
        "        stratify=strat_tmp,\n",
        "    )\n",
        "\n",
        "    G_TRAIN, G_VALID, G_TEST = set(g_train), set(g_valid), set(g_test)\n",
        "    df_train = df_clean[df_clean[GROUP_COL].isin(G_TRAIN)].copy()\n",
        "    df_valid = df_clean[df_clean[GROUP_COL].isin(G_VALID)].copy()\n",
        "    df_test  = df_clean[df_clean[GROUP_COL].isin(G_TEST)].copy()\n",
        "\n",
        "    # overlap check\n",
        "    def _overlap(a, b):\n",
        "        return set(a[GROUP_COL].astype(str)) & set(b[GROUP_COL].astype(str))\n",
        "    ov_tv = _overlap(df_train, df_valid)\n",
        "    ov_tt = _overlap(df_train, df_test)\n",
        "    ov_vt = _overlap(df_valid, df_test)\n",
        "    assert len(ov_tv) == 0 and len(ov_tt) == 0 and len(ov_vt) == 0, (\n",
        "        f\"Overlap {GROUP_COL} tra split! \"\n",
        "        f\"train∩valid={list(ov_tv)[:5]}, train∩test={list(ov_tt)[:5]}, valid∩test={list(ov_vt)[:5]}\"\n",
        "    )\n",
        "\n",
        "else:\n",
        "    logger.warning(\n",
        "        \"%s assente/non valido: uso fallback senza grouping (possibile leakage se ci sono duplicati).\",\n",
        "        GROUP_COL,\n",
        "    )\n",
        "\n",
        "    # stratify riga-level su decili target\n",
        "    bins_all = _strat_bins(df_clean[TARGET], q=N_DECILES)\n",
        "    df_tmp, df_test = train_test_split(\n",
        "        df_clean,\n",
        "        test_size=TEST_SIZE,\n",
        "        random_state=SEED,\n",
        "        stratify=_safe_stratify(bins_all),\n",
        "    )\n",
        "\n",
        "    val_rel = VAL_SIZE / max(1e-9, (1.0 - TEST_SIZE))\n",
        "    val_rel = float(min(max(val_rel, 0.05), 0.8))\n",
        "    bins_tmp = _strat_bins(df_tmp[TARGET], q=N_DECILES)\n",
        "    df_train, df_valid = train_test_split(\n",
        "        df_tmp,\n",
        "        test_size=val_rel,\n",
        "        random_state=SEED,\n",
        "        stratify=_safe_stratify(bins_tmp),\n",
        "    )\n",
        "\n",
        "# --- 3) Log e verifiche generali\n",
        "for name, part in ((\"train\", df_train), (\"valid\", df_valid), (\"test\", df_test)):\n",
        "    logger.info(\"%s: %d rows, %d cols\", name, len(part), part.shape[1])\n",
        "\n",
        "# partition disjoint per index\n",
        "assert len(set(df_train.index) & set(df_valid.index)) == 0\n",
        "assert len(set(df_train.index) & set(df_test.index)) == 0\n",
        "assert len(set(df_valid.index) & set(df_test.index)) == 0\n",
        "\n",
        "# --- 4) Airbag anti-leakage sugli split\n",
        "for split_name, split_df in [(\"train\", df_train), (\"valid\", df_valid), (\"test\", df_test)]:\n",
        "    if any(\"price_per_sqm\" in c.lower() for c in split_df.columns):\n",
        "        logger.error(\"🔴 LEAKAGE: colonne 'price_per_sqm*' in df_%s!\", split_name)\n",
        "\n",
        "print(\"\\n✅ Split completato:\")\n",
        "print(f\"  Train: {df_train.shape}\")\n",
        "print(f\"  Valid: {df_valid.shape}\")\n",
        "print(f\"  Test:  {df_test.shape}\")\n",
        "print(f\"  Group column: {GROUP_COL}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "61650dba",
      "metadata": {},
      "outputs": [],
      "source": [
        "# A) PRE-CHAIN GLOBALE: canonizza geo + crea/riempi prior/derivate minime\n",
        "from shared.common.constants import SIZE_M2\n",
        "\n",
        "\n",
        "try:\n",
        "    from shared.common.config import ASSET_CONFIG  # type: ignore\n",
        "    _PROP = ASSET_CONFIG[\"property\"]\n",
        "    _CITY_BASE = {c.lower(): {z.lower(): v for z, v in d.items()}\n",
        "                  for c, d in (_PROP.get(\"city_base_prices\") or {}).items()}\n",
        "    _REGION_INDEX = {k.lower(): v for k, v in (_PROP.get(\"region_index\") or {\n",
        "        \"north\": 1.05, \"center\": 1.00, \"south\": 0.92\n",
        "    }).items()}\n",
        "except Exception:\n",
        "    _CITY_BASE = {}\n",
        "    _REGION_INDEX = {\"north\": 1.05, \"center\": 1.00, \"south\": 0.92}\n",
        "\n",
        "# mediane di fallback per zona e globale\n",
        "_ZONE_KEYS = set(z for d in _CITY_BASE.values() for z in d.keys())\n",
        "_ZONE_MED = {z: float(np.nanmedian([d.get(z, np.nan) for d in _CITY_BASE.values()])) for z in _ZONE_KEYS} if _CITY_BASE else {}\n",
        "_GLOBAL_CITYZONE_MED = float(np.nanmedian([v for d in _CITY_BASE.values() for v in d.values()])) if _CITY_BASE else 0.0\n",
        "\n",
        "def _canon_geo(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    if \"city\" not in out.columns and \"location\" in out.columns:\n",
        "        out[\"city\"] = out[\"location\"]\n",
        "    if \"zone\" not in out.columns:\n",
        "        out[\"zone\"] = \"semi_center\"\n",
        "    if \"region\" not in out.columns:\n",
        "        out[\"region\"] = \"center\"\n",
        "    for col in (\"city\",\"zone\",\"region\"):\n",
        "        if col in out.columns:\n",
        "            out[col] = out[col].astype(str).str.strip().str.lower()\n",
        "    return out\n",
        "\n",
        "def _ensure_priors_and_min_derivatives(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Crea/riempi: no_elev_high_floor, rooms_per_100sqm, city_zone_prior, region_index_prior.\"\"\"\n",
        "    out = _canon_geo(df)\n",
        "\n",
        "    # no_elev_high_floor = penalità (niente ascensore & piano > 1)\n",
        "    f = pd.to_numeric(out.get(\"floor\"), errors=\"coerce\")\n",
        "    e = pd.to_numeric(out.get(\"has_elevator\"), errors=\"coerce\").fillna(0)\n",
        "    out[\"no_elev_high_floor\"] = ((1 - e) * np.maximum(f - 1, 0)).astype(\"float64\")\n",
        "\n",
        "    # rooms_per_100sqm\n",
        "    s = pd.to_numeric(out.get(SIZE_M2), errors=\"coerce\").replace(0, np.nan)\n",
        "    r = pd.to_numeric(out.get(\"rooms\"), errors=\"coerce\")\n",
        "    out[\"rooms_per_100sqm\"] = (100.0 * r / s).astype(\"float64\")\n",
        "\n",
        "    # city_zone_prior (CITY_BASE con fallback zona→globale)\n",
        "    ci = out.get(\"city\", pd.Series(index=out.index, dtype=str)).astype(str).str.lower()\n",
        "    zo = out.get(\"zone\", pd.Series(index=out.index, dtype=str)).astype(str).str.lower()\n",
        "    vals = []\n",
        "    for c, z in zip(ci, zo):\n",
        "        v = _CITY_BASE.get(c, {}).get(z, np.nan)\n",
        "        if pd.isna(v):\n",
        "            v = _ZONE_MED.get(z, _GLOBAL_CITYZONE_MED)\n",
        "        vals.append(v)\n",
        "    out[\"city_zone_prior\"] = np.asarray(vals, dtype=\"float64\")\n",
        "\n",
        "    # region_index_prior (macroarea)\n",
        "    out[\"region_index_prior\"] = out[\"region\"].astype(str).str.lower().map(_REGION_INDEX).astype(\"float64\")\n",
        "\n",
        "    return out\n",
        "\n",
        "# applica a tutti gli split (non è leakage: sono feature ex-ante, no target)\n",
        "for _name in (\"df_train\",\"df_valid\",\"df_test\"):\n",
        "    if _name in globals() and isinstance(globals()[_name], pd.DataFrame):\n",
        "        globals()[_name] = _ensure_priors_and_min_derivatives(globals()[_name])\n",
        "\n",
        "try:\n",
        "    n_nan_cz = int(pd.to_numeric(df_train[\"city_zone_prior\"], errors=\"coerce\").isna().sum())\n",
        "    n_nan_ri = int(pd.to_numeric(df_train[\"region_index_prior\"], errors=\"coerce\").isna().sum())\n",
        "    n_nan_ne = int(pd.to_numeric(df_train[\"no_elev_high_floor\"], errors=\"coerce\").isna().sum())\n",
        "    n_nan_rr = int(pd.to_numeric(df_train[\"rooms_per_100sqm\"], errors=\"coerce\").isna().sum())\n",
        "    (ART_DIR / \"prechain_checks.txt\").write_text(\n",
        "        f\"train NaN city_zone_prior={n_nan_cz}, region_index_prior={n_nan_ri}, \"\n",
        "        f\"no_elev_high_floor={n_nan_ne}, rooms_per_100sqm={n_nan_rr}\\n\",\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "except Exception:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f60aec06-522b-4e18-ad3b-71c1f0e62d1b",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Anomaly Flags (train only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3379aac2-ab77-48cc-a77d-693a20d1104c",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-09-23 02:00:35,818] INFO model_trainer: Anomaly features (train only): ['condition_minus_risk', 'size_m2', 'luxury_score', 'env_score', 'distance_to_center_km', 'air_quality_index', 'noise_level', 'humidity_level', 'temperature_avg']\n",
            "[2025-09-23 02:00:44,214] INFO model_trainer: Anomalie raw: 315 | refined: 271\n",
            "[2025-09-23 02:00:44,244] INFO model_trainer: sample_weight da severity_score (mean=1.000, min=0.816, max=1.001)\n"
          ]
        }
      ],
      "source": [
        "# 04) Flags di outlier/anomalie SOLO sul TRAIN → feature/pesi (no leakage)\n",
        "from __future__ import annotations\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- costanti con import robusto\n",
        "try:\n",
        "    from shared.common.constants import ENV_SCORE, LUXURY_SCORE, SIZE_M2, VALUATION_K  # type: ignore\n",
        "except Exception:\n",
        "    ENV_SCORE, LUXURY_SCORE, SIZE_M2, VALUATION_K = \"env_score\", \"luxury_score\", \"size_m2\", \"valuation_k\"\n",
        "\n",
        "try:\n",
        "    from shared.common.constants import LAG_HOURS  # type: ignore\n",
        "except Exception:\n",
        "    LAG_HOURS = \"lag_hours\"\n",
        "\n",
        "# feature “leaky”/derivate definite nell’EDA (se presenti)\n",
        "try:\n",
        "    from shared.n02_explore_dataset.eda_core import AnomalyDetector, LEAKY_FEATURES, TARGET_DERIVED_FEATURES  # type: ignore\n",
        "except Exception:\n",
        "    AnomalyDetector = None  # fallback sotto\n",
        "    LEAKY_FEATURES = {\"price_per_sqm\", \"price_per_sqm_capped\", \"price_per_sqm_vs_region_avg\", \"valuation_k_log\"}\n",
        "    TARGET_DERIVED_FEATURES = {\"_viz_price_per_sqm\", \"valuation_k_decile\", \"valuation_rank\", \"is_top_valuation\"}\n",
        "\n",
        "# Parametri con override da config\n",
        "if \"TRAIN_CFG\" not in globals() or not isinstance(TRAIN_CFG, dict):\n",
        "    TRAIN_CFG = {}\n",
        "contamination       = float(TRAIN_CFG.get(\"anomaly_contamination\", 0.03))\n",
        "strong_z_threshold  = float(TRAIN_CFG.get(\"anomaly_strong_z\", 2.5))\n",
        "severity_percentile = float(TRAIN_CFG.get(\"anomaly_severity_pct\", 90.0))\n",
        "n_estimators        = int(TRAIN_CFG.get(\"anomaly_n_estimators\", 200))\n",
        "\n",
        "# 4.1 Scelta feature candidate (numeric, no target/leaky/derived)\n",
        "num_cols = [c for c in df_train.columns if pd.api.types.is_numeric_dtype(df_train[c])]\n",
        "exclude  = set(LEAKY_FEATURES) | set(TARGET_DERIVED_FEATURES) | {VALUATION_K, \"price_per_sqm\"}\n",
        "\n",
        "prefer   = [\n",
        "    \"condition_minus_risk\", SIZE_M2, LUXURY_SCORE, ENV_SCORE,\n",
        "    \"building_age_years\", \"distance_to_center_km\", LAG_HOURS,\n",
        "    \"air_quality_index\", \"noise_level\", \"humidity_level\", \"temperature_avg\",\n",
        "]\n",
        "feat_cand = [c for c in prefer if c in df_train.columns and c in num_cols and c not in exclude]\n",
        "\n",
        "# fallback: scegli le prime N numeriche con var > 0 e almeno 10 valori unici\n",
        "if len(feat_cand) < 3:\n",
        "    cand_pool = []\n",
        "    for c in num_cols:\n",
        "        if c in exclude or c == \"sample_weight\":\n",
        "            continue\n",
        "        s = pd.to_numeric(df_train[c], errors=\"coerce\")\n",
        "        if s.nunique(dropna=True) >= 10 and np.nanvar(s.values) > 0:\n",
        "            cand_pool.append((c, float(np.nanvar(s.values))))\n",
        "    cand_pool.sort(key=lambda x: x[1], reverse=True)\n",
        "    feat_cand = [c for c, _ in cand_pool[:8]]  # max 8\n",
        "\n",
        "if feat_cand:\n",
        "    logger.info(\"Anomaly features (train only): %s\", feat_cand)\n",
        "\n",
        "    # 4.2 Rilevamento anomalie (classe ufficiale → fallback z-score)\n",
        "    if AnomalyDetector is not None:\n",
        "        anom = AnomalyDetector(\n",
        "            contamination=contamination,\n",
        "            strong_z_threshold=strong_z_threshold,\n",
        "            severity_percentile=severity_percentile,\n",
        "            n_estimators=n_estimators,\n",
        "            random_state=SEED,\n",
        "        )\n",
        "        df_train_anom, anom_rep = anom.detect_anomalies(\n",
        "            df_train,\n",
        "            feature_candidates=feat_cand,\n",
        "            exclude_features=set(),  # già esclusi a monte\n",
        "        )\n",
        "    else:\n",
        "        # --- Fallback semplice: z-score medio + percentile su features candidate\n",
        "        X = df_train[feat_cand].copy()\n",
        "        X = X.apply(pd.to_numeric, errors=\"coerce\")\n",
        "        mu = X.mean(axis=0)\n",
        "        sd = X.std(axis=0).replace(0, np.nan)\n",
        "        z  = (X - mu) / sd\n",
        "        z_abs = z.abs()\n",
        "        z_mean = z_abs.mean(axis=1)  # severità media\n",
        "        thr = np.nanpercentile(z_mean.dropna().values, severity_percentile)\n",
        "        flags_raw = (z_abs > strong_z_threshold).any(axis=1)\n",
        "        flags_ref = (z_mean >= thr)\n",
        "\n",
        "        df_train_anom = df_train.copy()\n",
        "        df_train_anom[\"anomaly_flag\"]    = flags_raw.astype(np.int8)\n",
        "        df_train_anom[\"anomaly_refined\"] = flags_ref.astype(np.int8)\n",
        "        df_train_anom[\"severity_score\"]  = z_mean.fillna(0).astype(\"float32\")\n",
        "\n",
        "        n_raw = int(flags_raw.sum())\n",
        "        n_ref = int(flags_ref.sum())\n",
        "        anom_rep = {\n",
        "            \"method\": \"fallback_zscore\",\n",
        "            \"features\": feat_cand,\n",
        "            \"strong_z_threshold\": strong_z_threshold,\n",
        "            \"severity_percentile\": severity_percentile,\n",
        "            \"n_anomalies_raw\": n_raw,\n",
        "            \"n_anomalies_refined\": n_ref,\n",
        "        }\n",
        "\n",
        "    logger.info(\"Anomalie raw: %s | refined: %s\",\n",
        "                anom_rep.get(\"n_anomalies_raw\"), anom_rep.get(\"n_anomalies_refined\"))\n",
        "\n",
        "    # 4.3 trasferisci colonne utili SOLO su train (no leakage)\n",
        "    for col in (\"anomaly_flag\", \"anomaly_refined\", \"severity_score\"):\n",
        "        if col in df_train_anom.columns:\n",
        "            df_train.loc[df_train_anom.index, col] = df_train_anom[col]\n",
        "\n",
        "    # 4.4 salva report\n",
        "    (ART_DIR / \"anomaly_train_report.json\").write_text(\n",
        "        canonical_json_dumps(anom_rep),\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "else:\n",
        "    logger.info(\"Anomaly detection skipped: nessuna feature candidata valida.\")\n",
        "\n",
        "# 4.5 sample_weight (fallback = 1.0) — SOLO TRAIN\n",
        "if \"severity_score\" in df_train.columns and df_train[\"severity_score\"].notna().any():\n",
        "    sev = pd.to_numeric(df_train[\"severity_score\"], errors=\"coerce\").clip(lower=0).astype(\"float32\")\n",
        "    w   = 1.0 / (1.0 + sev)              # decresce con severità\n",
        "    w   = w.clip(lower=0.2, upper=1.0)   # evita pesi troppo piccoli\n",
        "    w   = w * (1.0 / max(w.mean(), 1e-6))  # normalize mean≈1.0\n",
        "    df_train[\"sample_weight\"] = w.astype(\"float32\")\n",
        "    logger.info(\"sample_weight da severity_score (mean=%.3f, min=%.3f, max=%.3f)\",\n",
        "                float(w.mean()), float(w.min()), float(w.max()))\n",
        "elif \"confidence_score\" in df_train.columns and df_train[\"confidence_score\"].notna().any():\n",
        "    w = pd.to_numeric(df_train[\"confidence_score\"], errors=\"coerce\").clip(0.2, 1.0).astype(\"float32\")\n",
        "    w = w * (1.0 / max(w.mean(), 1e-6))\n",
        "    df_train[\"sample_weight\"] = w\n",
        "    logger.info(\"sample_weight da confidence_score (mean=%.3f, min=%.3f, max=%.3f)\",\n",
        "                float(w.mean()), float(w.min()), float(w.max()))\n",
        "else:\n",
        "    df_train[\"sample_weight\"] = np.float32(1.0)\n",
        "    logger.info(\"sample_weight uniforme (1.0) — nessuna metrica disponibile.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c915f13-189c-4380-9c72-a30680dfdd1b",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Feature Preparation & Pipelines A/B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "26fd68d0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decile distribution:\n",
            " train: {0: 1050, 1: 1050, 2: 1050, 3: 1050, 4: 1050, 5: 1050, 6: 1050, 7: 1050, 8: 1050, 9: 1050}\n",
            " valid: {0: 225, 1: 225, 2: 225, 3: 225, 4: 225, 5: 225, 6: 225, 7: 225, 8: 225, 9: 225}\n",
            " test : {0: 225, 1: 225, 2: 225, 3: 225, 4: 225, 5: 225, 6: 225, 7: 225, 8: 225, 9: 225}\n"
          ]
        }
      ],
      "source": [
        "# 08-bis) Stratified splits su decili del target (train/valid/test) + distribuzioni\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "VALUATION_K = \"valuation_k\"\n",
        "SEED = int(globals().get(\"SEED\", 42))\n",
        "\n",
        "def _load_base_df():\n",
        "    # priorità: df già in RAM; in alternativa carica da outputs/\n",
        "    for name in (\"df_full\", \"df\", \"df_raw\", \"df_train_full\"):\n",
        "        if name in globals() and isinstance(globals()[name], pd.DataFrame) and VALUATION_K in globals()[name].columns:\n",
        "            return globals()[name].copy()\n",
        "    for p in [Path(\"outputs/dataset_generated.parquet\"), Path(\"outputs/dataset_generated.csv\")]:\n",
        "        if p.exists():\n",
        "            return pd.read_parquet(p) if p.suffix.lower() != \".csv\" else pd.read_csv(p)\n",
        "    raise RuntimeError(\"Dataset base non trovato per lo split stratificato.\")\n",
        "\n",
        "df_all = _load_base_df()\n",
        "df_all = df_all.loc[pd.to_numeric(df_all[VALUATION_K], errors=\"coerce\").notna()].copy()\n",
        "\n",
        "y_all = pd.to_numeric(df_all[VALUATION_K], errors=\"coerce\").to_numpy(dtype=\"float64\")\n",
        "\n",
        "# Decili robusti\n",
        "try:\n",
        "    d_all = pd.qcut(y_all, q=10, labels=False, duplicates=\"drop\")\n",
        "    if d_all.isna().any():\n",
        "        raise ValueError(\"qcut produced NaNs\")\n",
        "    d_all = d_all.to_numpy(dtype=int)\n",
        "except Exception:\n",
        "    qs = np.quantile(y_all[~np.isnan(y_all)], np.linspace(0, 1, 11))\n",
        "    qs = np.unique(qs)\n",
        "    if len(qs) < 3:\n",
        "        d_all = np.zeros_like(y_all, dtype=int)\n",
        "    else:\n",
        "        d_all = np.clip(np.digitize(y_all, qs[1:-1], right=True), 0, 9)\n",
        "\n",
        "# Split 70/15/15 con stratify\n",
        "idx_all = np.arange(len(df_all))\n",
        "idx_tr, idx_tmp, y_tr, y_tmp, d_tr, d_tmp = train_test_split(\n",
        "    idx_all, y_all, d_all, test_size=0.30, stratify=d_all, random_state=SEED, shuffle=True\n",
        ")\n",
        "idx_va, idx_te, y_va, y_te, d_va, d_te = train_test_split(\n",
        "    idx_tmp, y_tmp, d_tmp, test_size=0.50, stratify=d_tmp, random_state=SEED, shuffle=True\n",
        ")\n",
        "\n",
        "df_train = df_all.iloc[idx_tr].copy()\n",
        "df_valid = df_all.iloc[idx_va].copy()\n",
        "df_test  = df_all.iloc[idx_te].copy()\n",
        "\n",
        "# Esponi anche i decili (la cella 09 li raccoglie nel manifest “best-effort”)\n",
        "d_train, d_valid, d_test = d_tr, d_va, d_te\n",
        "\n",
        "def _counts(arr):\n",
        "    uniq, cnt = np.unique(arr, return_counts=True)\n",
        "    return {int(k): int(v) for k, v in zip(uniq, cnt)}\n",
        "\n",
        "print(\"Decile distribution:\")\n",
        "print(\" train:\", _counts(d_train))\n",
        "print(\" valid:\", _counts(d_valid))\n",
        "print(\" test :\", _counts(d_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "dd171f89-0d87-4a14-b26a-3e39db97ebec",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== VERIFICA FEATURES =====\n",
            "Categoriche: ['zone', 'urban_type', 'region', 'energy_class', 'condition', 'heating', 'view', 'public_transport_nearby']\n",
            "Numeriche  : ['size_m2', 'rooms', 'bathrooms', 'year_built', 'floor', 'building_floors', 'is_top_floor', 'has_elevator', 'has_garden', 'has_balcony', 'log_size_m2', 'sqm_per_room', 'baths_per_100sqm', 'elev_x_floor', 'no_elev_high_floor', 'rooms_per_100sqm', 'city_zone_prior', 'region_index_prior']\n",
            "Deriver attivo?: True\n",
            "Model     A: RandomForest | n_features: 26\n",
            "Model     B: RandomForest | n_features: 26\n"
          ]
        }
      ],
      "source": [
        "# === FEATURE PREP + ANALISI + AUTO-UPDATE (UNIFICATA) ========================\n",
        "from __future__ import annotations\n",
        "import os, re, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# ── util / costanti minime\n",
        "try:\n",
        "    from shared.n03_train_model.preprocessing import ML_LEAKY_FEATURES  # type: ignore\n",
        "except Exception:\n",
        "    ML_LEAKY_FEATURES = {\n",
        "        \"price_per_sqm\",\"price_per_sqm_vs_region_avg\",\"price_per_sqm_capped\",\n",
        "        \"valuation_k_log\",\"_viz_price_per_sqm\",\n",
        "        \"valuation_k_decile\",\"valuation_rank\",\"is_top_valuation\"\n",
        "    }\n",
        "\n",
        "try:\n",
        "    NumpyJSONEncoder\n",
        "except NameError:\n",
        "    class NumpyJSONEncoder(json.JSONEncoder):\n",
        "        def default(self, obj):\n",
        "            import numpy as _np\n",
        "            if isinstance(obj, (_np.integer,)):  return int(obj)\n",
        "            if isinstance(obj, (_np.floating,)): return float(obj)\n",
        "            if isinstance(obj, (_np.ndarray,)):  return obj.tolist()\n",
        "            return super().default(obj)\n",
        "\n",
        "\n",
        "def _ensure_columns(df_part: pd.DataFrame, required: list[str]) -> pd.DataFrame:\n",
        "    missing = [c for c in required if c not in df_part.columns]\n",
        "    if missing:\n",
        "        for c in missing:\n",
        "            df_part[c] = np.nan\n",
        "    return df_part[required]\n",
        "\n",
        "from shared.common.constants import VALUATION_K, ASSET_ID\n",
        "from shared.common.config import ASSET_CONFIG\n",
        "\n",
        "# ── helpers\n",
        "def _uniq(xs: list[str]) -> list[str]: return list(dict.fromkeys(xs))\n",
        "def _matches_any(col: str, pats: list[str]) -> bool: return any(re.search(p, col, re.I) for p in pats)\n",
        "def _is_numeric(s: pd.Series) -> bool: return pd.api.types.is_numeric_dtype(s)\n",
        "\n",
        "# ==== 0) Policy allowlist + esclusioni (guided by config) ====================\n",
        "_cfg = ASSET_CONFIG[\"property\"]\n",
        "CFG_CAT = list(_cfg.get(\"categorical\", []))\n",
        "CFG_NUM = list(_cfg.get(\"numeric\", []))\n",
        "CFG_EXC = set(_cfg.get(\"exclude\", []))\n",
        "\n",
        "EXTRA_KEEP = [\"rooms\",\"has_elevator\",\"has_garage\",\"has_garden\",\"has_balcony\",\n",
        "              \"is_top_floor\",\"listing_month\",\"city\",\"zone\",\"urban_type\"]\n",
        "FEATURE_ALLOWLIST = _uniq([*(CFG_CAT + CFG_NUM), *EXTRA_KEEP])\n",
        "\n",
        "ALWAYS_EXCLUDE = set(CFG_EXC) | {\n",
        "    VALUATION_K,\n",
        "    ASSET_ID, \"record_id\",\"listing_id\",\"asset_type_id\",\n",
        "    \"source\",\"source_name\",\"source_url\",\"dataset_version\",\n",
        "    \"prediction_ts\",\"last_verified_ts\",\"ingestion_ts\",\n",
        "    \"created_at\",\"updated_at\",\"listing_ts\",\"lag_hours\",\n",
        "    \"sample_weight\",\"weight\",\"severity_score\",\n",
        "    \"outlier_count\",\"n_outlier_sources\",\"outlier_source\",\n",
        "    \"confidence_score\",\n",
        "    \"y_pred\",\"predicted_valuation_k\",\"valuation_k_hat\",\n",
        "    \"valuation_k_log\",\"valuation_k_decile\",\"valuation_rank\",\"is_top_valuation\",\n",
        "    \"strongly_incoherent\",\"price_per_sqm_capped_violated\",\n",
        "}\n",
        "\n",
        "EXCLUDE_PATTERNS = [\n",
        "    r\"price_per_sqm\",\n",
        "    r\"^valuation_k_.+\",\n",
        "    r\"(?:^|_)id$\",\n",
        "    r\"(?:^|_)(created|updated|ingestion|prediction|last_verified|listing)_ts$\",\n",
        "    r\"_url$|_hash$\",\n",
        "    r\"^y_pred$|_pred(?:iction)?_\",\n",
        "    r\"(?:^|_)(avg|mean|median|benchmark|zscore|rank|decile)(?:_|$).*?(price|valuation)\",\n",
        "    r\"(price|valuation).*?(avg|mean|median|benchmark|zscore|rank|decile)\",\n",
        "    r\"(?:^|_)drift(?:_|$)|(?:^|_)caps?(?:_|$)|(?:^|_)vs_(?:_|$)\",\n",
        "]\n",
        "\n",
        "# leakage hard-stop\n",
        "leaky_check = [c for c in ML_LEAKY_FEATURES if c in df_train.columns]\n",
        "if leaky_check:\n",
        "    raise ValueError(f\"Leakage detected in training set: {leaky_check}\")\n",
        "\n",
        "DYNAMIC_EXCLUDE = {c for c in df_train.columns if _matches_any(c, EXCLUDE_PATTERNS)}\n",
        "EXCLUDE_ALL = set(ALWAYS_EXCLUDE) | set(DYNAMIC_EXCLUDE)\n",
        "\n",
        "# ==== 1) Split iniziale cat / num (allowlist-aware) ==========================\n",
        "# Nota: per evitare overweight della macroarea, NON obblighiamo 'region' come categorica.\n",
        "CATEGORICAL_FEATURES = _uniq([\"city\",\"zone\",\"urban_type\"] + [c for c in CFG_CAT if c not in {\"location\"}])\n",
        "cat_cols = [c for c in CATEGORICAL_FEATURES if c in df_train.columns and c not in EXCLUDE_ALL]\n",
        "num_cols = [c for c in df_train.columns if (c not in EXCLUDE_ALL) and _is_numeric(df_train[c])]\n",
        "\n",
        "allow = set([c for c in FEATURE_ALLOWLIST if c in df_train.columns])\n",
        "cat_cols = [c for c in _uniq(cat_cols) if c in allow]\n",
        "num_cols = [c for c in _uniq(num_cols) if (c not in set(cat_cols)) and (c in allow)]\n",
        "\n",
        "MIN_FEATS = int(os.getenv(\"MIN_FEATS_ALLOWLIST\", \"12\"))\n",
        "if len(cat_cols) + len(num_cols) < MIN_FEATS:\n",
        "    more_cat = [c for c in CATEGORICAL_FEATURES if c in df_train.columns and c not in EXCLUDE_ALL and c not in cat_cols]\n",
        "    more_num = []\n",
        "    for c in df_train.columns:\n",
        "        if c in EXCLUDE_ALL or c in cat_cols or c in num_cols: \n",
        "            continue\n",
        "        s = pd.to_numeric(df_train[c], errors=\"coerce\")\n",
        "        if _is_numeric(s) and s.nunique(dropna=True) >= 10 and np.nanvar(s.values) > 0:\n",
        "            more_num.append(c)\n",
        "    cat_cols = _uniq(cat_cols + more_cat)[:15]\n",
        "    num_cols = _uniq(num_cols + more_num)[:25]\n",
        "\n",
        "constant_cols = [c for c in num_cols if df_train[c].nunique(dropna=True) <= 1]\n",
        "if constant_cols:\n",
        "    num_cols = [c for c in num_cols if c not in constant_cols]\n",
        "\n",
        "# ==== 2) (opzionale) step di derivazione — SOLO via transformer importabile ===\n",
        "# Niente logiche/priors qui: se il transformer c'è lo usiamo, altrimenti si procede senza.\n",
        "_include_derive = False\n",
        "DERIVED_FEATURES = [\n",
        "    \"log_size_m2\",\"sqm_per_room\",\"baths_per_100sqm\",\n",
        "    \"elev_x_floor\",\"no_elev_high_floor\",\"rooms_per_100sqm\",\n",
        "    \"city_zone_prior\",\"region_index_prior\",\n",
        "]\n",
        "feature_deriver = \"passthrough\"\n",
        "try:\n",
        "    # prova più namespace\n",
        "    try:\n",
        "        from shared.common.transformers import PropertyDerivedFeatures  # type: ignore\n",
        "    except Exception:\n",
        "        from shared.common.transformers import PropertyDerivedFeatures  # type: ignore  # noqa\n",
        "    feature_deriver = PropertyDerivedFeatures()  # usa i default interni\n",
        "    _include_derive = True\n",
        "except Exception:\n",
        "    _include_derive = False\n",
        "\n",
        "# se il deriver è attivo, dichiara le derivate tra le numeriche (verranno create nello step precedente al prep)\n",
        "if _include_derive:\n",
        "    num_cols = _uniq(num_cols + DERIVED_FEATURES)\n",
        "\n",
        "# ==== 3) Preprocessori (OHE compat) =========================================\n",
        "def _build_ohe(min_freq=None, as_sparse=True):\n",
        "    kw = dict(handle_unknown=\"ignore\")\n",
        "    if isinstance(min_freq, (int, float)):\n",
        "        try: kw[\"min_frequency\"] = min_freq\n",
        "        except TypeError: pass\n",
        "    try:\n",
        "        return OneHotEncoder(sparse_output=as_sparse, **kw)  # sklearn >=1.2\n",
        "    except TypeError:\n",
        "        return OneHotEncoder(sparse=as_sparse, **kw)         # sklearn <1.2\n",
        "\n",
        "min_freq = TRAIN_CFG.get(\"ohe_min_frequency\", None)\n",
        "\n",
        "cat_pipe = Pipeline([\n",
        "    (\"impute\", SimpleImputer(strategy=\"constant\", fill_value=\"__MISSING__\")),\n",
        "    (\"encode\", _build_ohe(min_freq)),\n",
        "])\n",
        "num_pipe = Pipeline([(\"impute\", SimpleImputer(strategy=\"median\"))])\n",
        "\n",
        "# ==== 4) Colonne & pipeline A/B =============================================\n",
        "num_cols_B = [c for c in num_cols if c != \"confidence_score\"]\n",
        "\n",
        "preproc_A = ColumnTransformer([(\"cat\", cat_pipe, cat_cols), (\"num\", num_pipe, num_cols)], remainder=\"drop\")\n",
        "preproc_B = ColumnTransformer([(\"cat\", cat_pipe, cat_cols), (\"num\", num_pipe, num_cols_B)], remainder=\"drop\")\n",
        "\n",
        "features_A = _uniq(cat_cols + num_cols)\n",
        "features_B = _uniq(cat_cols + num_cols_B)\n",
        "\n",
        "df_train_A = _ensure_columns(df_train.copy(), features_A)\n",
        "df_valid_A = _ensure_columns(df_valid.copy(), features_A)\n",
        "df_test_A  = _ensure_columns(df_test.copy(),  features_A)\n",
        "\n",
        "df_train_B = _ensure_columns(df_train.copy(), features_B)\n",
        "df_valid_B = _ensure_columns(df_valid.copy(), features_B)\n",
        "df_test_B  = _ensure_columns(df_test.copy(),  features_B)\n",
        "\n",
        "X_train = df_train_A[features_A].copy()\n",
        "X_valid = df_valid_A[features_A].copy()\n",
        "X_test  = df_test_A[features_A].copy()\n",
        "\n",
        "Xtr_B = df_train_B[features_B].copy()\n",
        "Xva_B = df_valid_B[features_B].copy()\n",
        "Xte_B = df_test_B[features_B].copy()\n",
        "\n",
        "MODEL_KIND = str(TRAIN_CFG.get(\"model\", os.getenv(\"MODEL_KIND\",\"rf\"))).lower()\n",
        "ModelA = RandomForestRegressor; ModelB = RandomForestRegressor\n",
        "MODEL_FAMILY_A = MODEL_FAMILY_B = \"RandomForest\"\n",
        "\n",
        "if MODEL_KIND in {\"xgb\",\"xgboost\"}:\n",
        "    try:\n",
        "        from xgboost import XGBRegressor  # type: ignore\n",
        "        ModelA = ModelB = XGBRegressor\n",
        "        MODEL_FAMILY_A = MODEL_FAMILY_B = \"XGBRegressor\"\n",
        "    except Exception:\n",
        "        MODEL_FAMILY_A = MODEL_FAMILY_B = \"RandomForest\"\n",
        "\n",
        "if MODEL_FAMILY_A == \"RandomForest\":\n",
        "    model_A = ModelA(n_estimators=200, random_state=SEED, n_jobs=-1, max_depth=None, min_samples_leaf=2)\n",
        "    model_B = ModelB(n_estimators=200, random_state=SEED, n_jobs=-1, max_depth=None, min_samples_leaf=2)\n",
        "else:\n",
        "    model_A = ModelA(n_estimators=200, max_depth=6, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8,\n",
        "                     reg_alpha=0.0, reg_lambda=1.0, random_state=SEED, tree_method=\"hist\")\n",
        "    model_B = ModelB(n_estimators=200, max_depth=6, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8,\n",
        "                     reg_alpha=0.0, reg_lambda=1.0, random_state=SEED, tree_method=\"hist\")\n",
        "\n",
        "steps_A = [(\"prep\", preproc_A), (\"model\", model_A)]\n",
        "steps_B = [(\"prep\", preproc_B), (\"model\", model_B)]\n",
        "if _include_derive:\n",
        "    steps_A = [(\"derive\", feature_deriver)] + steps_A\n",
        "    steps_B = [(\"derive\", feature_deriver)] + steps_B\n",
        "\n",
        "pipe_A = Pipeline(steps_A)\n",
        "pipe_B = Pipeline(steps_B)\n",
        "\n",
        "# ==== 5) Analisi semplice & artefatti =======================================\n",
        "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
        "card_rows = []\n",
        "for c in features_A:\n",
        "    if c not in df_train.columns: continue\n",
        "    s = df_train[c]\n",
        "    card_rows.append({\n",
        "        \"feature\": c, \"dtype\": str(s.dtype),\n",
        "        \"n_unique\": int(s.nunique(dropna=True)),\n",
        "        \"pct_missing\": float(s.isna().mean()*100.0),\n",
        "        \"is_categorical_like\": bool(s.dtype.name in (\"object\",\"category\",\"bool\")),\n",
        "        \"is_numeric\": bool(pd.api.types.is_numeric_dtype(s)),\n",
        "    })\n",
        "pd.DataFrame(card_rows).sort_values([\"is_categorical_like\",\"n_unique\"], ascending=[False,True]) \\\n",
        "  .to_csv(ART_DIR / \"mlprep_cardinality_missing.csv\", index=False)\n",
        "\n",
        "# ==== 6) Allinea target/pesi (senza manipolarli qui) =========================\n",
        "def _align_targets_and_weights():\n",
        "    global y_train, y_valid, y_test, y_val_orig, y_test_orig, w_train\n",
        "    y_train = df_train.loc[X_train.index, VALUATION_K].to_numpy()\n",
        "    y_valid = df_valid.loc[X_valid.index, VALUATION_K].to_numpy()\n",
        "    y_test  = df_test.loc[X_test.index,  VALUATION_K].to_numpy()\n",
        "    y_val_orig  = y_valid.copy(); y_test_orig = y_test.copy()\n",
        "    if \"Xtr_B\" in globals():\n",
        "        if \"sample_weight\" in df_train.columns:\n",
        "            w_train = df_train.loc[Xtr_B.index, \"sample_weight\"].astype(\"float32\").to_numpy()\n",
        "        else:\n",
        "            w_train = np.ones(len(Xtr_B), dtype=\"float32\")\n",
        "    assert len(X_train)==len(y_train) and len(X_valid)==len(y_val_orig) and len(X_test)==len(y_test_orig)\n",
        "_align_targets_and_weights()\n",
        "\n",
        "print(\"\\n===== VERIFICA FEATURES =====\")\n",
        "print(\"Categoriche:\", cat_cols)\n",
        "print(\"Numeriche  :\", num_cols)\n",
        "print(\"Deriver attivo?:\", _include_derive)\n",
        "print(\"Model     A:\", MODEL_FAMILY_A, \"| n_features:\", len(features_A))\n",
        "print(\"Model     B:\", MODEL_FAMILY_B, \"| n_features:\", len(features_B))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8ee34b56",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === PATCH EVAL: assicurati che le derivate/priors ci siano (no all-NaN) ===\n",
        "from __future__ import annotations\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "# prova a leggere i mapping da config (fallback sicuri)\n",
        "try:\n",
        "    from shared.common.config import ASSET_CONFIG  # type: ignore\n",
        "    _PROP = ASSET_CONFIG[\"property\"]\n",
        "    _CITY_BASE = {c.lower(): {z.lower(): v for z, v in d.items()}\n",
        "                  for c, d in (_PROP.get(\"city_base_prices\") or {}).items()}\n",
        "    _REGION_INDEX = {k.lower(): float(v) for k, v in (_PROP.get(\"region_index\") or {\n",
        "        \"north\": 1.05, \"center\": 1.00, \"south\": 0.92\n",
        "    }).items()}\n",
        "except Exception:\n",
        "    _CITY_BASE = {}\n",
        "    _REGION_INDEX = {\"north\": 1.05, \"center\": 1.00, \"south\": 0.92}\n",
        "\n",
        "# mediane di fallback per zona e globale (se serve)\n",
        "_ZONE_KEYS = set(z for d in _CITY_BASE.values() for z in d.keys())\n",
        "_ZONE_MED = {z: float(np.nanmedian([d.get(z, np.nan) for d in _CITY_BASE.values()]))\n",
        "             for z in _ZONE_KEYS} if _CITY_BASE else {}\n",
        "_GLOBAL_CITYZONE_MED = (float(np.nanmedian([v for d in _CITY_BASE.values() for v in d.values()]))\n",
        "                        if _CITY_BASE else 0.0)\n",
        "\n",
        "_DERIVED_ALL = [\n",
        "    \"log_size_m2\",\"sqm_per_room\",\"baths_per_100sqm\",\"elev_x_floor\",\n",
        "    \"no_elev_high_floor\",\"rooms_per_100sqm\",\"city_zone_prior\",\"region_index_prior\",\n",
        "]\n",
        "\n",
        "def _needs(col: str, df: pd.DataFrame) -> bool:\n",
        "    \"\"\"Serve calcolarla? Solo se attesa nelle features e assente o tutta NaN.\"\"\"\n",
        "    if 'features_A' not in globals():\n",
        "        return False\n",
        "    if col not in features_A:\n",
        "        return False\n",
        "    if col not in df.columns:\n",
        "        return True\n",
        "    s = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "    return not s.notna().any()\n",
        "\n",
        "def _ensure_eval_derivatives(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "\n",
        "    # basi comode\n",
        "    size = pd.to_numeric(out.get(\"size_m2\"), errors=\"coerce\")\n",
        "    rooms = pd.to_numeric(out.get(\"rooms\"), errors=\"coerce\")\n",
        "    baths = pd.to_numeric(out.get(\"bathrooms\"), errors=\"coerce\")\n",
        "    floor = pd.to_numeric(out.get(\"floor\"), errors=\"coerce\")\n",
        "    elev  = pd.to_numeric(out.get(\"has_elevator\"), errors=\"coerce\").fillna(0)\n",
        "\n",
        "    # 1) derivate geometriche/funzionali\n",
        "    if _needs(\"log_size_m2\", out) and \"size_m2\" in out.columns:\n",
        "        out[\"log_size_m2\"] = np.log1p(size)\n",
        "\n",
        "    if _needs(\"sqm_per_room\", out) and {\"size_m2\",\"rooms\"}.issubset(out.columns):\n",
        "        out[\"sqm_per_room\"] = size / rooms.replace(0, np.nan)\n",
        "\n",
        "    if _needs(\"baths_per_100sqm\", out) and {\"bathrooms\",\"size_m2\"}.issubset(out.columns):\n",
        "        out[\"baths_per_100sqm\"] = 100.0 * baths / size.replace(0, np.nan)\n",
        "\n",
        "    if _needs(\"elev_x_floor\", out) and {\"has_elevator\",\"floor\"}.issubset(out.columns):\n",
        "        out[\"elev_x_floor\"] = (elev * np.maximum(floor - 1, 0)).astype(\"float64\")\n",
        "\n",
        "    if _needs(\"no_elev_high_floor\", out) and {\"has_elevator\",\"floor\"}.issubset(out.columns):\n",
        "        out[\"no_elev_high_floor\"] = ((1 - elev) * np.maximum(floor - 1, 0)).astype(\"float64\")\n",
        "\n",
        "    if _needs(\"rooms_per_100sqm\", out) and {\"rooms\",\"size_m2\"}.issubset(out.columns):\n",
        "        out[\"rooms_per_100sqm\"] = (100.0 * rooms / size.replace(0, np.nan)).astype(\"float64\")\n",
        "\n",
        "    # 2) priors city×zone e macroarea\n",
        "    if _needs(\"city_zone_prior\", out):\n",
        "        if \"city\" not in out.columns and \"location\" in out.columns:\n",
        "            out[\"city\"] = out[\"location\"]\n",
        "        if \"zone\" not in out.columns:\n",
        "            out[\"zone\"] = \"semi_center\"\n",
        "        ci = out.get(\"city\").astype(str).str.strip().str.lower() if \"city\" in out.columns else pd.Series(\"\", index=out.index)\n",
        "        zo = out.get(\"zone\").astype(str).str.strip().str.lower() if \"zone\" in out.columns else pd.Series(\"semi_center\", index=out.index)\n",
        "        vals = []\n",
        "        for c, z in zip(ci, zo):\n",
        "            v = _CITY_BASE.get(c, {}).get(z, np.nan)\n",
        "            if pd.isna(v):\n",
        "                v = _ZONE_MED.get(z, _GLOBAL_CITYZONE_MED)\n",
        "            vals.append(v)\n",
        "        out[\"city_zone_prior\"] = np.asarray(vals, dtype=\"float64\")\n",
        "\n",
        "    if _needs(\"region_index_prior\", out):\n",
        "        if \"region\" not in out.columns:\n",
        "            out[\"region\"] = \"center\"\n",
        "        out[\"region_index_prior\"] = out[\"region\"].astype(str).str.strip().str.lower().map(_REGION_INDEX).astype(\"float64\")\n",
        "\n",
        "    return out\n",
        "\n",
        "# applica solo se davvero servono (evita side-effect inutili)\n",
        "for _name in (\"df_train\",\"df_valid\",\"df_test\"):\n",
        "    if _name in globals() and isinstance(globals()[_name], pd.DataFrame):\n",
        "        globals()[_name] = _ensure_eval_derivatives(globals()[_name])\n",
        "\n",
        "# riallinea le matrici usate in queste celle di valutazione\n",
        "if \"features_A\" in globals():\n",
        "    X_train = _ensure_columns(df_train.copy(), features_A) if \"df_train\" in globals() else X_train\n",
        "    X_valid = _ensure_columns(df_valid.copy(), features_A) if \"df_valid\" in globals() else X_valid\n",
        "    X_test  = _ensure_columns(df_test.copy(),  features_A) if \"df_test\"  in globals() else X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0e4b3f3d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Leakage sentinel (VALID, shuffled y)  MAE=228.67  RMSE=310.42  R2=-0.1219\n",
            "Colonne proxy sospette in uso: —\n"
          ]
        }
      ],
      "source": [
        "# === Leakage sentinel + scan proxy (TTR, target in scala naturale) ===\n",
        "from __future__ import annotations\n",
        "import re, numpy as np\n",
        "from sklearn.base import clone\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "def _report(y_true, y_pred, tag: str):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    print(f\"{tag}  MAE={mae:.2f}  RMSE={rmse:.2f}  R2={r2:.4f}\")\n",
        "\n",
        "# y (scala naturale) per VALID — fallback se non definito\n",
        "try:\n",
        "    y_val_orig\n",
        "except NameError:\n",
        "    y_val_orig = df_valid.loc[X_valid.index, VALUATION_K].to_numpy(dtype=np.float64)\n",
        "\n",
        "# 1) Sentinel: shuffle del target in scala naturale\n",
        "y_train_shuf = shuffle(y_train, random_state=SEED).astype(np.float64)\n",
        "\n",
        "# ricostruisci estimatore coerente con la prep\n",
        "try:\n",
        "    base_model = clone(model_A)\n",
        "except Exception:\n",
        "    base_model = RandomForestRegressor(n_estimators=200, max_depth=None,\n",
        "                                       min_samples_leaf=2, random_state=SEED, n_jobs=-1)\n",
        "\n",
        "inner_pipe = Pipeline([(\"prep\", preproc_A), (\"model\", base_model)])\n",
        "\n",
        "def _log1p64(y):  return np.log1p(np.asarray(y, dtype=np.float64))\n",
        "def _expm164(y):  return np.expm1(np.asarray(y, dtype=np.float64))\n",
        "\n",
        "sentinel_reg = TransformedTargetRegressor(\n",
        "    regressor=inner_pipe,\n",
        "    func=_log1p64,\n",
        "    inverse_func=_expm164,\n",
        "    check_inverse=False,\n",
        ")\n",
        "\n",
        "fit_params = {}\n",
        "if \"w_train\" in globals() and isinstance(w_train, np.ndarray) and len(w_train) == len(X_train):\n",
        "    # ✅ chiave relativa alla Pipeline interna (step \"model\")\n",
        "    fit_params = {\"model__sample_weight\": w_train}\n",
        "\n",
        "sentinel_reg.fit(X_train, y_train_shuf, **fit_params)\n",
        "pred_val_shuf = np.clip(sentinel_reg.predict(X_valid), 0, None)\n",
        "_report(y_val_orig, pred_val_shuf, \"Leakage sentinel (VALID, shuffled y)\")\n",
        "\n",
        "# 2) Scan colonne proxy sospette nelle features in uso\n",
        "sus_patterns = [\n",
        "    r\"(?:^|_)(avg|mean|median|benchmark|zscore|rank|decile)(?:_|$).*?(price|valuation)\",\n",
        "    r\"(price|valuation).*?(avg|mean|median|benchmark|zscore|rank|decile)\",\n",
        "    r\"(?:^|_)drift(?:_|$)\", r\"(?:^|_)caps?(?:_|$)\", r\"(?:^|_)vs_(?:_|$)\"\n",
        "]\n",
        "def _is_susp(c: str) -> bool:\n",
        "    return any(re.search(p, c, re.I) for p in sus_patterns)\n",
        "\n",
        "in_use_cols = list(X_train.columns)\n",
        "print(\"Colonne proxy sospette in uso:\", [c for c in in_use_cols if _is_susp(c)] or \"—\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b5e232fd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SAFE] usando subset train: 210/10500 righe\n",
            "[pre_fn] IN: shape=(210, 26) mem=0.1 MB\n",
            "[pre_fn] after fill_priors: shape=(210, 26) mem=0.1 MB\n",
            "[pre_fn] after ensure_columns(26): shape=(210, 26) mem=0.1 MB\n"
          ]
        }
      ],
      "source": [
        "# === PATCH CV: pre-trasformazione robusta + fast pipe clone-safe (SAFE) ======\n",
        "from __future__ import annotations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import clone\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# --- parametri \"safe\"\n",
        "FRAC = 0.02            # usa ~2% del train (molto prudente)\n",
        "N_MAX = 5000           # oppure massimo N_MAX righe\n",
        "USE_DERIVE = False     # True per riattivare derive.transform(X) quando vuoi testarlo\n",
        "\n",
        "def _mem_mb(df: pd.DataFrame) -> float:\n",
        "    try:\n",
        "        return float(df.memory_usage(deep=True).sum()) / (1024**2)\n",
        "    except Exception:\n",
        "        return -1.0\n",
        "\n",
        "# 1) mapping priors (robusto ai missing)\n",
        "try:\n",
        "    from shared.common.config import ASSET_CONFIG  # type: ignore\n",
        "    _PROP = ASSET_CONFIG[\"property\"]\n",
        "    _CITY_BASE = {c.lower(): {z.lower(): float(v) for z, v in d.items()}\n",
        "                  for c, d in (_PROP.get(\"city_base_prices\") or {}).items()}\n",
        "    _REGION_INDEX = {k.lower(): float(v) for k, v in (_PROP.get(\"region_index\") or {\n",
        "        \"north\": 1.05, \"center\": 1.00, \"south\": 0.92\n",
        "    }).items()}\n",
        "except Exception:\n",
        "    _CITY_BASE = {}\n",
        "    _REGION_INDEX = {\"north\": 1.05, \"center\": 1.00, \"south\": 0.92}\n",
        "\n",
        "# mediane di fallback\n",
        "_ZONE_KEYS = set(z for d in _CITY_BASE.values() for z in d.keys())\n",
        "_ZONE_MED = {z: float(np.nanmedian([d.get(z, np.nan) for d in _CITY_BASE.values()]))\n",
        "             for z in _ZONE_KEYS} if _CITY_BASE else {}\n",
        "_GLOBAL_CITYZONE_MED = float(np.nanmedian([v for d in _CITY_BASE.values() for v in d.values()])) if _CITY_BASE else 0.0\n",
        "\n",
        "def _required_cols_from_prep(prep: \"ColumnTransformer\") -> list[str]:\n",
        "    req: list[str] = []\n",
        "    for _, _, cols in getattr(prep, \"transformers\", []):\n",
        "        if cols is None or cols == \"drop\":\n",
        "            continue\n",
        "        if isinstance(cols, (list, tuple, np.ndarray)):\n",
        "            req.extend([str(c) for c in cols])\n",
        "        else:\n",
        "            req.append(str(cols))\n",
        "    # dedup preservando ordine\n",
        "    seen, out = set(), []\n",
        "    for c in req:\n",
        "        if c not in seen:\n",
        "            seen.add(c)\n",
        "            out.append(c)\n",
        "    return out\n",
        "\n",
        "def _ensure_columns(df_part: pd.DataFrame, required: list[str]) -> pd.DataFrame:\n",
        "    miss = [c for c in required if c not in df_part.columns]\n",
        "    if miss:\n",
        "        for c in miss:\n",
        "            df_part[c] = np.nan\n",
        "    return df_part[required]\n",
        "\n",
        "def _fill_priors_for_cv(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "\n",
        "    # garantisci basi\n",
        "    if \"city\" not in out.columns and \"location\" in out.columns:\n",
        "        out[\"city\"] = out[\"location\"]\n",
        "    if \"zone\" not in out.columns:\n",
        "        out[\"zone\"] = \"semi_center\"\n",
        "    if \"region\" not in out.columns:\n",
        "        out[\"region\"] = \"center\"\n",
        "\n",
        "    # canonizza stringhe\n",
        "    for c in (\"city\", \"zone\", \"region\"):\n",
        "        if c in out.columns:\n",
        "            out[c] = out[c].astype(str).str.strip().str.lower()\n",
        "\n",
        "    # city_zone_prior\n",
        "    if \"city_zone_prior\" not in out.columns or pd.isna(out[\"city_zone_prior\"]).all():\n",
        "        ci = out.get(\"city\", pd.Series(index=out.index, dtype=str)).astype(str)\n",
        "        zo = out.get(\"zone\", pd.Series(index=out.index, dtype=str)).astype(str)\n",
        "        vals = []\n",
        "        for cc, zz in zip(ci, zo):\n",
        "            v = _CITY_BASE.get(cc, {}).get(zz, np.nan)\n",
        "            if pd.isna(v):\n",
        "                v = _ZONE_MED.get(zz, _GLOBAL_CITYZONE_MED)\n",
        "            vals.append(v)\n",
        "        out[\"city_zone_prior\"] = np.asarray(vals, dtype=\"float64\")\n",
        "    else:\n",
        "        out[\"city_zone_prior\"] = pd.to_numeric(out[\"city_zone_prior\"], errors=\"coerce\").astype(\"float64\")\n",
        "\n",
        "    # region_index_prior\n",
        "    if \"region_index_prior\" not in out.columns or pd.isna(out[\"region_index_prior\"]).all():\n",
        "        out[\"region_index_prior\"] = out[\"region\"].map(_REGION_INDEX).astype(\"float64\")\n",
        "    else:\n",
        "        out[\"region_index_prior\"] = pd.to_numeric(out[\"region_index_prior\"], errors=\"coerce\").astype(\"float64\")\n",
        "\n",
        "    # fallback finale\n",
        "    if pd.isna(out[\"city_zone_prior\"]).all():\n",
        "        out[\"city_zone_prior\"] = float(_GLOBAL_CITYZONE_MED)\n",
        "    if pd.isna(out[\"region_index_prior\"]).all():\n",
        "        out[\"region_index_prior\"] = float(np.nanmean(list(_REGION_INDEX.values())))\n",
        "\n",
        "    return out\n",
        "\n",
        "def _build_fast_pipe_and_prefn(base_pipe: Pipeline):\n",
        "    steps = getattr(base_pipe, \"named_steps\", {})\n",
        "    prep = steps.get(\"prep\", None)\n",
        "    reg  = steps.get(\"rf\", steps.get(\"model\", None))\n",
        "    derive = steps.get(\"derive\", None)\n",
        "    if prep is None or reg is None:\n",
        "        raise RuntimeError(\"Pipeline base priva di 'prep' o step finale (rf/model).\")\n",
        "\n",
        "    # clone \"safe\"\n",
        "    try:\n",
        "        prep_fast = clone(prep)\n",
        "    except Exception:\n",
        "        prep_fast = prep\n",
        "    try:\n",
        "        reg_fast = clone(reg)\n",
        "    except Exception:\n",
        "        from sklearn.ensemble import RandomForestRegressor\n",
        "        # n_jobs=1 per non saturare CPU / RAM durante debug\n",
        "        reg_fast = RandomForestRegressor(n_estimators=200, random_state=SEED, n_jobs=1, min_samples_leaf=2)\n",
        "\n",
        "    last_name = \"rf\" if \"rf\" in steps else (\"model\" if \"model\" in steps else \"est\")\n",
        "    fast_pipe = Pipeline([(\"prep\", prep_fast), (last_name, reg_fast)])\n",
        "\n",
        "    required = _required_cols_from_prep(prep)\n",
        "\n",
        "    def pre_fn(Xdf: pd.DataFrame) -> pd.DataFrame:\n",
        "        X2 = Xdf.copy()\n",
        "        print(f\"[pre_fn] IN: shape={X2.shape} mem={_mem_mb(X2):.1f} MB\")\n",
        "        if USE_DERIVE and derive is not None and hasattr(derive, \"transform\"):\n",
        "            try:\n",
        "                X2 = derive.transform(X2)\n",
        "                print(f\"[pre_fn] after derive.transform: shape={X2.shape} mem={_mem_mb(X2):.1f} MB\")\n",
        "            except MemoryError:\n",
        "                print(\"[pre_fn] MemoryError in derive.transform → skipping derive for questo run\")\n",
        "            except Exception as e:\n",
        "                print(f\"[pre_fn] WARN derive.transform failed: {e} → skipping\")\n",
        "        X2 = _fill_priors_for_cv(X2)\n",
        "        print(f\"[pre_fn] after fill_priors: shape={X2.shape} mem={_mem_mb(X2):.1f} MB\")\n",
        "        X2 = _ensure_columns(X2, required)\n",
        "        print(f\"[pre_fn] after ensure_columns({len(required)}): shape={X2.shape} mem={_mem_mb(X2):.1f} MB\")\n",
        "        return X2\n",
        "\n",
        "    return fast_pipe, pre_fn\n",
        "\n",
        "# costruisci fast pipe + funzione di pre-trasformazione\n",
        "fast_pipe, _pre_fn = _build_fast_pipe_and_prefn(pipe_A)\n",
        "\n",
        "# subset \"safe\" prima della pre-trasformazione (deterministico, niente shuffle)\n",
        "n_total = len(X_train)\n",
        "take = int(min(n_total, max(1, int(FRAC * n_total)))) if FRAC < 1.0 else n_total\n",
        "if N_MAX is not None:\n",
        "    take = min(take, int(N_MAX))\n",
        "X_train_safe = X_train.head(take).copy()\n",
        "print(f\"[SAFE] usando subset train: {take}/{n_total} righe\")\n",
        "\n",
        "# pre-trasforma X per la CV (safe)\n",
        "X_train_cv = _pre_fn(X_train_safe)\n",
        "\n",
        "# pesi (se presenti) allineati allo stesso subset\n",
        "w_full = None\n",
        "if \"sample_weight\" in df_train.columns:\n",
        "    w_full = df_train.loc[X_train_safe.index, \"sample_weight\"].astype(\"float64\").to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "776b03be",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # === Group-aware CV: GroupShuffleSplit 5× e LOLO Top-K (TTR, NO expm1 manuale) ===\n",
        "# from __future__ import annotations\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import GroupShuffleSplit, LeaveOneGroupOut\n",
        "# from sklearn.base import clone\n",
        "# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "# from sklearn.compose import TransformedTargetRegressor\n",
        "\n",
        "# # 0) Wrapper numericamente stabili per TTR\n",
        "# def _log1p64(y):  return np.log1p(np.asarray(y, dtype=np.float64))\n",
        "# def _expm164(y):  return np.expm1(np.asarray(y, dtype=np.float64))\n",
        "\n",
        "# # 1) Scegli colonna di gruppo\n",
        "# GROUP_CANDIDATES = [\"location\", \"region\", \"zone\", \"urban_type\"]\n",
        "# group_col = next((c for c in GROUP_CANDIDATES if c in df_train.columns), None)\n",
        "\n",
        "# if group_col is None:\n",
        "#     print(\"⛔ Nessuna colonna di gruppo disponibile (location/region/zone/urban_type). Salto GSS/LOLO.\")\n",
        "# else:\n",
        "#     # y/weights/groups ALLINEATI a X_train_cv\n",
        "#     # Se hai già y_train come Serie/ndarray full, ricavalo dall'indice del df sorgente:\n",
        "#     y_train_nat = df_train.loc[X_train_cv.index, VALUATION_K].to_numpy(dtype=np.float64)\n",
        "#     groups_full = df_train.loc[X_train_cv.index, group_col].astype(str).to_numpy()\n",
        "\n",
        "#     w_full = None\n",
        "#     if \"sample_weight\" in df_train.columns:\n",
        "#         w_full = df_train.loc[X_train_cv.index, \"sample_weight\"].astype(\"float64\").to_numpy()\n",
        "\n",
        "#     # helper: fit+metriche su uno split (usa TTR e passa pesi allo step finale della Pipeline)\n",
        "#     def _fit_eval_on_split(X, y, tr_idx, va_idx, base_pipe, w: np.ndarray | None):\n",
        "#         inner = clone(base_pipe)\n",
        "#         reg = TransformedTargetRegressor(\n",
        "#             regressor=inner,\n",
        "#             func=_log1p64,\n",
        "#             inverse_func=_expm164,\n",
        "#             check_inverse=False,\n",
        "#         )\n",
        "#         # individua nome step finale che accetta sample_weight\n",
        "#         if \"rf\" in inner.named_steps:\n",
        "#             last_step = \"rf\"\n",
        "#         elif \"model\" in inner.named_steps:\n",
        "#             last_step = \"model\"\n",
        "#         else:\n",
        "#             last_step = list(inner.named_steps.keys())[-1]\n",
        "\n",
        "#         fit_params = {}\n",
        "#         if w is not None and len(w) == len(X):\n",
        "#             fit_params = {f\"{last_step}__sample_weight\": w[tr_idx]}\n",
        "\n",
        "#         reg.fit(X.iloc[tr_idx], y[tr_idx], **fit_params)\n",
        "#         pred = np.clip(reg.predict(X.iloc[va_idx]), 0, None)  # già in k€\n",
        "#         true = y[va_idx]\n",
        "\n",
        "#         mae = mean_absolute_error(true, pred)\n",
        "#         rmse = np.sqrt(mean_squared_error(true, pred))\n",
        "#         r2 = r2_score(true, pred)\n",
        "#         return mae, rmse, r2\n",
        "\n",
        "#     # per non saturare\n",
        "#     if \"rf\" in fast_pipe.named_steps:\n",
        "#         fast_pipe.named_steps[\"rf\"].set_params(n_estimators=200)\n",
        "\n",
        "#     # --- A) GroupShuffleSplit 5× (tutto sul subset X_train_cv)\n",
        "#     gss = GroupShuffleSplit(n_splits=5, test_size=0.2, random_state=SEED)\n",
        "#     maeL, rmseL, r2L = [], [], []\n",
        "#     for tr_idx, va_idx in gss.split(X_train_cv, y_train_nat, groups=groups_full):\n",
        "#         mae, rmse, r2 = _fit_eval_on_split(X_train_cv, y_train_nat, tr_idx, va_idx, fast_pipe, w_full)\n",
        "#         maeL.append(mae); rmseL.append(rmse); r2L.append(r2)\n",
        "\n",
        "#     print(\n",
        "#         f\"GroupShuffleSplit (5×, group={group_col}) → \"\n",
        "#         f\"MAE={np.mean(maeL):.2f}±{np.std(maeL):.2f}  \"\n",
        "#         f\"RMSE={np.mean(rmseL):.2f}±{np.std(rmseL):.2f}  \"\n",
        "#         f\"R2={np.mean(r2L):.4f}±{np.std(r2L):.4f}\"\n",
        "#     )\n",
        "\n",
        "#     # --- B) LOLO Top-K (sul subset coerente)\n",
        "#     K = 10\n",
        "#     vc = pd.Series(groups_full).value_counts()\n",
        "#     top_groups = vc.index[:min(K, len(vc))]\n",
        "#     mask = pd.Series(groups_full).isin(top_groups).to_numpy()\n",
        "\n",
        "#     # usa SEMPRE il subset pretrasformato\n",
        "#     Xk = X_train_cv.loc[X_train_cv.index[mask]]\n",
        "#     yk = y_train_nat[mask]\n",
        "#     gk = groups_full[mask]\n",
        "#     wk = None if w_full is None else w_full[mask]\n",
        "\n",
        "#     if len(np.unique(gk)) < 2 or len(Xk) < 10:\n",
        "#         print(\"LOLO Top-K: gruppi insufficienti. Salto.\")\n",
        "#     else:\n",
        "#         logo = LeaveOneGroupOut()\n",
        "#         maeL2, rmseL2, r2L2 = [], [], []\n",
        "#         fast_pipe2 = clone(fast_pipe)\n",
        "\n",
        "#         for tr_idx, va_idx in logo.split(Xk, yk, groups=gk):\n",
        "#             mae, rmse, r2 = _fit_eval_on_split(Xk, yk, tr_idx, va_idx, fast_pipe2, wk)\n",
        "#             maeL2.append(mae); rmseL2.append(rmse); r2L2.append(r2)\n",
        "\n",
        "#         print(\n",
        "#             f\"LOLO Top-{len(np.unique(gk))} (group={group_col}) → \"\n",
        "#             f\"MAE={np.mean(maeL2):.2f}±{np.std(maeL2):.2f}  \"\n",
        "#             f\"RMSE={np.mean(rmseL2):.2f}±{np.std(rmseL2):.2f}  \"\n",
        "#             f\"R2={np.mean(r2L2):.4f}±{np.std(r2L2):.4f}\"\n",
        "#         )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "29ef33fc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Minimal VALID → MAE=71.31  RMSE=101.43  R2=0.880219  (features: 21)\n"
          ]
        }
      ],
      "source": [
        "# === Minimal features (TTR, chiavi fit_params corrette) ===\n",
        "from __future__ import annotations\n",
        "import numpy as np\n",
        "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# wrapper numericamente stabili\n",
        "def _log1p64(y):  return np.log1p(np.asarray(y, dtype=np.float64))\n",
        "def _expm164(y):  return np.expm1(np.asarray(y, dtype=np.float64))\n",
        "\n",
        "base_keep_all = [\n",
        "    \"size_m2\",\"rooms\",\"bathrooms\",\"year_built\",\"age_years\",\n",
        "    \"floor\",\"building_floors\",\"is_top_floor\",\"is_ground_floor\",\"has_elevator\",\n",
        "    \"garage\",\"parking_spot\",\"has_garden\",\"has_balcony\",\n",
        "    \"distance_to_center_km\",\"orientation\",\"view\",\"region\",\"zone\",\"urban_type\",\"location\"\n",
        "]\n",
        "base_keep = [c for c in base_keep_all if c in df_train.columns]\n",
        "\n",
        "if len(base_keep) < 2:\n",
        "    print(\"⛔ Minimal: meno di 2 feature base disponibili.\")\n",
        "else:\n",
        "    # split cat/num basati su dtype nel TRAIN\n",
        "    cat_b = [c for c in base_keep if df_train[c].dtype.name in (\"object\",\"category\",\"bool\")]\n",
        "    num_b = [c for c in base_keep if c not in set(cat_b)]\n",
        "\n",
        "    prep_b = ColumnTransformer([\n",
        "        (\"cat\", Pipeline([\n",
        "            (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "            (\"enc\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
        "        ]), cat_b),\n",
        "        (\"num\", Pipeline([\n",
        "            (\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "        ]), num_b),\n",
        "    ], remainder=\"drop\")\n",
        "\n",
        "    pipe_inner = Pipeline([\n",
        "        (\"prep\", prep_b),\n",
        "        (\"rf\", RandomForestRegressor(\n",
        "            n_estimators=200, random_state=SEED, n_jobs=-1, min_samples_leaf=2\n",
        "        )),\n",
        "    ])\n",
        "\n",
        "    # TTR: log1p/expm1 gestiti qui, niente expm1 manuale\n",
        "    pipe_min = TransformedTargetRegressor(\n",
        "        regressor=pipe_inner,\n",
        "        func=_log1p64, inverse_func=_expm164,\n",
        "        check_inverse=False,\n",
        "    )\n",
        "\n",
        "    # allinea colonne\n",
        "    def _ensure(df_part, cols):\n",
        "        miss = [c for c in cols if c not in df_part.columns]\n",
        "        if miss:\n",
        "            for c in miss: df_part[c] = np.nan\n",
        "        return df_part[cols]\n",
        "\n",
        "    Xtr_min = _ensure(df_train.copy(), base_keep).loc[X_train.index]\n",
        "    Xva_min = _ensure(df_valid.copy(), base_keep).loc[X_valid.index]\n",
        "\n",
        "    # y in scala naturale (k€)\n",
        "    y_tr_nat = df_train.loc[Xtr_min.index, VALUATION_K].astype(\"float64\").to_numpy()\n",
        "    y_va_nat = df_valid.loc[Xva_min.index, VALUATION_K].astype(\"float64\").to_numpy()\n",
        "\n",
        "    # fit_params: **niente 'regressor__'**\n",
        "    fit_params = {}\n",
        "    if \"sample_weight\" in df_train.columns:\n",
        "        w = df_train.loc[Xtr_min.index, \"sample_weight\"].astype(\"float64\").to_numpy()\n",
        "        fit_params = {\"rf__sample_weight\": w}\n",
        "\n",
        "    pipe_min.fit(Xtr_min, y_tr_nat, **fit_params)\n",
        "    pred_min = np.clip(pipe_min.predict(Xva_min), 0, None)  # già k€\n",
        "\n",
        "    mae = mean_absolute_error(y_va_nat, pred_min)\n",
        "    rmse = np.sqrt(mean_squared_error(y_va_nat, pred_min))\n",
        "    r2 = r2_score(y_va_nat, pred_min)\n",
        "\n",
        "    print(f\"Minimal VALID → MAE={mae:.2f}  RMSE={rmse:.2f}  R2={r2:.6f}  (features: {len(base_keep)})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "2ebecbf5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DROP ['distance_to_center_km', 'size_m2'] → VALID  MAE=89.62  RMSE=129.24  R2=0.8055  (kept 25 cols)\n",
            "DROP ['size_m2'] → VALID  MAE=89.62  RMSE=129.24  R2=0.8055  (kept 25 cols)\n",
            "DROP ['distance_to_center_km'] → VALID  MAE=89.56  RMSE=129.21  R2=0.8056  (kept 26 cols)\n",
            "GSS 3× (drop size_m2 & distance_to_center_km) → MAE=76.75±15.00  RMSE=105.66±15.98  R2=0.7895±0.0665\n"
          ]
        }
      ],
      "source": [
        "# === Ablation sicura: rimuovi alcune colonne e valuta su VALID (e opz. GSS) ===\n",
        "from __future__ import annotations\n",
        "import numpy as np\n",
        "from sklearn.base import clone\n",
        "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "# -- wrapper numericamente stabili (se non già definiti)\n",
        "try:\n",
        "    _log1p64\n",
        "    _expm164\n",
        "except NameError:\n",
        "    def _log1p64(y):  return np.log1p(np.asarray(y, dtype=np.float64))\n",
        "    def _expm164(y):  return np.expm1(np.asarray(y, dtype=np.float64))\n",
        "\n",
        "def _build_preproc_from(base_prep: ColumnTransformer,\n",
        "                        keep_cat: list[str],\n",
        "                        keep_num: list[str]) -> ColumnTransformer:\n",
        "    \"\"\"\n",
        "    Ricostruisce un ColumnTransformer usando (se disponibili) i trasformatori\n",
        "    del prep di base, altrimenti crea pipe di default.\n",
        "    \"\"\"\n",
        "    cat_est, num_est = None, None\n",
        "    if hasattr(base_prep, \"transformers\"):\n",
        "        for name, est, cols in base_prep.transformers:\n",
        "            if name == \"cat\":\n",
        "                cat_est = clone(est)\n",
        "            elif name == \"num\":\n",
        "                num_est = clone(est)\n",
        "\n",
        "    if cat_est is None:\n",
        "        cat_est = Pipeline([\n",
        "            (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "            (\"encode\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
        "        ])\n",
        "    if num_est is None:\n",
        "        num_est = Pipeline([\n",
        "            (\"impute\", SimpleImputer(strategy=\"median\")),\n",
        "        ])\n",
        "\n",
        "    transformers = []\n",
        "    if keep_cat:\n",
        "        transformers.append((\"cat\", cat_est, keep_cat))\n",
        "    if keep_num:\n",
        "        transformers.append((\"num\", num_est, keep_num))\n",
        "    if not transformers:\n",
        "        raise ValueError(\"Nessuna feature rimanente per l'ablation.\")\n",
        "\n",
        "    return ColumnTransformer(transformers=transformers, remainder=\"drop\")\n",
        "\n",
        "def _ensure(df_part, cols):\n",
        "    \"\"\"Garantisce che tutte le colonne esistano (aggiunge NaN se mancano) e restituisce solo quelle in 'cols'.\"\"\"\n",
        "    miss = [c for c in cols if c not in df_part.columns]\n",
        "    if miss:\n",
        "        for c in miss: df_part[c] = np.nan\n",
        "    return df_part[cols]\n",
        "\n",
        "def _make_fit_params_for_ttr_regressor(pipe_base: Pipeline, mask=None):\n",
        "    \"\"\"\n",
        "    Crea il dict dei fit_params per passare i pesi allo step finale della Pipeline interna.\n",
        "    NOTA: con TTR NON usare 'regressor__' qui; le chiavi vanno direttamente allo step (rf/model).\n",
        "    \"\"\"\n",
        "    if (\"sample_weight\" in df_train.columns) and hasattr(pipe_base, \"named_steps\"):\n",
        "        if \"rf\" in pipe_base.named_steps:\n",
        "            key = \"rf__sample_weight\"\n",
        "        elif \"model\" in pipe_base.named_steps:\n",
        "            key = \"model__sample_weight\"\n",
        "        else:\n",
        "            last = list(pipe_base.named_steps.keys())[-1]\n",
        "            key = f\"{last}__sample_weight\"\n",
        "\n",
        "        w = df_train.loc[X_train.index, \"sample_weight\"].astype(\"float64\").to_numpy()\n",
        "        if mask is not None:\n",
        "            w = w[mask]\n",
        "        return {key: w}\n",
        "    return {}\n",
        "\n",
        "def eval_drop(cols_to_drop, base_pipe=pipe_A):\n",
        "    \"\"\"\n",
        "    Esegue un’ablation (drop di alcune colonne) e valuta su VALID usando TTR (log1p/expm1).\n",
        "    Ritorna (ttr_pipeline, keep_features).\n",
        "    \"\"\"\n",
        "    cols_to_drop = set(cols_to_drop)\n",
        "    keep = [c for c in features_A if c not in cols_to_drop]\n",
        "    keep_cat = [c for c in cat_cols if c in keep]\n",
        "    keep_num = [c for c in num_cols if c in keep]\n",
        "\n",
        "    # ricostruisci preproc coerente\n",
        "    preproc_new = _build_preproc_from(base_pipe.named_steps[\"prep\"], keep_cat, keep_num)\n",
        "\n",
        "    # modello base\n",
        "    rf = RandomForestRegressor(\n",
        "        n_estimators=200, random_state=SEED, n_jobs=-1, max_depth=None, min_samples_leaf=2\n",
        "    )\n",
        "    pipe_base = Pipeline([(\"prep\", preproc_new), (\"rf\", rf)])\n",
        "\n",
        "    # TTR per gestire scala target in modo consistente\n",
        "    ttr = TransformedTargetRegressor(\n",
        "        regressor=pipe_base,\n",
        "        func=_log1p64,\n",
        "        inverse_func=_expm164,\n",
        "        check_inverse=False,\n",
        "    )\n",
        "\n",
        "    # prepara matrici e target (scala naturale k€)\n",
        "    Xtr = _ensure(df_train.copy(), keep).loc[X_train.index]\n",
        "    Xva = _ensure(df_valid.copy(), keep).loc[X_valid.index]\n",
        "    y_tr_nat = df_train.loc[Xtr.index, VALUATION_K].astype(\"float64\").to_numpy()\n",
        "    y_va_nat = df_valid.loc[Xva.index, VALUATION_K].astype(\"float64\").to_numpy()\n",
        "\n",
        "    # fit con pesi (se presenti)\n",
        "    fit_params = _make_fit_params_for_ttr_regressor(pipe_base)\n",
        "    ttr.fit(Xtr, y_tr_nat, **fit_params)\n",
        "\n",
        "    # predizioni già in scala naturale\n",
        "    predV = np.clip(ttr.predict(Xva), 0, None)\n",
        "    mae = mean_absolute_error(y_va_nat, predV)\n",
        "    rmse = np.sqrt(mean_squared_error(y_va_nat, predV))\n",
        "    r2 = r2_score(y_va_nat, predV)\n",
        "\n",
        "    print(f\"DROP {sorted(cols_to_drop)} → VALID  MAE={mae:.2f}  RMSE={rmse:.2f}  R2={r2:.4f}  (kept {len(keep)} cols)\")\n",
        "    return ttr, keep\n",
        "\n",
        "# Esempi di uso (come prima)\n",
        "p_drop_both, keep_cols_both = eval_drop([\"size_m2\", \"distance_to_center_km\"])\n",
        "p_drop_size, keep_cols_size = eval_drop([\"size_m2\"])\n",
        "p_drop_dist, keep_cols_dist = eval_drop([\"distance_to_center_km\"])\n",
        "\n",
        "# --- GSS rapido con set ridotto (3 split) per il modello p_drop_both ---\n",
        "GROUP_CANDIDATES = [\"location\",\"region\",\"zone\",\"urban_type\"]\n",
        "group_col = next((c for c in GROUP_CANDIDATES if c in df_train.columns), None)\n",
        "\n",
        "if group_col:\n",
        "    # gruppi allineati a X_train\n",
        "    groups = df_train.loc[X_train.index, group_col].astype(str).to_numpy()\n",
        "\n",
        "    # ricostruisci una versione \"fast\" della pipeline interna con meno alberi\n",
        "    fast_inner = clone(p_drop_both.regressor)   # Pipeline(prep, rf)\n",
        "    if \"rf\" in fast_inner.named_steps:\n",
        "        fast_inner.named_steps[\"rf\"].set_params(n_estimators=200)\n",
        "\n",
        "    # TTR fast\n",
        "    ttr_fast = TransformedTargetRegressor(\n",
        "        regressor=fast_inner,\n",
        "        func=_log1p64,\n",
        "        inverse_func=_expm164,\n",
        "        check_inverse=False,\n",
        "    )\n",
        "\n",
        "    # prepara X e y in scala naturale per i fold\n",
        "    X_full = _ensure(df_train.copy(), keep_cols_both).loc[X_train.index]\n",
        "    y_full = df_train.loc[X_train.index, VALUATION_K].astype(\"float64\").to_numpy()\n",
        "\n",
        "    # pesi allineati se presenti\n",
        "    w_full = df_train.loc[X_train.index, \"sample_weight\"].astype(\"float64\").to_numpy() if \"sample_weight\" in df_train.columns else None\n",
        "\n",
        "    gss = GroupShuffleSplit(n_splits=3, test_size=0.2, random_state=SEED)\n",
        "    maeL, rmseL, r2L = [], [], []\n",
        "\n",
        "    for tr, va in gss.split(X_full, y_full, groups=groups):\n",
        "        # fit params per questo split (NB: chiavi relative alla Pipeline interna del TTR)\n",
        "        fit_params = {}\n",
        "        if w_full is not None:\n",
        "            # passali allo step 'rf' dentro regressor\n",
        "            fit_params = {\"rf__sample_weight\": w_full[tr]}\n",
        "\n",
        "        q = clone(ttr_fast)\n",
        "        q.fit(X_full.iloc[tr], y_full[tr], **fit_params)\n",
        "        pred = np.clip(q.predict(X_full.iloc[va]), 0, None)  # già in k€\n",
        "        true = y_full[va]\n",
        "        maeL.append(mean_absolute_error(true, pred))\n",
        "        rmseL.append(np.sqrt(mean_squared_error(true, pred)))\n",
        "        r2L.append(r2_score(true, pred))\n",
        "\n",
        "    print(\n",
        "        f\"GSS 3× (drop size_m2 & distance_to_center_km) → \"\n",
        "        f\"MAE={np.mean(maeL):.2f}±{np.std(maeL):.2f}  \"\n",
        "        f\"RMSE={np.mean(rmseL):.2f}±{np.std(rmseL):.2f}  \"\n",
        "        f\"R2={np.mean(r2L):.4f}±{np.std(r2L):.4f}\"\n",
        "    )\n",
        "else:\n",
        "    print(\"GSS: nessuna colonna di gruppo disponibile.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f10dc080",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GSS 5× (drop distance_to_center_km) → MAE=69.27±15.27  RMSE=97.81±17.07  R2=0.8334±0.0752\n"
          ]
        }
      ],
      "source": [
        "# === GSS 5× senza distance_to_center_km (robusto ai nomi step + TTR + pesi) ===\n",
        "from __future__ import annotations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.base import clone\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# wrapper numericamente stabili per TTR\n",
        "def _log1p64(y):  return np.log1p(np.asarray(y, dtype=np.float64))\n",
        "def _expm164(y):  return np.expm1(np.asarray(y, dtype=np.float64))\n",
        "\n",
        "def _extract_base_regressor(base_pipe) -> RandomForestRegressor:\n",
        "    \"\"\"Prova a clonare lo step finale del tuo pipe (rf/model), altrimenti crea un RF.\"\"\"\n",
        "    try:\n",
        "        steps = getattr(base_pipe, \"named_steps\", {})\n",
        "        if \"rf\" in steps:\n",
        "            return clone(steps[\"rf\"])\n",
        "        if \"model\" in steps:\n",
        "            return clone(steps[\"model\"])\n",
        "    except Exception:\n",
        "        pass\n",
        "    return RandomForestRegressor(\n",
        "        n_estimators=200, random_state=SEED, n_jobs=-1, min_samples_leaf=2\n",
        "    )\n",
        "\n",
        "def _fit_params_key_for(pipe_inner: Pipeline) -> str:\n",
        "    \"\"\"Ritorna la chiave corretta per passare sample_weight allo step finale.\"\"\"\n",
        "    if \"rf\" in pipe_inner.named_steps:\n",
        "        return \"rf__sample_weight\"\n",
        "    if \"model\" in pipe_inner.named_steps:\n",
        "        return \"model__sample_weight\"\n",
        "    last = list(pipe_inner.named_steps.keys())[-1]\n",
        "    return f\"{last}__sample_weight\"\n",
        "\n",
        "drop_col = \"distance_to_center_km\"\n",
        "keep_cols = [c for c in features_A if c != drop_col]\n",
        "keep_cat  = [c for c in cat_cols if c in keep_cols]\n",
        "keep_num  = [c for c in num_cols if c in keep_cols]\n",
        "\n",
        "# prep minimale per le colonne mantenute\n",
        "cat_est = Pipeline([\n",
        "    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"encode\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
        "])\n",
        "num_est = Pipeline([(\"impute\", SimpleImputer(strategy=\"median\"))])\n",
        "prep = ColumnTransformer([(\"cat\", cat_est, keep_cat), (\"num\", num_est, keep_num)], remainder=\"drop\")\n",
        "\n",
        "# regressor di base coerente col tuo pipe_A (o RF di default)\n",
        "base_reg = _extract_base_regressor(pipe_A)\n",
        "\n",
        "# pipeline interna + TTR (gestione log1p/expm1)\n",
        "inner = Pipeline([(\"prep\", prep), (\"rf\", base_reg)])\n",
        "ttr_template = TransformedTargetRegressor(\n",
        "    regressor=inner, func=_log1p64, inverse_func=_expm164, check_inverse=False\n",
        ")\n",
        "\n",
        "# allinea X, y (scala naturale, k€), pesi e gruppi agli indici di X_train\n",
        "X_keep = X_train[keep_cols]\n",
        "y_nat  = df_train.loc[X_keep.index, VALUATION_K].astype(\"float64\").to_numpy()\n",
        "w_full = None\n",
        "if \"sample_weight\" in df_train.columns:\n",
        "    w_full = df_train.loc[X_keep.index, \"sample_weight\"].astype(\"float64\").to_numpy()\n",
        "\n",
        "group_col = next((c for c in [\"location\",\"region\",\"zone\",\"urban_type\"] if c in df_train.columns), None)\n",
        "if group_col is None:\n",
        "    raise RuntimeError(\"Nessuna colonna di gruppo disponibile (location/region/zone/urban_type).\")\n",
        "groups = df_train.loc[X_keep.index, group_col].astype(str).to_numpy()\n",
        "\n",
        "# GSS 5×\n",
        "gss = GroupShuffleSplit(n_splits=5, test_size=0.2, random_state=SEED)\n",
        "maeL, rmseL, r2L = [], [], []\n",
        "\n",
        "# chiave corretta per i pesi\n",
        "fit_key = _fit_params_key_for(inner)\n",
        "\n",
        "for tr, va in gss.split(X_keep, y_nat, groups=groups):\n",
        "    ttr = clone(ttr_template)\n",
        "\n",
        "    fit_params = {}\n",
        "    if w_full is not None:\n",
        "        # ⚠️ con TTR le chiavi vanno DIRETTE allo step della pipeline interna (niente 'regressor__')\n",
        "        fit_params = {fit_key: w_full[tr]}\n",
        "\n",
        "    ttr.fit(X_keep.iloc[tr], y_nat[tr], **fit_params)\n",
        "    pred = np.clip(ttr.predict(X_keep.iloc[va]), 0, None)  # già k€\n",
        "    true = y_nat[va]\n",
        "\n",
        "    maeL.append(mean_absolute_error(true, pred))\n",
        "    rmseL.append(np.sqrt(mean_squared_error(true, pred)))\n",
        "    r2L.append(r2_score(true, pred))\n",
        "\n",
        "print(\n",
        "    f\"GSS 5× (drop {drop_col}) → \"\n",
        "    f\"MAE={np.mean(maeL):.2f}±{np.std(maeL):.2f}  \"\n",
        "    f\"RMSE={np.mean(rmseL):.2f}±{np.std(rmseL):.2f}  \"\n",
        "    f\"R2={np.mean(r2L):.4f}±{np.std(r2L):.4f}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9fdc4b9-98c5-43ac-9804-13c954b02c63",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Train & Validation (A vs B) & Champion Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "91aed31d-f657-4b2e-a37a-5bf3a588b6f3",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-09-23 02:02:48,243] INFO model_trainer: Saved RF baselines summary → c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\artifacts\\rf_baselines_summary.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A (conf as feature)  VALID: {'MAE': 89.56272530042413, 'RMSE': 129.2595490385816, 'R2': 0.805476593788474}  TEST: {'MAE': 89.21759994432595, 'RMSE': 130.06073796384024, 'R2': 0.8142825877423215}   (fit 17.45s)\n",
            "B (conf as weight)   VALID: {'MAE': 89.56272530042415, 'RMSE': 129.2595490385816, 'R2': 0.805476593788474}  TEST: {'MAE': 89.21759994432597, 'RMSE': 130.06073796384027, 'R2': 0.8142825877423214}   (fit 18.99s)\n",
            "Champion: A\n"
          ]
        }
      ],
      "source": [
        "# 06) Fit & validation (RF baseline A/B) con TTR, pesi corretti e metriche robuste — SAFE\n",
        "\n",
        "from __future__ import annotations\n",
        "import time, json, numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "def _metrics(y_true, y_pred):\n",
        "    mae  = float(mean_absolute_error(y_true, y_pred))\n",
        "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "    r2   = float(r2_score(y_true, y_pred))\n",
        "    return {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}\n",
        "\n",
        "# --- 0) Sanitize liste features: uniche e disgiunte\n",
        "def _uniq(xs):\n",
        "    return list(dict.fromkeys(xs))\n",
        "\n",
        "cat_cols = _uniq(cat_cols)\n",
        "_num_all = _uniq(num_cols)\n",
        "num_cols = [c for c in _num_all if c not in set(cat_cols)]\n",
        "num_cols_B = [c for c in num_cols if c != \"confidence_score\"]  # B: niente confidence come feature\n",
        "\n",
        "_inter = set(cat_cols) & set(num_cols)\n",
        "if _inter:\n",
        "    logger.warning(\"Colonne presenti sia in cat che num (rimosse da num): %s\", sorted(_inter))\n",
        "\n",
        "# --- 1) Preprocessori con le liste pulite\n",
        "cat_pipe = Pipeline([\n",
        "    (\"impute\", SimpleImputer(strategy=\"constant\", fill_value=\"__MISSING__\")),\n",
        "    (\"encode\", _build_ohe(min_freq)),\n",
        "])\n",
        "num_pipe = Pipeline([\n",
        "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
        "])\n",
        "\n",
        "preproc_A = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", cat_pipe, cat_cols),\n",
        "        (\"num\", num_pipe, num_cols),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        ")\n",
        "preproc_B = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", cat_pipe, cat_cols),\n",
        "        (\"num\", num_pipe, num_cols_B),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        ")\n",
        "\n",
        "# --- 2) Reallinea le colonne tra split (se mancano, NaN → imputazione)\n",
        "def _ensure_columns(df_part, required):\n",
        "    missing = [c for c in required if c not in df_part.columns]\n",
        "    if missing:\n",
        "        for c in missing:\n",
        "            df_part[c] = np.nan\n",
        "        logger.info(\"Aggiunte colonne mancanti allo split: %s\", missing)\n",
        "    return df_part[required]\n",
        "\n",
        "features_A = cat_cols + num_cols\n",
        "features_B = cat_cols + num_cols_B\n",
        "\n",
        "df_train_A = _ensure_columns(df_train.copy(), features_A)\n",
        "df_valid_A = _ensure_columns(df_valid.copy(), features_A)\n",
        "df_test_A  = _ensure_columns(df_test.copy(),  features_A)\n",
        "\n",
        "df_train_B = _ensure_columns(df_train.copy(), features_B)\n",
        "df_valid_B = _ensure_columns(df_valid.copy(), features_B)\n",
        "df_test_B  = _ensure_columns(df_test.copy(),  features_B)\n",
        "\n",
        "X_train = df_train_A[features_A].copy()\n",
        "X_valid = df_valid_A[features_A].copy()\n",
        "X_test  = df_test_A[features_A].copy()\n",
        "\n",
        "Xtr_B = df_train_B[features_B].copy()\n",
        "Xva_B = df_valid_B[features_B].copy()\n",
        "Xte_B = df_test_B[features_B].copy()\n",
        "\n",
        "# --- 3) Target in SCALA NATURALE (k€) — TTR farà log1p/expm1\n",
        "y_train_nat = df_train[VALUATION_K].astype(\"float64\").to_numpy()\n",
        "y_val_nat   = df_valid[VALUATION_K].astype(\"float64\").to_numpy()\n",
        "y_tst_nat   = df_test[VALUATION_K].astype(\"float64\").to_numpy()\n",
        "\n",
        "# --- 4) Modelli di base\n",
        "model_A = RandomForestRegressor(\n",
        "    n_estimators=400, random_state=SEED, n_jobs=-1, max_depth=None, min_samples_leaf=2\n",
        ")\n",
        "model_B = RandomForestRegressor(\n",
        "    n_estimators=400, random_state=SEED, n_jobs=-1, max_depth=None, min_samples_leaf=2\n",
        ")\n",
        "\n",
        "# Pipeline interne con step finale chiamato \"model\"\n",
        "pipe_A_inner = Pipeline([(\"prep\", preproc_A), (\"model\", model_A)])\n",
        "pipe_B_inner = Pipeline([(\"prep\", preproc_B), (\"model\", model_B)])\n",
        "\n",
        "# TTR per applicare log1p/expm1 in modo consistente\n",
        "ttr_A = TransformedTargetRegressor(regressor=pipe_A_inner, func=np.log1p, inverse_func=np.expm1)\n",
        "ttr_B = TransformedTargetRegressor(regressor=pipe_B_inner, func=np.log1p, inverse_func=np.expm1)\n",
        "\n",
        "# --- 5) Fit A (confidence come feature, nessun peso)\n",
        "t0 = time.perf_counter()\n",
        "ttr_A.fit(X_train, y_train_nat)\n",
        "tA = time.perf_counter() - t0\n",
        "\n",
        "pred_val_A = np.clip(ttr_A.predict(X_valid), 0, None)\n",
        "pred_tst_A = np.clip(ttr_A.predict(X_test),  0, None)\n",
        "\n",
        "# --- 6) Fit B (confidence esclusa come feature, usata come PESO se presente)\n",
        "fit_params_B = {}\n",
        "if \"sample_weight\" in df_train.columns:\n",
        "    w_train = df_train.loc[Xtr_B.index, \"sample_weight\"].astype(\"float64\").to_numpy()\n",
        "    # ✅ con TTR, i parametri vanno all'interno della pipeline: regressor__model__sample_weight\n",
        "    fit_params_B = {\"model__sample_weight\": w_train}  # <-- chiave relativa alla Pipeline interna\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "ttr_B.fit(Xtr_B, y_train_nat, **fit_params_B)\n",
        "tB = time.perf_counter() - t0\n",
        "\n",
        "pred_val_B = np.clip(ttr_B.predict(Xva_B), 0, None)\n",
        "pred_tst_B = np.clip(ttr_B.predict(Xte_B), 0, None)\n",
        "\n",
        "pipe_A = ttr_A.regressor_\n",
        "pipe_B = ttr_B.regressor_\n",
        "\n",
        "# ttr_champion = ttr_A if champion == \"A\" else ttr_B\n",
        "\n",
        "# --- 7) Metriche su scala naturale\n",
        "mA_val = _metrics(y_val_nat, pred_val_A)\n",
        "mA_tst = _metrics(y_tst_nat, pred_tst_A)\n",
        "mB_val = _metrics(y_val_nat, pred_val_B)\n",
        "mB_tst = _metrics(y_tst_nat, pred_tst_B)\n",
        "\n",
        "print(f\"A (conf as feature)  VALID: {mA_val}  TEST: {mA_tst}   (fit {tA:.2f}s)\")\n",
        "print(f\"B (conf as weight)   VALID: {mB_val}  TEST: {mB_tst}   (fit {tB:.2f}s)\")\n",
        "\n",
        "# --- 8) Selezione champion su VALID (MAE, tie-break RMSE)\n",
        "def _champ(mA, mB):\n",
        "    if mA[\"MAE\"] < mB[\"MAE\"]:\n",
        "        return \"A\"\n",
        "    if mA[\"MAE\"] > mB[\"MAE\"]:\n",
        "        return \"B\"\n",
        "    return \"A\" if mA[\"RMSE\"] <= mB[\"RMSE\"] else \"B\"\n",
        "\n",
        "champion = _champ(mA_val, mB_val)\n",
        "print(\"Champion:\", champion)\n",
        "\n",
        "# --- 9) Salva mini-report\n",
        "summary = {\n",
        "    \"timing_sec\": {\"A\": round(tA, 3), \"B\": round(tB, 3)},\n",
        "    \"A\": {\"VALID\": mA_val, \"TEST\": mA_tst, \"n_features\": len(features_A)},\n",
        "    \"B\": {\"VALID\": mB_val, \"TEST\": mB_tst, \"n_features\": len(features_B)},\n",
        "    \"champion\": champion,\n",
        "}\n",
        "(ART_DIR / \"rf_baselines_summary.json\").write_text(\n",
        "    canonical_json_dumps(summary), encoding=\"utf-8\"\n",
        ")\n",
        "logger.info(\"Saved RF baselines summary → %s\", ART_DIR / \"rf_baselines_summary.json\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ad3bab1",
      "metadata": {},
      "source": [
        "### (RF A/B): significatività & blending"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "53daff85",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ΔMAE (A−B) bootstrap: mean=-0.0000, 95% CI=(-0.0000,0.0000), p≈1.002\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_10316\\1057882324.py:76: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda g: pd.Series({\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\artifacts\\ab_compare_by_decile.csv , c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\artifacts\\ab_compare_by_decile.parquet\n",
            "Saved: c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\artifacts\\blend_search_AB.csv , c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\artifacts\\blend_search_AB.parquet\n",
            "Blend best α on VALID = 0.00  |  MAE_VALID=89.563  |  MAE_TEST=89.218  |  R2_TEST=0.8143\n"
          ]
        }
      ],
      "source": [
        "# 06.1) RF A vs B: bootstrap ΔMAE + breakdown per decile + blending VALID→TEST (robusto)\n",
        "from __future__ import annotations\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# === Guard-rails\n",
        "for name in [\"pred_val_A\", \"pred_val_B\", \"pred_tst_A\", \"pred_tst_B\"]:\n",
        "    if name not in globals():\n",
        "        raise RuntimeError(f\"Variabile mancante: {name}\")\n",
        "\n",
        "if len(df_test) == 0 or len(df_valid) == 0:\n",
        "    raise RuntimeError(\"Split vuoti: df_valid/df_test non possono essere vuoti.\")\n",
        "\n",
        "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _as_1d_float(x):\n",
        "    a = np.asarray(x, dtype=\"float64\").reshape(-1)\n",
        "    # sostituisci inf con nan e poi imputiamo con mediana\n",
        "    a[~np.isfinite(a)] = np.nan\n",
        "    if np.isnan(a).any():\n",
        "        med = np.nanmedian(a)\n",
        "        a = np.where(np.isnan(a), med, a)\n",
        "    return a\n",
        "\n",
        "# === Array puliti (TEST)\n",
        "y_true_t = _as_1d_float(df_test[VALUATION_K].to_numpy())\n",
        "yhatA_t  = _as_1d_float(pred_tst_A)\n",
        "yhatB_t  = _as_1d_float(pred_tst_B)\n",
        "\n",
        "# riallineo lunghezze se necessario (difetti estremi)\n",
        "n = min(len(y_true_t), len(yhatA_t), len(yhatB_t))\n",
        "y_true_t, yhatA_t, yhatB_t = y_true_t[:n], yhatA_t[:n], yhatB_t[:n]\n",
        "\n",
        "# === Paired bootstrap ΔMAE su TEST (A−B)\n",
        "B = 1000\n",
        "rng_boot = np.random.default_rng(SEED + 101)\n",
        "idx_mat = rng_boot.integers(0, n, size=(B, n))\n",
        "\n",
        "def _mae(a, b):  # veloce e robusto\n",
        "    return np.mean(np.abs(a - b))\n",
        "\n",
        "deltas = np.fromiter(\n",
        "    (_mae(y_true_t[idx], yhatA_t[idx]) - _mae(y_true_t[idx], yhatB_t[idx]) for idx in idx_mat),\n",
        "    dtype=\"float64\", count=B\n",
        ")\n",
        "\n",
        "ci_lo, ci_hi = np.percentile(deltas, [2.5, 97.5])\n",
        "# p-value 2 code rispetto a 0\n",
        "p_two_sided = 2.0 * min((deltas <= 0).mean(), (deltas >= 0).mean())\n",
        "\n",
        "print(f\"ΔMAE (A−B) bootstrap: mean={deltas.mean():.4f}, 95% CI=({ci_lo:.4f},{ci_hi:.4f}), p≈{p_two_sided:.3f}\")\n",
        "\n",
        "ab_summary = {\n",
        "    \"delta_mae_mean\": float(deltas.mean()),\n",
        "    \"ci_95\": [float(ci_lo), float(ci_hi)],\n",
        "    \"p_two_sided\": float(p_two_sided),\n",
        "    \"B\": int(B),\n",
        "    \"n_test\": int(n),\n",
        "    \"mae_A_test\": float(_mae(y_true_t, yhatA_t)),\n",
        "    \"mae_B_test\": float(_mae(y_true_t, yhatB_t)),\n",
        "}\n",
        "(ART_DIR / \"ab_bootstrap_summary.json\").write_text(canonical_json_dumps(ab_summary), encoding=\"utf-8\")\n",
        "\n",
        "# === Breakdown per decile (TEST)\n",
        "try:\n",
        "    dec = pd.qcut(y_true_t, q=10, labels=False, duplicates=\"drop\")\n",
        "except Exception:\n",
        "    dec = pd.Series(np.zeros_like(y_true_t, dtype=int))\n",
        "\n",
        "rep = (\n",
        "    pd.DataFrame({\"y\": y_true_t, \"yA\": yhatA_t, \"yB\": yhatB_t, \"dec\": dec})\n",
        "    .groupby(\"dec\", observed=True, sort=True)\n",
        "    .apply(lambda g: pd.Series({\n",
        "        \"n\": int(len(g)),\n",
        "        \"MAE_A\": float(mean_absolute_error(g[\"y\"], g[\"yA\"])),\n",
        "        \"MAE_B\": float(mean_absolute_error(g[\"y\"], g[\"yB\"])),\n",
        "        \"ΔMAE_AminusB\": float(mean_absolute_error(g[\"y\"], g[\"yA\"]) - mean_absolute_error(g[\"y\"], g[\"yB\"])),\n",
        "        \"R2_A\": float(r2_score(g[\"y\"], g[\"yA\"])) if g[\"y\"].nunique() > 1 else np.nan,\n",
        "        \"R2_B\": float(r2_score(g[\"y\"], g[\"yB\"])) if g[\"y\"].nunique() > 1 else np.nan,\n",
        "    }))\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "rep_csv  = ART_DIR / \"ab_compare_by_decile.csv\"\n",
        "rep_parq = ART_DIR / \"ab_compare_by_decile.parquet\"\n",
        "rep.to_csv(rep_csv, index=False)\n",
        "rep.to_parquet(rep_parq, index=False)\n",
        "print(\"Saved:\", rep_csv, \",\", rep_parq)\n",
        "\n",
        "# === Blending semplice α∈[0,1] su VALID → reporting anche su TEST\n",
        "y_true_v = _as_1d_float(df_valid[VALUATION_K].to_numpy())\n",
        "yhatA_v  = _as_1d_float(pred_val_A)\n",
        "yhatB_v  = _as_1d_float(pred_val_B)\n",
        "\n",
        "m = min(len(y_true_v), len(yhatA_v), len(yhatB_v))\n",
        "y_true_v, yhatA_v, yhatB_v = y_true_v[:m], yhatA_v[:m], yhatB_v[:m]\n",
        "\n",
        "alphas = np.linspace(0.0, 1.0, 21)\n",
        "rows = []\n",
        "best_idx = None\n",
        "for i, a in enumerate(alphas):\n",
        "    y_blend_v = a * yhatB_v + (1.0 - a) * yhatA_v\n",
        "    y_blend_t = a * yhatB_t + (1.0 - a) * yhatA_t\n",
        "    mae_v = mean_absolute_error(y_true_v, y_blend_v)\n",
        "    mae_t = mean_absolute_error(y_true_t, y_blend_t)\n",
        "    rmse_t = np.sqrt(mean_squared_error(y_true_t, y_blend_t))\n",
        "    r2_t = r2_score(y_true_t, y_blend_t)\n",
        "    rows.append({\n",
        "        \"alpha\": float(a),\n",
        "        \"MAE_VALID\": float(mae_v),\n",
        "        \"MAE_TEST\": float(mae_t),\n",
        "        \"RMSE_TEST\": float(rmse_t),\n",
        "        \"R2_TEST\": float(r2_t),\n",
        "    })\n",
        "    if best_idx is None or mae_v < rows[best_idx][\"MAE_VALID\"]:\n",
        "        best_idx = i\n",
        "\n",
        "blend_df = pd.DataFrame(rows).sort_values(\"MAE_VALID\", ascending=True)\n",
        "blend_csv  = ART_DIR / \"blend_search_AB.csv\"\n",
        "blend_parq = ART_DIR / \"blend_search_AB.parquet\"\n",
        "blend_df.to_csv(blend_csv, index=False)\n",
        "blend_df.to_parquet(blend_parq, index=False)\n",
        "\n",
        "best_row = blend_df.iloc[0]\n",
        "print(\"Saved:\", blend_csv, \",\", blend_parq)\n",
        "print(\n",
        "    f\"Blend best α on VALID = {best_row['alpha']:.2f}  \"\n",
        "    f\"|  MAE_VALID={best_row['MAE_VALID']:.3f}  \"\n",
        "    f\"|  MAE_TEST={best_row['MAE_TEST']:.3f}  \"\n",
        "    f\"|  R2_TEST={best_row['R2_TEST']:.4f}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4ad2735",
      "metadata": {},
      "source": [
        "### XGBoost + Optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "9e53e7e1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected target unit: kEUR  (median=429.7)  MAX_LOG=9.868\n"
          ]
        }
      ],
      "source": [
        "# ---- TARGET SCALE HANDLER (EUR vs kEUR) ----\n",
        "import numpy as np\n",
        "\n",
        "TARGET_RAW_TRAIN = df_train[VALUATION_K].astype(\"float64\").to_numpy()\n",
        "p50 = float(np.nanmedian(TARGET_RAW_TRAIN))\n",
        "\n",
        "# eur_if_big: se la mediana > 2_000 assumo che la colonna sia in EURO; altrimenti è già in k€\n",
        "UNIT_SCALE = 1000.0 if p50 > 2000.0 else 1.0       # divide-by for training\n",
        "UNIT_LABEL = \"EUR\" if UNIT_SCALE == 1000.0 else \"kEUR\"\n",
        "\n",
        "def to_log(y_nat):\n",
        "    \"\"\"porta il target su scala log1p, uniformando all'unità di training.\"\"\"\n",
        "    return np.log1p(np.asarray(y_nat, float) / UNIT_SCALE)\n",
        "\n",
        "def from_log(y_log):\n",
        "    \"\"\"torna su scala naturale nell'UNITÀ ORIGINALE DEL DATASET (stessa di df[VALUATION_K]).\"\"\"\n",
        "    return np.expm1(np.asarray(y_log, float)) * UNIT_SCALE\n",
        "\n",
        "# Tetto numericamente stabile calcolato nella stessa unità del training\n",
        "p999 = float(np.nanpercentile(TARGET_RAW_TRAIN / UNIT_SCALE, 99.9))\n",
        "MAX_LOG = float(np.log1p(p999 * 10.0))  # 10× il 99.9p nella stessa unità del training\n",
        "\n",
        "def safe_expm1_scaled(y_log):\n",
        "    z = np.asarray(y_log, float)\n",
        "    z = np.clip(z, -20.0, MAX_LOG)      # evita overflow\n",
        "    return np.expm1(z) * UNIT_SCALE     # torna alla stessa unità del dataset\n",
        "\n",
        "# y in log per il training, coerente con UNIT_SCALE\n",
        "y_train = to_log(df_train[VALUATION_K].values)\n",
        "y_valid = to_log(df_valid[VALUATION_K].values)\n",
        "y_test  = to_log(df_test [VALUATION_K].values)\n",
        "\n",
        "# per metriche, i \"true\" restano nella loro unità originale (df_... è già in quella unità)\n",
        "y_val_true = df_valid[VALUATION_K].astype(\"float64\").to_numpy()\n",
        "y_tst_true = df_test [VALUATION_K].astype(\"float64\").to_numpy()\n",
        "\n",
        "print(f\"Detected target unit: {UNIT_LABEL}  (median={p50:,.1f})  MAX_LOG={MAX_LOG:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ef6e7363",
      "metadata": {},
      "outputs": [],
      "source": [
        "# XGBoost + Optuna — import & setup (coerente con notebooks/outputs/modeling/property)\n",
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    import optuna\n",
        "    from optuna.pruners import MedianPruner\n",
        "    from optuna.samplers import TPESampler\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"Servono xgboost e optuna (pip install xgboost optuna)\") from e\n",
        "\n",
        "# Dir coerenti (siamo dentro notebooks/)\n",
        "BASE_OUT = Path(\"outputs\")\n",
        "PROP_DIR = BASE_OUT / \"modeling\" / \"property\"\n",
        "XGB_DIR  = PROP_DIR / \"xgb\"\n",
        "for d in (PROP_DIR, XGB_DIR):\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Config da TRAIN_CFG (già caricato in alto nel nb)\n",
        "N_TRIALS   = int(TRAIN_CFG.get(\"xgb_optuna_trials\", 25))\n",
        "EARLY_STOP = int(TRAIN_CFG.get(\"xgb_early_stopping_rounds\", 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "ab0bc72f",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-09-23 02:02:50,794] A new study created in memory with name: no-name-6b003a8f-991d-43bb-923d-8e514a853470\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83ba655ff2d4449dbcb55e08563de72d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[I 2025-09-23 02:02:53,196] Trial 0 finished with value: 83.75421361112556 and parameters: {'n_estimators': 924, 'max_depth': 12, 'learning_rate': 0.08960785365368121, 'subsample': 0.8394633936788146, 'colsample_bytree': 0.6624074561769746, 'min_child_weight': 2.403950683025824, 'reg_alpha': 2.5502648504032812e-08, 'reg_lambda': 2.9154431891537547, 'gamma': 3.005575058716044}. Best is trial 0 with value: 83.75421361112556.\n",
            "[I 2025-09-23 02:02:55,413] Trial 1 finished with value: 83.09478065889328 and parameters: {'n_estimators': 1392, 'max_depth': 4, 'learning_rate': 0.18276027831785724, 'subsample': 0.9329770563201687, 'colsample_bytree': 0.6849356442713105, 'min_child_weight': 2.636424704863906, 'reg_alpha': 1.9223460470643606e-07, 'reg_lambda': 0.016480446427978974, 'gamma': 2.6237821581611893}. Best is trial 1 with value: 83.09478065889328.\n",
            "[I 2025-09-23 02:02:57,594] Trial 2 finished with value: 81.72831839661737 and parameters: {'n_estimators': 1005, 'max_depth': 6, 'learning_rate': 0.06252287916406217, 'subsample': 0.6557975442608167, 'colsample_bytree': 0.7168578594140873, 'min_child_weight': 4.297256589643226, 'reg_alpha': 1.5577217702692994e-05, 'reg_lambda': 1.382623217936987, 'gamma': 0.9983689107917987}. Best is trial 2 with value: 81.72831839661737.\n",
            "[I 2025-09-23 02:03:00,091] Trial 3 finished with value: 85.10934631920387 and parameters: {'n_estimators': 1120, 'max_depth': 9, 'learning_rate': 0.011492999300221412, 'subsample': 0.8430179407605753, 'colsample_bytree': 0.6682096494749166, 'min_child_weight': 1.5854643368675156, 'reg_alpha': 0.04387314432435398, 'reg_lambda': 7.2866537374910445, 'gamma': 4.041986740582305}. Best is trial 2 with value: 81.72831839661737.\n",
            "[I 2025-09-23 02:03:01,534] Trial 4 finished with value: 81.3338785766445 and parameters: {'n_estimators': 826, 'max_depth': 4, 'learning_rate': 0.07766184280392888, 'subsample': 0.7760609974958406, 'colsample_bytree': 0.6488152939379115, 'min_child_weight': 5.456592191001432, 'reg_alpha': 1.7406828393128058e-08, 'reg_lambda': 4.337920697490942, 'gamma': 1.2938999080000846}. Best is trial 4 with value: 81.3338785766445.\n",
            "[I 2025-09-23 02:03:04,055] Trial 5 finished with value: 85.54210662297385 and parameters: {'n_estimators': 1328, 'max_depth': 6, 'learning_rate': 0.04749239763680407, 'subsample': 0.8186841117373118, 'colsample_bytree': 0.6739417822102108, 'min_child_weight': 9.726261649881026, 'reg_alpha': 0.0026664274004676823, 'reg_lambda': 5.727904470799623, 'gamma': 4.474136752138244}. Best is trial 4 with value: 81.3338785766445.\n",
            "[I 2025-09-23 02:03:06,909] Trial 6 finished with value: 85.60457288995723 and parameters: {'n_estimators': 1237, 'max_depth': 12, 'learning_rate': 0.01303561122512888, 'subsample': 0.6783931449676581, 'colsample_bytree': 0.6180909155642152, 'min_child_weight': 3.927972976869379, 'reg_alpha': 5.257036929213663e-06, 'reg_lambda': 0.01217295809836997, 'gamma': 4.143687545759647}. Best is trial 4 with value: 81.3338785766445.\n",
            "[I 2025-09-23 02:03:08,796] Trial 7 finished with value: 81.30377548183822 and parameters: {'n_estimators': 899, 'max_depth': 6, 'learning_rate': 0.05082341959721458, 'subsample': 0.6563696899899051, 'colsample_bytree': 0.9208787923016158, 'min_child_weight': 1.6709557931179373, 'reg_alpha': 0.08094845352286129, 'reg_lambda': 1.2273800987852967, 'gamma': 0.993578407670862}. Best is trial 7 with value: 81.30377548183822.\n",
            "[I 2025-09-23 02:03:09,670] Trial 8 finished with value: 84.9616645895613 and parameters: {'n_estimators': 407, 'max_depth': 11, 'learning_rate': 0.08310795711416077, 'subsample': 0.8916028672163949, 'colsample_bytree': 0.9085081386743783, 'min_child_weight': 1.6664018656068134, 'reg_alpha': 3.230428252240957e-06, 'reg_lambda': 0.0029072088906598446, 'gamma': 4.315517129377968}. Best is trial 7 with value: 81.30377548183822.\n",
            "[I 2025-09-23 02:03:12,339] Trial 9 finished with value: 83.09617323548419 and parameters: {'n_estimators': 1273, 'max_depth': 6, 'learning_rate': 0.012097379927033842, 'subsample': 0.7243929286862649, 'colsample_bytree': 0.7300733288106989, 'min_child_weight': 7.566455605042576, 'reg_alpha': 0.00029033694281285587, 'reg_lambda': 3.53875886477924, 'gamma': 2.3610746258097466}. Best is trial 7 with value: 81.30377548183822.\n",
            "[I 2025-09-23 02:03:16,211] Trial 10 finished with value: 80.12430095940911 and parameters: {'n_estimators': 1724, 'max_depth': 8, 'learning_rate': 0.02579364481890643, 'subsample': 0.6071847502459278, 'colsample_bytree': 0.9924283304357215, 'min_child_weight': 7.212223662122307, 'reg_alpha': 0.06475366071424538, 'reg_lambda': 0.31356971313563664, 'gamma': 0.1989980303850739}. Best is trial 10 with value: 80.12430095940911.\n",
            "[I 2025-09-23 02:03:23,281] Trial 11 finished with value: 80.0997031755149 and parameters: {'n_estimators': 1795, 'max_depth': 8, 'learning_rate': 0.023807634312747353, 'subsample': 0.6097259194088174, 'colsample_bytree': 0.9962928066797128, 'min_child_weight': 7.3242930948219875, 'reg_alpha': 0.05893263395458023, 'reg_lambda': 0.29163991155219965, 'gamma': 0.02336785351386947}. Best is trial 11 with value: 80.0997031755149.\n",
            "[I 2025-09-23 02:03:30,591] Trial 12 finished with value: 80.70515157479338 and parameters: {'n_estimators': 1754, 'max_depth': 9, 'learning_rate': 0.023968552236191435, 'subsample': 0.6050785460502632, 'colsample_bytree': 0.9988599342221308, 'min_child_weight': 7.459713835929907, 'reg_alpha': 0.00476541350031391, 'reg_lambda': 0.15055494946272593, 'gamma': 0.015535997573864943}. Best is trial 11 with value: 80.0997031755149.\n",
            "[I 2025-09-23 02:03:35,080] Trial 13 finished with value: 80.2285946783975 and parameters: {'n_estimators': 1795, 'max_depth': 8, 'learning_rate': 0.02619770539052082, 'subsample': 0.6013755413731107, 'colsample_bytree': 0.9942603012292254, 'min_child_weight': 7.454417041056622, 'reg_alpha': 0.0032617505583166155, 'reg_lambda': 0.19815924679531885, 'gamma': 0.09190109754132769}. Best is trial 11 with value: 80.0997031755149.\n",
            "[I 2025-09-23 02:03:39,023] Trial 14 finished with value: 82.57710930820491 and parameters: {'n_estimators': 1556, 'max_depth': 9, 'learning_rate': 0.030594503962742933, 'subsample': 0.9984067957444853, 'colsample_bytree': 0.8537415142699929, 'min_child_weight': 9.103501396805832, 'reg_alpha': 0.00029204798189662106, 'reg_lambda': 0.3996047132855011, 'gamma': 1.6951653707119088}. Best is trial 11 with value: 80.0997031755149.\n",
            "[I 2025-09-23 02:03:43,899] Trial 15 finished with value: 80.50426236947425 and parameters: {'n_estimators': 1583, 'max_depth': 8, 'learning_rate': 0.018400550828965724, 'subsample': 0.7190852606077679, 'colsample_bytree': 0.9437731661608388, 'min_child_weight': 6.3410308727685925, 'reg_alpha': 0.013239222469202016, 'reg_lambda': 0.04303234469727153, 'gamma': 0.524233989443942}. Best is trial 11 with value: 80.0997031755149.\n",
            "[I 2025-09-23 02:03:46,654] Trial 16 finished with value: 82.28329828687173 and parameters: {'n_estimators': 1552, 'max_depth': 7, 'learning_rate': 0.03667604998501823, 'subsample': 0.7398929922464663, 'colsample_bytree': 0.8250051428101366, 'min_child_weight': 8.436546048976265, 'reg_alpha': 0.000358910176920784, 'reg_lambda': 0.6741831816019808, 'gamma': 1.79195142395271}. Best is trial 11 with value: 80.0997031755149.\n",
            "[I 2025-09-23 02:03:50,003] Trial 17 finished with value: 80.92358024528956 and parameters: {'n_estimators': 1667, 'max_depth': 11, 'learning_rate': 0.01836025878844076, 'subsample': 0.6383744886047794, 'colsample_bytree': 0.8734893671963153, 'min_child_weight': 6.126128208056022, 'reg_alpha': 0.020786687299414415, 'reg_lambda': 0.09742667005770365, 'gamma': 0.6145483701281097}. Best is trial 11 with value: 80.0997031755149.\n",
            "[I 2025-09-23 02:03:52,181] Trial 18 finished with value: 80.68215549098615 and parameters: {'n_estimators': 686, 'max_depth': 10, 'learning_rate': 0.018444825002317942, 'subsample': 0.6924106883472467, 'colsample_bytree': 0.9574161886297862, 'min_child_weight': 5.048229395256277, 'reg_alpha': 0.08984944929833691, 'reg_lambda': 0.04521303089320843, 'gamma': 0.291043679231337}. Best is trial 11 with value: 80.0997031755149.\n",
            "[I 2025-09-23 02:03:54,698] Trial 19 finished with value: 81.95744179489827 and parameters: {'n_estimators': 1468, 'max_depth': 7, 'learning_rate': 0.036136436985130034, 'subsample': 0.7727431143998708, 'colsample_bytree': 0.764088169035724, 'min_child_weight': 6.377706595582659, 'reg_alpha': 0.0007891634471982375, 'reg_lambda': 0.3724447166632402, 'gamma': 1.7228560355394587}. Best is trial 11 with value: 80.0997031755149.\n",
            "[I 2025-09-23 02:03:57,387] Trial 20 finished with value: 85.19948216724649 and parameters: {'n_estimators': 1694, 'max_depth': 7, 'learning_rate': 0.1312102588277893, 'subsample': 0.6318304387427113, 'colsample_bytree': 0.8916140208975962, 'min_child_weight': 8.389948424437886, 'reg_alpha': 7.195375333150764e-05, 'reg_lambda': 0.0011129709953152675, 'gamma': 3.583595382004592}. Best is trial 11 with value: 80.0997031755149.\n",
            "[I 2025-09-23 02:04:02,805] Trial 21 finished with value: 80.77184549178814 and parameters: {'n_estimators': 1762, 'max_depth': 8, 'learning_rate': 0.02506523111038057, 'subsample': 0.6053391488974321, 'colsample_bytree': 0.99763691494987, 'min_child_weight': 7.329075699963392, 'reg_alpha': 0.007475693678320597, 'reg_lambda': 0.23649450388337215, 'gamma': 0.028345538121102132}. Best is trial 11 with value: 80.0997031755149.\n",
            "[I 2025-09-23 02:04:06,102] Trial 22 finished with value: 80.88114265844392 and parameters: {'n_estimators': 1782, 'max_depth': 8, 'learning_rate': 0.02517377324047638, 'subsample': 0.6174331441847896, 'colsample_bytree': 0.9668867733462511, 'min_child_weight': 6.899381756362117, 'reg_alpha': 0.0023758386919899934, 'reg_lambda': 0.09547864805618882, 'gamma': 0.6608413857851274}. Best is trial 11 with value: 80.0997031755149.\n",
            "[I 2025-09-23 02:04:09,200] Trial 23 finished with value: 81.62199618077793 and parameters: {'n_estimators': 1644, 'max_depth': 10, 'learning_rate': 0.01594797367859363, 'subsample': 0.6802210286121898, 'colsample_bytree': 0.9717201750008196, 'min_child_weight': 8.207775298269423, 'reg_alpha': 0.023500286880657823, 'reg_lambda': 1.069692780182429, 'gamma': 1.0810806524959218}. Best is trial 11 with value: 80.0997031755149.\n",
            "[I 2025-09-23 02:04:12,877] Trial 24 finished with value: 80.68059831442805 and parameters: {'n_estimators': 1471, 'max_depth': 9, 'learning_rate': 0.031224841149112035, 'subsample': 0.6512724702232536, 'colsample_bytree': 0.9305714933739584, 'min_child_weight': 9.346877734724588, 'reg_alpha': 0.0013716202120310603, 'reg_lambda': 0.03628294690625198, 'gamma': 0.36006368313980763}. Best is trial 11 with value: 80.0997031755149.\n",
            "Best trial: 11 MAE (valid, natural): 80.0997031755149\n"
          ]
        }
      ],
      "source": [
        "# 07ter) Optuna: tuning XGBoost (setup B: confidence come peso) — safe & robust\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.base import clone\n",
        "import inspect\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# tetto dinamico: 10× il 99.9° percentile del target (in k€), poi log1p\n",
        "MAX_LOG = float(np.log1p(np.nanpercentile(df_train[VALUATION_K].values, 99.9) * 10.0))\n",
        "\n",
        "def safe_expm1(z, max_log=MAX_LOG):\n",
        "    z = np.asarray(z, dtype=np.float64)\n",
        "    z = np.clip(z, -20.0, max_log)  # -20 ~ ~0 in scala naturale; max_log evita overflow\n",
        "    return np.expm1(z)\n",
        "\n",
        "\n",
        "# Preprocess identico a setup B (senza confidence tra le feature)\n",
        "_transformers = []\n",
        "if \"cat_cols\" in globals() and len(cat_cols) > 0:\n",
        "    _transformers.append((\"cat\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1), cat_cols))\n",
        "if \"num_cols_B\" in globals() and len(num_cols_B) > 0:\n",
        "    _transformers.append((\"num\", \"passthrough\", num_cols_B))\n",
        "preproc_B = ColumnTransformer(transformers=_transformers, remainder=\"drop\")\n",
        "\n",
        "# Feature list (dedup, ordine preservato)\n",
        "features_B = list(dict.fromkeys([*(cat_cols if \"cat_cols\" in globals() else []),\n",
        "                                 *(num_cols_B if \"num_cols_B\" in globals() else [])]))\n",
        "\n",
        "# Pesi (confidence/sample_weight) se presenti → allineati all'indice di X_train\n",
        "if \"sample_weight\" in df_train.columns:\n",
        "    weights_B = df_train.loc[X_train.index, \"sample_weight\"].to_numpy(dtype=float)\n",
        "else:\n",
        "    weights_B = np.ones(len(X_train), dtype=float)\n",
        "\n",
        "def _suggest_params(trial: optuna.Trial) -> dict:\n",
        "    return {\n",
        "        \"n_estimators\":     trial.suggest_int(\"n_estimators\", 400, 1800),\n",
        "        \"max_depth\":        trial.suggest_int(\"max_depth\", 4, 12),\n",
        "        \"learning_rate\":    trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
        "        \"subsample\":        trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
        "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1.0, 10.0),\n",
        "        \"reg_alpha\":        trial.suggest_float(\"reg_alpha\", 1e-8, 1e-1, log=True),\n",
        "        \"reg_lambda\":       trial.suggest_float(\"reg_lambda\", 1e-3, 10.0, log=True),\n",
        "        \"gamma\":            trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
        "        \"random_state\":     SEED,\n",
        "        \"tree_method\":      \"hist\",\n",
        "        \"n_jobs\":           -1,\n",
        "    }\n",
        "\n",
        "def _fit_xgb_with_preproc(params: dict,\n",
        "                          Xtr_df: pd.DataFrame, ytr_log: np.ndarray,\n",
        "                          Xva_df: pd.DataFrame, yva_log: np.ndarray,\n",
        "                          w: np.ndarray, early_rounds: int = EARLY_STOP):\n",
        "    \"\"\"\n",
        "    Allena XGB su target log1p (usiamo y_train/y_valid già in log1p),\n",
        "    early-stopping su valid (MAE calcolata in log-space da XGB).\n",
        "    Le metriche 'finali' le faremo poi in scala naturale con expm1.\n",
        "    \"\"\"\n",
        "    prep = clone(preproc_B)\n",
        "    Xt = prep.fit_transform(Xtr_df)\n",
        "    Xv = prep.transform(Xva_df)\n",
        "\n",
        "    model = xgb.XGBRegressor(**params)\n",
        "    sig = inspect.signature(model.fit)\n",
        "    fit_kwargs = {}\n",
        "    if \"eval_set\" in sig.parameters:        fit_kwargs[\"eval_set\"] = [(Xv, yva_log)]\n",
        "    if \"eval_metric\" in sig.parameters:     fit_kwargs[\"eval_metric\"] = \"mae\"\n",
        "    if \"sample_weight\" in sig.parameters:   fit_kwargs[\"sample_weight\"] = w\n",
        "    if \"early_stopping_rounds\" in sig.parameters:\n",
        "        fit_kwargs[\"early_stopping_rounds\"] = early_rounds\n",
        "    elif \"callbacks\" in sig.parameters:\n",
        "        from xgboost.callback import EarlyStopping\n",
        "        fit_kwargs[\"callbacks\"] = [EarlyStopping(rounds=early_rounds, save_best=True)]\n",
        "    if \"verbose\" in sig.parameters:         fit_kwargs[\"verbose\"] = False\n",
        "\n",
        "    model.fit(Xt, ytr_log, **fit_kwargs)\n",
        "    pipe = Pipeline([(\"prep\", prep), (\"xgb\", model)])\n",
        "    return pipe, model, Xt, Xv\n",
        "\n",
        "def objective(trial: optuna.Trial) -> float:\n",
        "    params = _suggest_params(trial)\n",
        "    Xtr_df, Xva_df = X_train[features_B], X_valid[features_B]\n",
        "    # y_train/y_valid sono in log1p dalle celle RF — li riusiamo\n",
        "    pipe, model, Xt, Xv = _fit_xgb_with_preproc(params, Xtr_df, y_train, Xva_df, y_valid, weights_B)\n",
        "    # misura l'obiettivo in scala naturale\n",
        "    y_log_pred = model.predict(Xv)              # pred in log\n",
        "    y_log_pred = model.predict(Xv)\n",
        "    pred_nat = safe_expm1_scaled(y_log_pred)\n",
        "    if not np.all(np.isfinite(pred_nat)):  # trial instabile → penalizza\n",
        "        return 1e9\n",
        "    return mean_absolute_error(y_val_true, pred_nat)\n",
        "\n",
        "study = optuna.create_study(\n",
        "    direction=\"minimize\",\n",
        "    pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=5),\n",
        "    sampler=TPESampler(seed=SEED),\n",
        ")\n",
        "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)\n",
        "\n",
        "print(\"Best trial:\", study.best_trial.number, \"MAE (valid, natural):\", study.best_value)\n",
        "best_params = study.best_trial.params\n",
        "(XGB_DIR / \"optuna_best_params_setupB.json\").write_text(canonical_json_dumps(best_params), encoding=\"utf-8\")\n",
        "study.trials_dataframe().to_parquet(XGB_DIR / \"optuna_trials_setupB.parquet\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "550bceee",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGB_B VALID: {'MAE': 80.16493868923531, 'RMSE': 118.83112915289428, 'R2': 0.8355980211764904} | unit: kEUR\n",
            "XGB_B TEST : {'MAE': 82.44617842378852, 'RMSE': 123.9313541635718, 'R2': 0.8313747578922511} | unit: kEUR\n"
          ]
        }
      ],
      "source": [
        "# 07quater-bis) Build XGB finale (setup B) + metriche VALID/TEST\n",
        "\n",
        "import inspect, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "def _rmse(y_true, y_pred):\n",
        "    try:\n",
        "        return mean_squared_error(y_true, y_pred, squared=False)\n",
        "    except TypeError:\n",
        "        return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "# Best params da Optuna o fallback sensato\n",
        "bp_path = XGB_DIR / \"optuna_best_params_setupB.json\"\n",
        "if bp_path.exists():\n",
        "    best_params = json.loads(bp_path.read_text(encoding=\"utf-8\"))\n",
        "else:\n",
        "    best_params = {\n",
        "        \"n_estimators\": 900,\n",
        "        \"max_depth\": 8,\n",
        "        \"learning_rate\": 0.06,\n",
        "        \"subsample\": 0.9,\n",
        "        \"colsample_bytree\": 0.9,\n",
        "        \"min_child_weight\": 3.0,\n",
        "        \"reg_alpha\": 1e-6,\n",
        "        \"reg_lambda\": 1.0,\n",
        "        \"gamma\": 0.0,\n",
        "        \"random_state\": SEED,\n",
        "        \"tree_method\": \"hist\",\n",
        "        \"n_jobs\": -1,\n",
        "    }\n",
        "\n",
        "# Fit finale con early stopping su VALID\n",
        "pipe_xgb_B, model_xgb_B, Xt, Xv = _fit_xgb_with_preproc(\n",
        "    best_params,\n",
        "    X_train[features_B], y_train,\n",
        "    X_valid[features_B], y_valid,\n",
        "    weights_B,\n",
        "    early_rounds=EARLY_STOP,\n",
        ")\n",
        "\n",
        "# Predizioni in scala naturale (k€)\n",
        "ylog_val = pipe_xgb_B.named_steps[\"xgb\"].predict(\n",
        "    pipe_xgb_B.named_steps[\"prep\"].transform(X_valid[features_B])\n",
        ")\n",
        "ylog_tst = pipe_xgb_B.named_steps[\"xgb\"].predict(\n",
        "    pipe_xgb_B.named_steps[\"prep\"].transform(X_test[features_B])\n",
        ")\n",
        "pred_val_XGB = safe_expm1_scaled(ylog_val)\n",
        "pred_tst_XGB = safe_expm1_scaled(ylog_tst)\n",
        "\n",
        "m_val = {\n",
        "    \"MAE\": float(mean_absolute_error(y_val_true, pred_val_XGB)),\n",
        "    \"RMSE\": float(np.sqrt(mean_squared_error(y_val_true, pred_val_XGB))),\n",
        "    \"R2\":  float(r2_score(y_val_true, pred_val_XGB)),\n",
        "}\n",
        "m_tst = {\n",
        "    \"MAE\": float(mean_absolute_error(y_tst_true, pred_tst_XGB)),\n",
        "    \"RMSE\": float(np.sqrt(mean_squared_error(y_tst_true, pred_tst_XGB))),\n",
        "    \"R2\":  float(r2_score(y_tst_true, pred_tst_XGB)),\n",
        "}\n",
        "print(\"XGB_B VALID:\", m_val, \"| unit:\", UNIT_LABEL)\n",
        "print(\"XGB_B TEST :\", m_tst,  \"| unit:\", UNIT_LABEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "eddb94ef",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved XGB model to: outputs\\modeling\\property\\xgb\\xgb_setupB_champion.joblib\n",
            "Saved: outputs\\modeling\\property\\xgb\\xgb_gain_importance.json\n",
            "Updated manifest: outputs\\modeling\\property\\training_manifest.json\n"
          ]
        }
      ],
      "source": [
        "# Persistenza XGB + gain importance + update manifest (coerente con PROP_DIR)\n",
        "\n",
        "from pathlib import Path\n",
        "import joblib, json, hashlib\n",
        "\n",
        "xgb_path = XGB_DIR / \"xgb_setupB_champion.joblib\"\n",
        "\n",
        "# 1) Salva pipeline completa (prep + xgb)\n",
        "joblib.dump(pipe_xgb_B, xgb_path)\n",
        "print(\"Saved XGB model to:\", xgb_path)\n",
        "\n",
        "# 2) Gain importance (se disponibile)\n",
        "try:\n",
        "    booster = pipe_xgb_B.named_steps[\"xgb\"].get_booster()\n",
        "    f_gain = booster.get_score(importance_type=\"gain\")\n",
        "    (XGB_DIR / \"xgb_gain_importance.json\").write_text(canonical_json_dumps(f_gain), encoding=\"utf-8\")\n",
        "    print(\"Saved:\", XGB_DIR / \"xgb_gain_importance.json\")\n",
        "except Exception as e:\n",
        "    print(\"Gain importance not available:\", e)\n",
        "\n",
        "# 3) Hash per tracciabilità\n",
        "h = hashlib.sha256()\n",
        "with open(xgb_path, \"rb\") as _f:\n",
        "    for chunk in iter(lambda: _f.read(1024 * 1024), b\"\"):\n",
        "        h.update(chunk)\n",
        "xgb_sha = h.hexdigest()\n",
        "\n",
        "# 4) Aggiorna training manifest nella cartella PROPERTY\n",
        "manifest_path = PROP_DIR / \"training_manifest.json\"\n",
        "manifest = json.loads(manifest_path.read_text(encoding=\"utf-8\")) if manifest_path.exists() else {}\n",
        "\n",
        "# paths\n",
        "paths = manifest.setdefault(\"paths\", {})\n",
        "paths.update({\n",
        "    \"xgb_model\": str(xgb_path),\n",
        "    \"xgb_model_sha256\": xgb_sha,\n",
        "    \"xgb_gain_importance\": str(XGB_DIR / \"xgb_gain_importance.json\"),\n",
        "    \"xgb_optuna_best_params\": str(XGB_DIR / \"optuna_best_params_setupB.json\") if (XGB_DIR / \"optuna_best_params_setupB.json\").exists() else paths.get(\"xgb_optuna_best_params\"),\n",
        "    \"xgb_optuna_trials\": str(XGB_DIR / \"optuna_trials_setupB.parquet\") if (XGB_DIR / \"optuna_trials_setupB.parquet\").exists() else paths.get(\"xgb_optuna_trials\"),\n",
        "    # opzionale: punta il \"pipeline_path\" di default allo XGB champion\n",
        "    \"pipeline_path\": str(xgb_path),\n",
        "})\n",
        "\n",
        "# metrics\n",
        "metrics = manifest.setdefault(\"metrics\", {})\n",
        "metrics.update({\n",
        "    \"xgb_valid\": m_val,\n",
        "    \"xgb_test\":  m_tst,\n",
        "})\n",
        "\n",
        "# meta (utile per la UI/serving)\n",
        "manifest[\"model_meta\"] = {\n",
        "    \"value_model_name\": \"XGBRegressor\",\n",
        "    \"value_model_version\": \"v2\",          # etichetta “v2” per differenziarlo dall’RF v1\n",
        "    \"setup\": \"B_conf_as_weight\",\n",
        "    \"early_stopping_rounds\": int(EARLY_STOP),\n",
        "    \"optuna_trials\": int(N_TRIALS),\n",
        "    \"seed\": int(SEED),\n",
        "}\n",
        "\n",
        "manifest_path.write_text(canonical_json_dumps(manifest), encoding=\"utf-8\")\n",
        "print(\"Updated manifest:\", manifest_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "279cc11f-e303-48de-92c6-d38c9ec38bf1",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Feature Importance (on champion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "de9c6c54-9cfc-4c43-8cb6-721f5066387c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\figures\\rf_feature_importance_builtin.png\n",
            "Saved: c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\figures\\rf_feature_importance_permutation.png\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>importance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>f31</td>\n",
              "      <td>0.278601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>f41</td>\n",
              "      <td>0.256731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>f0</td>\n",
              "      <td>0.154237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>f34</td>\n",
              "      <td>0.049649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>f6</td>\n",
              "      <td>0.039623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>f48</td>\n",
              "      <td>0.036962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>f1</td>\n",
              "      <td>0.029908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>f2</td>\n",
              "      <td>0.029130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>f15</td>\n",
              "      <td>0.018387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>f43</td>\n",
              "      <td>0.011457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>f7</td>\n",
              "      <td>0.010020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>f13</td>\n",
              "      <td>0.009342</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   feature  importance\n",
              "0      f31    0.278601\n",
              "1      f41    0.256731\n",
              "2       f0    0.154237\n",
              "3      f34    0.049649\n",
              "4       f6    0.039623\n",
              "5      f48    0.036962\n",
              "6       f1    0.029908\n",
              "7       f2    0.029130\n",
              "8      f15    0.018387\n",
              "9      f43    0.011457\n",
              "10      f7    0.010020\n",
              "11     f13    0.009342"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>importance</th>\n",
              "      <th>std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>zone</td>\n",
              "      <td>0.480981</td>\n",
              "      <td>0.018369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>size_m2</td>\n",
              "      <td>0.252907</td>\n",
              "      <td>0.009442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>log_size_m2</td>\n",
              "      <td>0.220198</td>\n",
              "      <td>0.009085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>year_built</td>\n",
              "      <td>0.068529</td>\n",
              "      <td>0.005312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>energy_class</td>\n",
              "      <td>0.064900</td>\n",
              "      <td>0.006219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>region</td>\n",
              "      <td>0.045429</td>\n",
              "      <td>0.004541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>condition</td>\n",
              "      <td>0.042116</td>\n",
              "      <td>0.002706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>region_index_prior</td>\n",
              "      <td>0.030218</td>\n",
              "      <td>0.002839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>view</td>\n",
              "      <td>0.002019</td>\n",
              "      <td>0.000816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>floor</td>\n",
              "      <td>0.001843</td>\n",
              "      <td>0.001062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>heating</td>\n",
              "      <td>0.001312</td>\n",
              "      <td>0.000427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>is_top_floor</td>\n",
              "      <td>0.000840</td>\n",
              "      <td>0.000189</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               feature  importance       std\n",
              "0                 zone    0.480981  0.018369\n",
              "1              size_m2    0.252907  0.009442\n",
              "2          log_size_m2    0.220198  0.009085\n",
              "3           year_built    0.068529  0.005312\n",
              "4         energy_class    0.064900  0.006219\n",
              "5               region    0.045429  0.004541\n",
              "6            condition    0.042116  0.002706\n",
              "7   region_index_prior    0.030218  0.002839\n",
              "8                 view    0.002019  0.000816\n",
              "9                floor    0.001843  0.001062\n",
              "10             heating    0.001312  0.000427\n",
              "11        is_top_floor    0.000840  0.000189"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 07) Feature importances (RF champion) + salvataggio figure — ROBUST\n",
        "from __future__ import annotations\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "TOPN = 20  # barre nei grafici\n",
        "\n",
        "# -- helper: prendi per prima un'istanza esistente tra più nomi\n",
        "def _pick_first(*names):\n",
        "    for n in names:\n",
        "        if n in globals() and globals()[n] is not None:\n",
        "            return globals()[n]\n",
        "    return None\n",
        "\n",
        "# -- helper: estrai inner estimator, preproc e step finale da Pipeline o TTR\n",
        "def _extract_parts(model_like):\n",
        "    \"\"\"\n",
        "    Ritorna (inner_pipe, preproc, final_est, final_step_name, is_ttr)\n",
        "    - inner_pipe: Pipeline(\"prep\", \"...\"), l'oggetto che ha .named_steps\n",
        "    - preproc   : ColumnTransformer dello step \"prep\", se presente\n",
        "    - final_est : estimatore finale (es. RandomForestRegressor)\n",
        "    - final_step_name: nome dello step finale nella pipeline (es. \"rf\" o \"model\")\n",
        "    - is_ttr    : bool, True se model_like è un TransformedTargetRegressor\n",
        "    \"\"\"\n",
        "    is_ttr = isinstance(model_like, TransformedTargetRegressor)\n",
        "    inner = model_like.regressor_ if is_ttr and hasattr(model_like, \"regressor_\") else \\\n",
        "            (model_like.regressor if is_ttr and hasattr(model_like, \"regressor\") else model_like)\n",
        "\n",
        "    if not isinstance(inner, Pipeline):\n",
        "        raise RuntimeError(\"Pipeline interna non trovata: atteso Pipeline con step 'prep' e modello finale.\")\n",
        "\n",
        "    # preproc\n",
        "    preproc = inner.named_steps.get(\"prep\", None)\n",
        "\n",
        "    # step finale & estimator\n",
        "    # preferisci chiavi comuni, altrimenti prendi l'ultimo step\n",
        "    final_step_name = None\n",
        "    for cand in (\"rf\", \"model\", \"xgb\"):\n",
        "        if cand in inner.named_steps:\n",
        "            final_step_name = cand\n",
        "            break\n",
        "    if final_step_name is None:\n",
        "        final_step_name = list(inner.named_steps.keys())[-1]\n",
        "    final_est = inner.named_steps[final_step_name]\n",
        "\n",
        "    return inner, preproc, final_est, final_step_name, is_ttr\n",
        "\n",
        "# -- helper: ricava lista feature nella GIUSTA sequenza del CT (cat poi num)\n",
        "def _feature_names_from_ct(preproc, fallback_cat, fallback_num):\n",
        "    \"\"\"\n",
        "    Con OrdinalEncoder per le categoriche ⇒ 1 col/feature.\n",
        "    \"\"\"\n",
        "    if preproc is None or not hasattr(preproc, \"transformers\"):\n",
        "        # fallback: usa liste note\n",
        "        return [*fallback_cat, *[c for c in fallback_num if c not in set(fallback_cat)]]\n",
        "\n",
        "    cat_cols_ct, num_cols_ct = [], []\n",
        "    for name, est, cols in preproc.transformers:\n",
        "        if name == \"cat\":\n",
        "            cat_cols_ct = list(cols) if isinstance(cols, (list, tuple, np.ndarray, pd.Index)) else list(fallback_cat)\n",
        "        elif name == \"num\":\n",
        "            num_cols_ct = list(cols) if isinstance(cols, (list, tuple, np.ndarray, pd.Index)) else list(fallback_num)\n",
        "\n",
        "    # sicurezza ordine/duplicati\n",
        "    seen = set()\n",
        "    ordered = []\n",
        "    for c in list(cat_cols_ct) + list(num_cols_ct):\n",
        "        if c not in seen:\n",
        "            seen.add(c); ordered.append(c)\n",
        "    return ordered\n",
        "\n",
        "# -- 1) Scegli il CHAMPION e la relativa pipeline\n",
        "champ = (champion if \"champion\" in globals() else \"A\")  # default\n",
        "if champ == \"A\":\n",
        "    chosen = _pick_first(\"ttr_A\", \"pipe_A\")\n",
        "    # liste colonne previste per A\n",
        "    chosen_cat = [c for c in (cat_cols if \"cat_cols\" in globals() else [])]\n",
        "    chosen_num = [c for c in (num_cols  if \"num_cols\"  in globals() else [])]\n",
        "else:\n",
        "    chosen = _pick_first(\"ttr_B\", \"pipe_B\")\n",
        "    # liste colonne previste per B (no confidence come feature)\n",
        "    chosen_cat = [c for c in (cat_cols    if \"cat_cols\"    in globals() else [])]\n",
        "    chosen_num = [c for c in (num_cols_B  if \"num_cols_B\"  in globals() else [])]\n",
        "\n",
        "if chosen is None:\n",
        "    raise RuntimeError(\"Nessuna pipeline/TransformedTargetRegressor disponibile per il champion selezionato.\")\n",
        "\n",
        "# -- 2) Estrai parti interne e ricava feature usate davvero\n",
        "inner_pipe, preproc, final_est, final_step, is_ttr = _extract_parts(chosen)\n",
        "feat_in_use = _feature_names_from_ct(preproc, chosen_cat, chosen_num)\n",
        "\n",
        "# -- 3) Prepara X_test allineato (crea colonne mancanti → imputazione)\n",
        "def _ensure_cols(df_part: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
        "    dfp = df_part.copy()\n",
        "    missing = [c for c in cols if c not in dfp.columns]\n",
        "    for c in missing:\n",
        "        dfp[c] = np.nan\n",
        "    return dfp[cols]\n",
        "\n",
        "if \"df_test\" not in globals():\n",
        "    raise RuntimeError(\"df_test mancante.\")\n",
        "X_tst_use = _ensure_cols(df_test, feat_in_use)\n",
        "\n",
        "# -- 4) BUILT-IN importance (se disponibile) con nomi feature consistenti\n",
        "builtin_imp = None\n",
        "fi_raw = getattr(final_est, \"feature_importances_\", None)\n",
        "if fi_raw is not None:\n",
        "    try:\n",
        "        imp = np.asarray(fi_raw)\n",
        "        if imp.ndim == 1 and imp.size > 0:\n",
        "            # allinea lunghezze; se non combaciano, usa f0..f{n-1}\n",
        "            if len(feat_in_use) != imp.shape[0]:\n",
        "                feat_names = [f\"f{i}\" for i in range(int(imp.shape[0]))]\n",
        "            else:\n",
        "                feat_names = list(feat_in_use)\n",
        "            builtin_imp = (\n",
        "                pd.DataFrame({\"feature\": feat_names, \"importance\": imp.astype(float, copy=False)})\n",
        "                .sort_values(\"importance\", ascending=False)\n",
        "                .reset_index(drop=True)\n",
        "            )\n",
        "            builtin_imp.to_csv(MODEL_DIR / \"feature_importance_builtin.csv\", index=False)\n",
        "            builtin_imp.to_parquet(MODEL_DIR / \"feature_importance_builtin.parquet\", index=False)\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            _top = min(TOPN, len(builtin_imp))\n",
        "            ax = builtin_imp.head(_top).plot(kind=\"bar\", x=\"feature\", y=\"importance\", legend=False, rot=45)\n",
        "            ax.set_title(f\"RF Built-in Feature Importance (top {_top})\")\n",
        "            ax.set_ylabel(\"Importance\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(FIG_DIR / \"rf_feature_importance_builtin.png\", dpi=150)\n",
        "            plt.close()\n",
        "            print(\"Saved:\", FIG_DIR / \"rf_feature_importance_builtin.png\")\n",
        "        else:\n",
        "            print(\"⚠️ feature_importances_ presente ma vuoto/0-D → salto built-in.\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Impossibile calcolare built-in importance: {e}\")\n",
        "else:\n",
        "    print(\"ℹ️ feature_importances_ non disponibile sul modello finale → salto built-in.\")\n",
        "\n",
        "# -- 5) PERMUTATION importance\n",
        "#    Scala y coerente:\n",
        "#      - se TTR: predict è in scala NATURALE → passiamo y naturale\n",
        "#      - se Pipeline pura (train su log1p): passiamo y in log\n",
        "y_perm = None\n",
        "if is_ttr:\n",
        "    y_perm = df_test[VALUATION_K].to_numpy(dtype=\"float64\", copy=False)\n",
        "else:\n",
        "    y_perm = np.log1p(df_test[VALUATION_K].to_numpy(dtype=\"float64\", copy=False))\n",
        "\n",
        "perm = permutation_importance(\n",
        "    estimator=chosen,              # TTR o Pipeline\n",
        "    X=X_tst_use,\n",
        "    y=y_perm,\n",
        "    n_repeats=8,\n",
        "    random_state=SEED if \"SEED\" in globals() else 42,\n",
        "    n_jobs=-1,\n",
        "    scoring=\"r2\",\n",
        ")\n",
        "# nomi: usa feat_in_use (o fallback f0..)\n",
        "feat_names_pi = list(feat_in_use) if len(feat_in_use) == perm.importances_mean.shape[0] \\\n",
        "                else [f\"f{i}\" for i in range(perm.importances_mean.shape[0])]\n",
        "\n",
        "perm_imp = (\n",
        "    pd.DataFrame({\n",
        "        \"feature\": feat_names_pi,\n",
        "        \"importance\": perm.importances_mean.astype(float, copy=False),\n",
        "        \"std\": perm.importances_std.astype(float, copy=False),\n",
        "    })\n",
        "    .sort_values(\"importance\", ascending=False)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "perm_imp.to_csv(MODEL_DIR / \"feature_importance_permutation.csv\", index=False)\n",
        "perm_imp.to_parquet(MODEL_DIR / \"feature_importance_permutation.parquet\", index=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "_top = min(TOPN, len(perm_imp))\n",
        "ax = perm_imp.head(_top).plot(kind=\"bar\", x=\"feature\", y=\"importance\", yerr=\"std\", legend=False, rot=45)\n",
        "ax.set_title(f\"Permutation Importance (top {_top})\")\n",
        "ax.set_ylabel(\"Importance (mean ΔR²)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"rf_feature_importance_permutation.png\", dpi=150)\n",
        "plt.close()\n",
        "print(\"Saved:\", FIG_DIR / \"rf_feature_importance_permutation.png\")\n",
        "\n",
        "# -- 6) Anteprima (se disponibile)\n",
        "if builtin_imp is not None:\n",
        "    display(builtin_imp.head(12))\n",
        "display(perm_imp.head(12))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca8ec440",
      "metadata": {},
      "source": [
        "### Save figure importances & residuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "a72e0ffa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\figures\\rf_feature_importance_builtin_top20.png\n",
            "Saved: c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\figures\\rf_permutation_importance_top20.png\n",
            "Saved: c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\figures\\rf_residuals_test_hist.png\n",
            "Saved: c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\figures\\rf_residuals_vs_pred_test.png\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 900x600 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 900x600 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 07bis (v2) — Importances + Residuali TEST con ricalcolo robusto delle predizioni\n",
        "\n",
        "from __future__ import annotations\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "\n",
        "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------------- helpers ----------------\n",
        "def _expm1_safe(z, cap: float = 12.0):\n",
        "    z = np.asarray(z, dtype=np.float64)\n",
        "    z = np.clip(z, -20.0, cap)  # cap log-pred per evitare overflow\n",
        "    out = np.expm1(z)\n",
        "    out[out < 0] = 0.0\n",
        "    return out\n",
        "\n",
        "def _ensure_cols(df_part: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
        "    dfp = df_part.copy()\n",
        "    miss = [c for c in cols if c not in dfp.columns]\n",
        "    if miss:\n",
        "        for c in miss: dfp[c] = np.nan\n",
        "        print(f\"ℹ️ Aggiunte {len(miss)} colonne mancanti allo split TEST (imputate): {miss[:10]}\")\n",
        "    return dfp[cols]\n",
        "\n",
        "def _pick_first(*names):\n",
        "    for n in names:\n",
        "        if n in globals() and globals()[n] is not None:\n",
        "            return globals()[n]\n",
        "    return None\n",
        "\n",
        "def _predict_nat_safely(est, X_df: pd.DataFrame, cols: list[str], log_cap: float = 12.0):\n",
        "    X_use = _ensure_cols(X_df, cols)\n",
        "    if isinstance(est, TransformedTargetRegressor):\n",
        "        # prendi i LOG-pred dal regressor interno, poi expm1 safe\n",
        "        log_pred = est.regressor_.predict(X_use)\n",
        "        # diagnostica\n",
        "        q = np.nanpercentile(log_pred, [50, 90, 99, 99.9])\n",
        "        print(f\"log-pred percentiles (TTR inner): p50={q[0]:.3f}, p90={q[1]:.3f}, p99={q[2]:.3f}, p99.9={q[3]:.3f}\")\n",
        "        return _expm1_safe(log_pred, cap=log_cap)\n",
        "    else:\n",
        "        # pipeline RF/XGB allenata su log1p → predict = log-pred\n",
        "        log_pred = est.predict(X_use)\n",
        "        q = np.nanpercentile(log_pred, [50, 90, 99, 99.9])\n",
        "        print(f\"log-pred percentiles (pipe): p50={q[0]:.3f}, p90={q[1]:.3f}, p99={q[2]:.3f}, p99.9={q[3]:.3f}\")\n",
        "        return _expm1_safe(log_pred, cap=log_cap)\n",
        "\n",
        "def _plot_topN(df_imp: pd.DataFrame, val_col: str, title: str, out_path, xlabel: str):\n",
        "    if df_imp is None or not isinstance(df_imp, pd.DataFrame) or df_imp.empty:\n",
        "        print(f\"ℹ️ skip plot: {title} — dataframe vuoto\")\n",
        "        return\n",
        "    top = df_imp.head(20).iloc[::-1]\n",
        "    plt.figure(figsize=(9,6))\n",
        "    ax = top.plot(x=\"feature\", y=val_col, kind=\"barh\", legend=False)\n",
        "    ax.set_title(title); ax.set_xlabel(xlabel)\n",
        "    plt.tight_layout(); plt.savefig(out_path, dpi=150, bbox_inches=\"tight\"); plt.close()\n",
        "    print(\"Saved:\", out_path)\n",
        "\n",
        "# --------------- 1) Feature importances ---------------\n",
        "_plot_topN(\n",
        "    builtin_imp if \"builtin_imp\" in globals() else None,\n",
        "    \"importance\",\n",
        "    \"RF Feature Importance (built-in, top 20)\",\n",
        "    FIG_DIR / \"rf_feature_importance_builtin_top20.png\",\n",
        "    \"Importance\",\n",
        ")\n",
        "_plot_topN(\n",
        "    perm_imp if \"perm_imp\" in globals() else None,\n",
        "    \"importance\",\n",
        "    \"Permutation Importance (ΔR², top 20)\",\n",
        "    FIG_DIR / \"rf_permutation_importance_top20.png\",\n",
        "    \"Importance (mean ΔR²)\",\n",
        ")\n",
        "\n",
        "# --------------- 2) Residuali TEST ---------------\n",
        "if \"df_test\" not in globals():\n",
        "    raise RuntimeError(\"df_test non è definito — impossibile calcolare i residuali.\")\n",
        "\n",
        "y_true_t = df_test[VALUATION_K].to_numpy(dtype=\"float64\", copy=False)\n",
        "\n",
        "# Se esistono pred calcolate prima e coerenti le uso, altrimenti ricalcolo\n",
        "USE_CACHED = False\n",
        "if \"champion\" in globals():\n",
        "    if champion == \"A\" and \"pred_tst_A\" in globals() and isinstance(pred_tst_A, (np.ndarray, list)):\n",
        "        y_pred_t = np.asarray(pred_tst_A, dtype=np.float64); USE_CACHED = True\n",
        "    elif champion == \"B\" and \"pred_tst_B\" in globals() and isinstance(pred_tst_B, (np.ndarray, list)):\n",
        "        y_pred_t = np.asarray(pred_tst_B, dtype=np.float64); USE_CACHED = True\n",
        "\n",
        "if not USE_CACHED:\n",
        "    champ = champion if \"champion\" in globals() else \"A\"\n",
        "    # scegli estimator e lista colonne\n",
        "    if champ == \"A\":\n",
        "        est  = _pick_first(\"ttr_A\", \"pipe_A\", \"chosen_pipe\")\n",
        "        cols = _pick_first(\"features_A\") or ([*cat_cols, *num_cols] if \"cat_cols\" in globals() and \"num_cols\" in globals() else list(df_test.columns))\n",
        "    else:\n",
        "        est  = _pick_first(\"ttr_B\", \"pipe_B\", \"chosen_pipe\")\n",
        "        cols = _pick_first(\"features_B\") or ([*cat_cols, *num_cols_B] if \"cat_cols\" in globals() and \"num_cols_B\" in globals() else list(df_test.columns))\n",
        "    if est is None:\n",
        "        raise RuntimeError(\"Nessuna pipeline (ttr_*/pipe_*) disponibile per ricalcolare le predizioni TEST.\")\n",
        "\n",
        "    print(f\"Estimator scelto: {type(est).__name__} | n_cols={len(cols)} | esempi colonne: {cols[:10]}\")\n",
        "    missing = [c for c in cols if c not in df_test.columns]\n",
        "    if missing:\n",
        "        print(f\"⚠️ Mancano {len(missing)} colonne nel TEST (verranno imputate): {missing[:10]}\")\n",
        "\n",
        "    y_pred_t = _predict_nat_safely(est, df_test, cols, log_cap=float(TRAIN_CFG.get(\"log_cap_clip\", 12.0)))\n",
        "\n",
        "# Garantisci che sia 1D della lunghezza giusta\n",
        "y_pred_t = np.asarray(y_pred_t)\n",
        "if y_pred_t.ndim == 0:\n",
        "    # se è uno scalare (es. NaN), replico per evitare IndexError e fallisco con messaggio chiaro dopo\n",
        "    y_pred_t = np.full_like(y_true_t, fill_value=np.nan, dtype=np.float64)\n",
        "\n",
        "if y_pred_t.shape[0] != y_true_t.shape[0]:\n",
        "    raise RuntimeError(f\"Dimension mismatch: y_pred_t={y_pred_t.shape}, y_true_t={y_true_t.shape}. \"\n",
        "                       \"Controlla la lista colonne usata per il predict.\")\n",
        "\n",
        "non_finite_mask = ~(np.isfinite(y_true_t) & np.isfinite(y_pred_t))\n",
        "n_bad = int(non_finite_mask.sum())\n",
        "if n_bad > 0:\n",
        "    bad_idx = np.where(non_finite_mask)[0][:10].tolist()\n",
        "    print(f\"⚠️ Righe non finite (y_true/y_pred): {n_bad} / {len(y_true_t)}. Esempi idx: {bad_idx}\")\n",
        "\n",
        "mask = np.isfinite(y_true_t) & np.isfinite(y_pred_t)\n",
        "y_true_t = y_true_t[mask]\n",
        "y_pred_t = y_pred_t[mask]\n",
        "\n",
        "if len(y_true_t) < max(10, int(0.3 * len(df_test))):\n",
        "    raise RuntimeError(\n",
        "        \"Predizioni/target TEST contengono troppi NaN/Inf anche dopo clipping.\\n\"\n",
        "        \"- Verifica che la pipeline scelta sia *quella allenata* (ttr_A/ttr_B o pipe_A/pipe_B).\\n\"\n",
        "        \"- Stai passando le *stesse colonne* usate in training (features_A/B)?\\n\"\n",
        "        \"- Guarda i percentili dei log-pred stampati sopra: se sono enormi, aumenta TRAIN_CFG.log_cap_clip o rivedi il modello.\"\n",
        "    )\n",
        "\n",
        "# Residuali e plot\n",
        "residuals = y_true_t - y_pred_t\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.hist(residuals, bins=60, density=True)\n",
        "plt.title(f\"Residuals (TEST, champion {champion if 'champion' in globals() else 'A'})\")\n",
        "plt.xlabel(\"y − ŷ (k€)\"); plt.ylabel(\"Density\")\n",
        "plt.tight_layout()\n",
        "out_res = FIG_DIR / \"rf_residuals_test_hist.png\"\n",
        "plt.savefig(out_res, dpi=150, bbox_inches=\"tight\"); plt.close()\n",
        "print(\"Saved:\", out_res)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.scatter(y_pred_t, residuals, s=10, alpha=0.6)\n",
        "plt.axhline(0.0, linestyle=\"--\")\n",
        "plt.title(f\"Residuals vs Pred (TEST, champion {champion if 'champion' in globals() else 'A'})\")\n",
        "plt.xlabel(\"ŷ (k€)\"); plt.ylabel(\"y − ŷ (k€)\")\n",
        "plt.tight_layout()\n",
        "out_sc = FIG_DIR / \"rf_residuals_vs_pred_test.png\"\n",
        "plt.savefig(out_sc, dpi=150, bbox_inches=\"tight\"); plt.close()\n",
        "print(\"Saved:\", out_sc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02f8b0c9-97a8-474b-aac3-e31c9cdb58c2",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Valuations for Segments (decils e location) & Predictions Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "fe014bd9-acd1-4be4-989a-16455c836842",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\metrics_by_decile.csv\n",
            "Saved: c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\metrics_by_decile.parquet\n",
            "Saved: c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\metrics_by_location.csv\n",
            "Saved: c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\metrics_by_location.parquet\n",
            "Saved: c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\predictions_test.parquet\n",
            "Saved: c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\predictions_test.csv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>asset_id</th>\n",
              "      <th>location</th>\n",
              "      <th>valuation_k</th>\n",
              "      <th>y_pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>241</th>\n",
              "      <td>asset_000241</td>\n",
              "      <td>Verona</td>\n",
              "      <td>365.269989</td>\n",
              "      <td>473.946682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10797</th>\n",
              "      <td>asset_010797</td>\n",
              "      <td>Rome</td>\n",
              "      <td>507.190002</td>\n",
              "      <td>547.681194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8683</th>\n",
              "      <td>asset_008683</td>\n",
              "      <td>Bari</td>\n",
              "      <td>543.369995</td>\n",
              "      <td>599.624442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13467</th>\n",
              "      <td>asset_013467</td>\n",
              "      <td>Palermo</td>\n",
              "      <td>127.379997</td>\n",
              "      <td>114.592825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11405</th>\n",
              "      <td>asset_011405</td>\n",
              "      <td>Turin</td>\n",
              "      <td>1194.609985</td>\n",
              "      <td>1408.986054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14423</th>\n",
              "      <td>asset_014423</td>\n",
              "      <td>Trieste</td>\n",
              "      <td>554.070007</td>\n",
              "      <td>773.618130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7606</th>\n",
              "      <td>asset_007606</td>\n",
              "      <td>Venice</td>\n",
              "      <td>519.700012</td>\n",
              "      <td>488.239930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4930</th>\n",
              "      <td>asset_004930</td>\n",
              "      <td>Venice</td>\n",
              "      <td>384.390015</td>\n",
              "      <td>327.467058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14113</th>\n",
              "      <td>asset_014113</td>\n",
              "      <td>Rome</td>\n",
              "      <td>382.980011</td>\n",
              "      <td>342.861851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11964</th>\n",
              "      <td>asset_011964</td>\n",
              "      <td>Rome</td>\n",
              "      <td>167.779999</td>\n",
              "      <td>161.444375</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           asset_id location  valuation_k       y_pred\n",
              "241    asset_000241   Verona   365.269989   473.946682\n",
              "10797  asset_010797     Rome   507.190002   547.681194\n",
              "8683   asset_008683     Bari   543.369995   599.624442\n",
              "13467  asset_013467  Palermo   127.379997   114.592825\n",
              "11405  asset_011405    Turin  1194.609985  1408.986054\n",
              "14423  asset_014423  Trieste   554.070007   773.618130\n",
              "7606   asset_007606   Venice   519.700012   488.239930\n",
              "4930   asset_004930   Venice   384.390015   327.467058\n",
              "14113  asset_014113     Rome   382.980011   342.861851\n",
              "11964  asset_011964     Rome   167.779999   161.444375"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 08 (refactor) — Breakdown per decili/location + export predizioni TEST (robusto)\n",
        "\n",
        "from __future__ import annotations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "\n",
        "from shared.common.constants import ASSET_ID, LOCATION, VALUATION_K\n",
        "\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- helper numerico ---\n",
        "LOG_CAP = float(TRAIN_CFG.get(\"log_cap_clip\", 12.0))  # limita i log-pred per evitare overflow\n",
        "\n",
        "def _expm1_safe(z, cap: float = LOG_CAP):\n",
        "    z = np.asarray(z, dtype=np.float64)\n",
        "    z = np.clip(z, -20.0, cap)\n",
        "    out = np.expm1(z)\n",
        "    out[out < 0] = 0.0\n",
        "    return out\n",
        "\n",
        "def _ensure_cols(df_part: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
        "    dfp = df_part.copy()\n",
        "    miss = [c for c in cols if c not in dfp.columns]\n",
        "    if miss:\n",
        "        for c in miss:\n",
        "            dfp[c] = np.nan\n",
        "        print(f\"ℹ️ Aggiunte {len(miss)} colonne mancanti allo split TEST (imputate): {miss[:10]}\")\n",
        "    return dfp[cols]\n",
        "\n",
        "def _pick_first(*names):\n",
        "    for n in names:\n",
        "        if n in globals() and globals()[n] is not None:\n",
        "            return globals()[n]\n",
        "    return None\n",
        "\n",
        "def _predict_nat_test_from(est, X_df: pd.DataFrame, cols: list[str]) -> np.ndarray:\n",
        "    \"\"\"Predizioni in scala naturale (k€) con auto-gestione TTR vs pipeline log1p.\"\"\"\n",
        "    X_use = _ensure_cols(X_df, cols)\n",
        "    if isinstance(est, TransformedTargetRegressor):\n",
        "        pred_nat = est.predict(X_use)                # già in k€\n",
        "    else:\n",
        "        log_pred = est.predict(X_use)                # log1p(y)\n",
        "        pred_nat = _expm1_safe(log_pred)\n",
        "    return np.asarray(pred_nat, dtype=np.float64)\n",
        "\n",
        "# --- helper: groupby.apply compatibile con pandas nuovi/vecchi\n",
        "def _group_apply_safe(df: pd.DataFrame, key: str, func):\n",
        "    gb = df.groupby(key, observed=True)\n",
        "    try:\n",
        "        # pandas ≥ 2.2: esclude automaticamente la colonna di raggruppamento\n",
        "        out = gb.apply(func, include_groups=False)\n",
        "    except TypeError:\n",
        "        # pandas < 2.2: selezioniamo esplicitamente solo le colonne utili\n",
        "        out = gb[[\"y_true\", \"y_pred\"]].apply(func)\n",
        "    return out.reset_index()\n",
        "\n",
        "# --- prendi/ricostruisci y_pred_t in scala naturale (k€) ---\n",
        "def _get_champion_pred_test() -> np.ndarray:\n",
        "    # usa cache se valida\n",
        "    if \"y_pred_t\" in globals() and isinstance(y_pred_t, (np.ndarray, list)) and len(y_pred_t) == len(df_test):\n",
        "        return np.asarray(y_pred_t, dtype=np.float64)\n",
        "\n",
        "    if \"champion\" not in globals():\n",
        "        raise RuntimeError(\"Variabile 'champion' mancante.\")\n",
        "\n",
        "    # pred già calcolate in k€?\n",
        "    if champion == \"A\" and \"pred_tst_A\" in globals():\n",
        "        return np.asarray(pred_tst_A, dtype=np.float64)\n",
        "    if champion == \"B\" and \"pred_tst_B\" in globals():\n",
        "        return np.asarray(pred_tst_B, dtype=np.float64)\n",
        "\n",
        "    # altrimenti ricalcola\n",
        "    if champion == \"A\":\n",
        "        est  = _pick_first(\"ttr_A\", \"pipe_A\", \"chosen_pipe\")\n",
        "        cols = _pick_first(\"features_A\") or ([*cat_cols, *num_cols] if \"cat_cols\" in globals() and \"num_cols\" in globals() else list(df_test.columns))\n",
        "    else:\n",
        "        est  = _pick_first(\"ttr_B\", \"pipe_B\", \"chosen_pipe\")\n",
        "        cols = _pick_first(\"features_B\") or ([*cat_cols, *num_cols_B] if \"cat_cols\" in globals() and \"num_cols_B\" in globals() else list(df_test.columns))\n",
        "\n",
        "    if est is None:\n",
        "        raise RuntimeError(\"Nessuna pipeline disponibile per ricalcolare le predizioni TEST.\")\n",
        "\n",
        "    return _predict_nat_test_from(est, df_test, cols)\n",
        "\n",
        "# --- costruisci dataset metrico pulito ---\n",
        "y_true_t = df_test[VALUATION_K].to_numpy(dtype=np.float64, copy=False)\n",
        "y_pred_t = _get_champion_pred_test()\n",
        "\n",
        "# sanifica NaN/Inf\n",
        "mask = np.isfinite(y_true_t) & np.isfinite(y_pred_t)\n",
        "valid_n = int(mask.sum())\n",
        "if valid_n < max(30, int(0.3 * len(y_true_t))):\n",
        "    bad_idx = np.where(~mask)[0][:10].tolist()\n",
        "    raise RuntimeError(\n",
        "        f\"Predizioni TEST non finite: validi {valid_n}/{len(mask)}. \"\n",
        "        f\"Esempi idx problematici: {bad_idx}. Verifica allineamento feature e scala predizioni.\"\n",
        "    )\n",
        "\n",
        "yt = y_true_t[mask]\n",
        "yh = y_pred_t[mask]\n",
        "idx_valid = df_test.index[mask]\n",
        "\n",
        "cols_keep = [ASSET_ID] if ASSET_ID in df_test.columns else []\n",
        "if LOCATION in df_test.columns:\n",
        "    cols_keep.append(LOCATION)\n",
        "\n",
        "dfm = df_test.loc[idx_valid, cols_keep].copy()\n",
        "dfm[\"y_true\"] = yt\n",
        "dfm[\"y_pred\"] = yh\n",
        "\n",
        "# --- decili sul target naturale ---\n",
        "try:\n",
        "    dfm[\"decile\"] = pd.qcut(dfm[\"y_true\"], q=10, labels=False, duplicates=\"drop\")\n",
        "except Exception:\n",
        "    dfm[\"decile\"] = 0\n",
        "\n",
        "# --- funzioni metriche ---\n",
        "def _rmse(y_true, y_pred):\n",
        "    try:\n",
        "        return mean_squared_error(y_true, y_pred, squared=False)\n",
        "    except TypeError:\n",
        "        return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "def _agg_metrics(g: pd.DataFrame) -> pd.Series:\n",
        "    return pd.Series({\n",
        "        \"n\": int(len(g)),\n",
        "        \"MAE\": float(mean_absolute_error(g[\"y_true\"], g[\"y_pred\"])),\n",
        "        \"RMSE\": float(_rmse(g[\"y_true\"], g[\"y_pred\"])),\n",
        "        \"R2\": float(r2_score(g[\"y_true\"], g[\"y_pred\"])) if len(g) > 1 else np.nan,\n",
        "    })\n",
        "\n",
        "# --- Breakdown per decile ---\n",
        "dec_rep = _group_apply_safe(dfm, \"decile\", _agg_metrics)\n",
        "dec_rep.to_csv(MODEL_DIR / \"metrics_by_decile.csv\", index=False)\n",
        "dec_rep.to_parquet(MODEL_DIR / \"metrics_by_decile.parquet\", index=False)\n",
        "print(\"Saved:\", MODEL_DIR / \"metrics_by_decile.csv\")\n",
        "print(\"Saved:\", MODEL_DIR / \"metrics_by_decile.parquet\")\n",
        "\n",
        "# --- Breakdown per location ---\n",
        "if LOCATION in dfm.columns:\n",
        "    loc_rep = _group_apply_safe(dfm, LOCATION, _agg_metrics)\n",
        "else:\n",
        "    loc_rep = pd.DataFrame([{\n",
        "        LOCATION: \"NA\",\n",
        "        \"n\": int(len(dfm)),\n",
        "        \"MAE\": float(mean_absolute_error(dfm[\"y_true\"], dfm[\"y_pred\"])) if len(dfm) else np.nan,\n",
        "        \"RMSE\": float(_rmse(dfm[\"y_true\"], dfm[\"y_pred\"])) if len(dfm) else np.nan,\n",
        "        \"R2\": float(r2_score(dfm[\"y_true\"], dfm[\"y_pred\"])) if len(dfm) > 1 else np.nan,\n",
        "    }])\n",
        "\n",
        "loc_rep.to_csv(MODEL_DIR / \"metrics_by_location.csv\", index=False)\n",
        "loc_rep.to_parquet(MODEL_DIR / \"metrics_by_location.parquet\", index=False)\n",
        "print(\"Saved:\", MODEL_DIR / \"metrics_by_location.csv\")\n",
        "print(\"Saved:\", MODEL_DIR / \"metrics_by_location.parquet\")\n",
        "\n",
        "# --- export predizioni TEST ---\n",
        "pred_cols = []\n",
        "if ASSET_ID in dfm.columns: pred_cols.append(ASSET_ID)\n",
        "if LOCATION in dfm.columns: pred_cols.append(LOCATION)\n",
        "pred_df = dfm[pred_cols + [\"y_true\", \"y_pred\"]].rename(columns={\"y_true\": VALUATION_K})\n",
        "\n",
        "pred_df.to_parquet(MODEL_DIR / \"predictions_test.parquet\", index=False)\n",
        "pred_df.to_csv(MODEL_DIR / \"predictions_test.csv\", index=False, encoding=\"utf-8\")\n",
        "print(\"Saved:\", MODEL_DIR / \"predictions_test.parquet\")\n",
        "print(\"Saved:\", MODEL_DIR / \"predictions_test.csv\")\n",
        "\n",
        "display(pred_df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bf1f075-89a4-4e49-a30c-2031f17ab419",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Model Persistence & Manifest Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "701beb69",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[pick] champion = A\n",
            "Pipeline(steps=[('prep',\n",
            "                 ColumnTransformer(transformers=[('cat',\n",
            "                                                  Pipeline(steps=[('impute',\n",
            "                                                                   SimpleImputer(fill_value='__MISSING__',\n",
            "                                                                                 strategy='constant')),\n",
            "                                                                  ('encode',\n",
            "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
            "                                                  ['zone', 'urban_type',\n",
            "                                                   'region', 'energy_class',\n",
            "                                                   'condition', 'heating',\n",
            "                                                   'view',\n",
            "                                                   'public_transport_nearby']),\n",
            "                                                 ('num',\n",
            "                                                  Pipeline(steps=[('impute',\n",
            "                                                                   SimpleImputer...\n",
            "                                                   'bathrooms', 'year_built',\n",
            "                                                   'floor', 'building_floors',\n",
            "                                                   'is_top_floor',\n",
            "                                                   'has_elevator', 'has_garden',\n",
            "                                                   'has_balcony', 'log_size_m2',\n",
            "                                                   'sqm_per_room',\n",
            "                                                   'baths_per_100sqm',\n",
            "                                                   'elev_x_floor',\n",
            "                                                   'no_elev_high_floor',\n",
            "                                                   'rooms_per_100sqm',\n",
            "                                                   'city_zone_prior',\n",
            "                                                   'region_index_prior'])])),\n",
            "                ('model',\n",
            "                 RandomForestRegressor(min_samples_leaf=2, n_estimators=400,\n",
            "                                       n_jobs=-1, random_state=42))])\n",
            "[cols] da features_A\n",
            "✅ Champion: A | n_cols=26 | pipe=Pipeline\n",
            "   preview cols: ['zone', 'urban_type', 'region', 'energy_class', 'condition', 'heating', 'view', 'public_transport_nearby', 'size_m2', 'rooms', 'bathrooms', 'year_built', 'floor', 'building_floors', 'is_top_floor']\n"
          ]
        }
      ],
      "source": [
        "# === 08.9 SAFE: chosen_pipe/chosen_cols dal manifest (no introspezione pipeline) ===\n",
        "from __future__ import annotations\n",
        "import os, json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# (1) Limita thread BLAS/OpenMP (aiuta con crash random del kernel)\n",
        "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
        "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
        "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
        "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
        "\n",
        "def _load_features_from_manifest() -> list[str]:\n",
        "    base = Path(\"outputs/modeling/property\")\n",
        "    if not base.exists():\n",
        "        return []\n",
        "    # prova a trovare un manifest accanto al joblib\n",
        "    cand = []\n",
        "    for p in base.glob(\"value_regressor_v*.manifest*.json\"):\n",
        "        cand.append(p)\n",
        "    if not cand:\n",
        "        for p in base.glob(\"*.manifest*.json\"):\n",
        "            cand.append(p)\n",
        "    if not cand:\n",
        "        return []\n",
        "    # prendi il più recente\n",
        "    man = max(cand, key=lambda p: p.stat().st_mtime)\n",
        "    with man.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        meta = json.load(f)\n",
        "    feats = (\n",
        "        meta.get(\"feature_order\")\n",
        "        or meta.get(\"expected_features\")\n",
        "        or meta.get(\"features\")\n",
        "        or meta.get(\"input_features\")\n",
        "        or []\n",
        "    )\n",
        "    # normalizza e dedup preservando ordine\n",
        "    out, seen = [], set()\n",
        "    for c in feats:\n",
        "        c = str(c)\n",
        "        if c not in seen:\n",
        "            seen.add(c)\n",
        "            out.append(c)\n",
        "    print(f\"[features] dal manifest: {len(out)} (file: {man.name})\")\n",
        "    if out[:10]:\n",
        "        print(\"           preview:\", out[:10])\n",
        "    return out\n",
        "\n",
        "# 0) Champion\n",
        "if \"champion\" not in globals():\n",
        "    if \"mA_val\" in globals() and \"mB_val\" in globals():\n",
        "        champion = \"A\" if mA_val[\"MAE\"] <= mB_val[\"MAE\"] else \"B\"\n",
        "    elif \"pipe_A\" in globals():\n",
        "        champion = \"A\"\n",
        "    elif \"pipe_B\" in globals():\n",
        "        champion = \"B\"\n",
        "    else:\n",
        "        champion = \"A\"\n",
        "print(f\"[pick] champion = {champion}\")\n",
        "\n",
        "# 1) chosen_pipe (no introspezione)\n",
        "chosen_pipe = pipe_A\n",
        "print(pipe_A)\n",
        "# if champion == \"A\" and \"pipe_A\" in globals() and isinstance(pipe_A, Pipeline):\n",
        "#     chosen_pipe = pipe_A                          \n",
        "# elif champion == \"B\" and \"pipe_B\" in globals() and isinstance(pipe_B, Pipeline):\n",
        "#     chosen_pipe = pipe_B\n",
        "# elif \"fast_pipe\" in globals() and isinstance(fast_pipe, Pipeline):\n",
        "#     chosen_pipe = fast_pipe\n",
        "# if chosen_pipe is None:\n",
        "#     raise RuntimeError(\"⚠️ Nessuna Pipeline fittata disponibile.\")\n",
        "\n",
        "# 2) chosen_cols: manifest → features_* → cat/num → X_train_cv → X_train → df_train\n",
        "chosen_cols: list[str] = []\n",
        "\n",
        "# a) manifest\n",
        "chosen_cols = _load_features_from_manifest()\n",
        "\n",
        "# b) features_* nel contesto\n",
        "if not chosen_cols and champion == \"A\" and \"features_A\" in globals() and features_A:\n",
        "    chosen_cols = list(dict.fromkeys(map(str, features_A))); print(\"[cols] da features_A\")\n",
        "elif not chosen_cols and champion == \"B\" and \"features_B\" in globals() and features_B:\n",
        "    chosen_cols = list(dict.fromkeys(map(str, features_B))); print(\"[cols] da features_B\")\n",
        "\n",
        "# c) cat/num locali\n",
        "if not chosen_cols and \"cat_cols\" in globals() and \"num_cols\" in globals() and cat_cols and num_cols:\n",
        "    chosen_cols = list(dict.fromkeys([*(str(c) for c in cat_cols), *(str(c) for c in num_cols)]))\n",
        "    print(\"[cols] da cat_cols + num_cols\")\n",
        "\n",
        "# d) fallback dai dataframe già in RAM (nessuna trasformazione)\n",
        "if not chosen_cols and \"X_train_cv\" in globals() and isinstance(X_train_cv, pd.DataFrame):\n",
        "    chosen_cols = list(map(str, X_train_cv.columns)); print(\"[cols] da X_train_cv\")\n",
        "if not chosen_cols and \"X_train\" in globals() and isinstance(X_train, pd.DataFrame):\n",
        "    chosen_cols = list(map(str, X_train.columns)); print(\"[cols] da X_train\")\n",
        "if not chosen_cols and \"df_train\" in globals() and isinstance(df_train, pd.DataFrame):\n",
        "    drop_like = {\"valuation_k\", \"price_per_sqm\", \"sample_weight\"}\n",
        "    chosen_cols = [c for c in map(str, df_train.columns) if c not in drop_like]\n",
        "    print(\"[cols] da df_train (fallback)\")\n",
        "\n",
        "# 3) Filtra sulle colonne effettivamente presenti in df_train (se disponibile)\n",
        "if \"df_train\" in globals() and isinstance(df_train, pd.DataFrame) and chosen_cols:\n",
        "    before = len(chosen_cols)\n",
        "    chosen_cols = [c for c in chosen_cols if c in df_train.columns]\n",
        "    if len(chosen_cols) != before:\n",
        "        print(f\"[cols] filtrate su df_train: {before} → {len(chosen_cols)}\")\n",
        "\n",
        "print(f\"✅ Champion: {champion} | n_cols={len(chosen_cols)} | pipe={'None' if chosen_pipe is None else type(chosen_pipe).__name__}\")\n",
        "if chosen_cols:\n",
        "    print(\"   preview cols:\", chosen_cols[:15])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "a90328c1-1e44-4551-a9e9-bb18e4a8817c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Saved serving pipeline (no-refit): outputs\\modeling\\property\\value_regressor_v2.joblib\n",
            "✅ Saved feature_order: outputs\\modeling\\property\\feature_order.json\n",
            "✅ Saved meta: outputs\\modeling\\property\\value_regressor_v2_meta.json\n",
            "✅ Saved manifest: outputs\\modeling\\property\\training_manifest.json\n"
          ]
        }
      ],
      "source": [
        "# === 09) Persistenza modello (SAFE, no-refit) → notebooks/outputs/modeling/property ===\n",
        "from __future__ import annotations\n",
        "import os, json, hashlib\n",
        "from datetime import datetime, timezone\n",
        "from pathlib import Path\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Limita thread BLAS/OpenMP (stabilità)\n",
        "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
        "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
        "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
        "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
        "\n",
        "# ---- check base ----\n",
        "from scripts.model_registry import _is_fitted_pipeline\n",
        "assert _is_fitted_pipeline(chosen_pipe), \"chosen_pipe non fittato: riesegui la cella di training.\"\n",
        "\n",
        "\n",
        "# ---- trasformatori leggeri, importabili (pickle-safe) ----\n",
        "from shared.common.serving_transformers import GeoCanonizer, PriorsGuard\n",
        "try:\n",
        "    from shared.common.config import ASSET_CONFIG\n",
        "    _PROP = ASSET_CONFIG[\"property\"]\n",
        "    CITY_BASE = {str(c).lower(): {str(z).lower(): float(v) for z, v in d.items()}\n",
        "                 for c, d in (_PROP.get(\"city_base_prices\") or {}).items()}\n",
        "    REGION_INDEX = {str(k).lower(): float(v) for k, v in (_PROP.get(\"region_index\") or {\n",
        "        \"north\": 1.05, \"center\": 1.00, \"south\": 0.92\n",
        "    }).items()}\n",
        "except Exception:\n",
        "    CITY_BASE = {}\n",
        "    REGION_INDEX = {\"north\": 1.05, \"center\": 1.00, \"south\": 0.92}\n",
        "_ZONE_KEYS = set(z for d in CITY_BASE.values() for z in d.keys())\n",
        "_ZONE_MED = {z: float(np.nanmedian([d.get(z, np.nan) for d in CITY_BASE.values()]))\n",
        "             for z in _ZONE_KEYS} if CITY_BASE else {}\n",
        "_GLOBAL_CITYZONE_MED = float(np.nanmedian([v for d in CITY_BASE.values() for v in d.values()])) if CITY_BASE else 0.0\n",
        "\n",
        "# ---- wrapper pipeline per serving: NON rialleniamo niente ----\n",
        "# Se la tua pipeline già contiene derive/priors, questi step faranno essentially pass-through.\n",
        "serving_pipe = Pipeline(steps=[\n",
        "    (\"canon_geo\",   GeoCanonizer()),\n",
        "    (\"priors_guard\",PriorsGuard(\n",
        "        city_base=CITY_BASE,\n",
        "        region_index=REGION_INDEX,\n",
        "        zone_medians=_ZONE_MED,\n",
        "        global_cityzone_median=_GLOBAL_CITYZONE_MED,\n",
        "    )),\n",
        "    (\"core\",        chosen_pipe),   # ← modello già addestrato\n",
        "])\n",
        "\n",
        "# ---- paths (coerenti con l'API/start.bat) ----\n",
        "BASE_OUT = Path(\"outputs\")\n",
        "MODEL_DIR = BASE_OUT / \"modeling\"\n",
        "PROP_DIR  = MODEL_DIR / \"property\"\n",
        "FIG_DIR   = MODEL_DIR / \"figures\"\n",
        "ART_DIR   = MODEL_DIR / \"artifacts\"\n",
        "for d in (MODEL_DIR, PROP_DIR, FIG_DIR, ART_DIR):\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---- salvataggi ----\n",
        "def _sha256_file(p: Path, chunk: int = 1 << 20) -> str:\n",
        "    h = hashlib.sha256()\n",
        "    with p.open(\"rb\") as f:\n",
        "        for ch in iter(lambda: f.read(chunk), b\"\"):\n",
        "            h.update(ch)\n",
        "    return h.hexdigest()\n",
        "\n",
        "# 1) feature_order (solo raw usate dall’API)\n",
        "feature_order = list(map(str, chosen_cols))\n",
        "FEATURES_FILE = (PROP_DIR / \"feature_order.json\")\n",
        "FEATURES_FILE.write_text(json.dumps(feature_order, ensure_ascii=False, separators=(\",\", \":\")), encoding=\"utf-8\")\n",
        "feature_order_sha256 = hashlib.sha256(FEATURES_FILE.read_bytes()).hexdigest()\n",
        "\n",
        "# 2) pipeline\n",
        "pipe_path = PROP_DIR / \"value_regressor_v2.joblib\"\n",
        "joblib.dump(serving_pipe, pipe_path)\n",
        "_loaded = joblib.load(pipe_path)\n",
        "assert _is_fitted_pipeline(_loaded), \"Pipeline salvata non risulta fittata.\"\n",
        "pipeline_sha = _sha256_file(pipe_path)\n",
        "\n",
        "# 3) meta\n",
        "meta_path = PROP_DIR / \"value_regressor_v2_meta.json\"\n",
        "model_meta = {\n",
        "    \"asset_type\": \"property\",\n",
        "    \"task\": \"value_regressor\",\n",
        "    \"model_version\": \"v2\",\n",
        "    \"model_class\": type(chosen_pipe.steps[-1][1]).__name__,\n",
        "    \"trained_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),\n",
        "    \"schema_version\": \"2.0\",\n",
        "    \"model_hash\": pipeline_sha,              # p1.mh\n",
        "    \"pipeline_sha256\": pipeline_sha,\n",
        "    \"feature_order_sha256\": feature_order_sha256,\n",
        "    \"n_features\": int(len(feature_order)),\n",
        "    \"features_categorical\": [c for c in feature_order if c in globals().get(\"cat_cols\", [])],\n",
        "    \"features_numeric\": [c for c in feature_order if c not in globals().get(\"cat_cols\", [])],\n",
        "    \"target_name\": \"valuation_k\",\n",
        "    \"unit\": \"k_eur\",\n",
        "    \"feature_order_path\": str(FEATURES_FILE.resolve().as_posix()),\n",
        "    \"pipeline_path\": str(pipe_path.resolve().as_posix()),\n",
        "}\n",
        "meta_path.write_text(json.dumps(model_meta, ensure_ascii=False, separators=(\",\", \":\")), encoding=\"utf-8\")\n",
        "\n",
        "# 4) manifest (minimo indispensabile per l’API)\n",
        "manifest_path = PROP_DIR / \"training_manifest.json\"\n",
        "paths = {\n",
        "    \"pipeline\": str(pipe_path.resolve().as_posix()),\n",
        "    \"manifest\": str(manifest_path.resolve().as_posix()),\n",
        "    \"feature_order\": str(FEATURES_FILE.resolve().as_posix()),\n",
        "}\n",
        "metrics = globals().get(\"metrics\", {}) or {}\n",
        "feature_config = {\n",
        "    \"categorical\": model_meta[\"features_categorical\"],\n",
        "    \"numeric\": model_meta[\"features_numeric\"],\n",
        "}\n",
        "manifest = {\n",
        "    \"generated_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),\n",
        "    \"schema_version\": \"2.0\",\n",
        "    \"asset_type\": \"property\",\n",
        "    \"task\": \"value_regressor\",\n",
        "    \"paths\": paths,\n",
        "    \"model_meta\": {\n",
        "        \"model_version\": model_meta[\"model_version\"],\n",
        "        \"model_class\": model_meta[\"model_class\"],\n",
        "        \"model_hash\": model_meta[\"model_hash\"],\n",
        "        \"feature_order_sha256\": feature_order_sha256,\n",
        "    },\n",
        "    \"metrics\": metrics,\n",
        "    \"feature_order\": feature_order,  # <- comodo se vuoi leggerlo direttamente\n",
        "    \"expected_features\": feature_config,\n",
        "}\n",
        "manifest_path.write_text(json.dumps(manifest, ensure_ascii=False, separators=(\",\", \":\")), encoding=\"utf-8\")\n",
        "\n",
        "print(\"✅ Saved serving pipeline (no-refit):\", pipe_path)\n",
        "print(\"✅ Saved feature_order:\", FEATURES_FILE)\n",
        "print(\"✅ Saved meta:\", meta_path)\n",
        "print(\"✅ Saved manifest:\", manifest_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "b009b74b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Worst-k salvato: outputs\\modeling\\worst_k.json\n"
          ]
        }
      ],
      "source": [
        "# === Worst-k 10% ===\n",
        "from shared.common.utils import canonical_json_dumps\n",
        "import numpy as np, json\n",
        "from pathlib import Path\n",
        "\n",
        "def _worst_k(y_true: np.ndarray, y_pred: np.ndarray, k: float = 0.10) -> dict:\n",
        "    err = np.abs(y_true - y_pred).astype(float)\n",
        "    n = max(1, int(len(err) * k))\n",
        "    top = np.partition(err, -n)[-n:]\n",
        "    return {\n",
        "        \"worst_k\": float(k),\n",
        "        \"worst_k_mean_abs_err\": float(top.mean()),\n",
        "        \"worst_k_max_abs_err\": float(top.max()),\n",
        "        \"worst_k_count\": int(n),\n",
        "    }\n",
        "\n",
        "# Sostituisci dfm_* con il df che usi per le metriche finali (deve avere y_true / y_pred)\n",
        "y_true_np = dfm[\"y_true\"].to_numpy()\n",
        "y_pred_np = dfm[\"y_pred\"].to_numpy()\n",
        "\n",
        "wk = _worst_k(y_true_np, y_pred_np, k=0.10)\n",
        "wk_path = Path(\"outputs/modeling/worst_k.json\")\n",
        "wk_path.write_text(canonical_json_dumps(wk), encoding=\"utf-8\")\n",
        "\n",
        "# Aggiorna manifest training\n",
        "PROP_DIR = Path(\"outputs/modeling/property\")\n",
        "mpath = PROP_DIR / \"training_manifest.json\"\n",
        "m = json.loads(mpath.read_text(encoding=\"utf-8\")) if mpath.exists() else {}\n",
        "mx = m.setdefault(\"metrics\", {})\n",
        "mx[\"worst_k_10pct\"] = wk\n",
        "mpath.write_text(canonical_json_dumps(m), encoding=\"utf-8\")\n",
        "\n",
        "print(\"✅ Worst-k salvato:\", wk_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "d2088df6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # === Golden parity set (5 record deterministici dal TEST) ===\n",
        "# from shared.common.utils import canonical_json_dumps\n",
        "# import numpy as np, json\n",
        "# from pathlib import Path\n",
        "\n",
        "# MODEL_DIR = Path(\"outputs/modeling\")\n",
        "# PROP_DIR = MODEL_DIR / \"property\"\n",
        "\n",
        "# # Assumiamo che df_test e feature_order esistano già\n",
        "# rng = np.random.default_rng(SEED)\n",
        "# take = min(5, len(df_test))\n",
        "# idx = sorted(rng.choice(df_test.index, size=take, replace=False).tolist())\n",
        "\n",
        "# golden_inputs = df_test.loc[idx, feature_order].copy()\n",
        "# golden_inputs_path = MODEL_DIR / \"golden_inputs.json\"\n",
        "# golden_inputs_path.write_text(\n",
        "#     canonical_json_dumps(json.loads(golden_inputs.to_json(orient=\"records\"))),\n",
        "#     encoding=\"utf-8\"\n",
        "# )\n",
        "\n",
        "# # Predizioni con la pipeline servibile già fittata (serving_pipe_v2)\n",
        "# y_pred_golden = serving_pipe_v2.predict(df_test.loc[idx, feature_order])\n",
        "# golden_train_preds = pd.DataFrame({\n",
        "#     \"index\": idx,\n",
        "#     VALUATION_K: df_test.loc[idx, VALUATION_K].to_numpy(dtype=np.float64),\n",
        "#     \"y_pred\": y_pred_golden.astype(float),\n",
        "# })\n",
        "# golden_train_preds_path = MODEL_DIR / \"golden_predictions_train.csv\"\n",
        "# golden_train_preds.to_csv(golden_train_preds_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "# # Aggiorna manifest training con i path golden\n",
        "# manifest_path = PROP_DIR / \"training_manifest.json\"\n",
        "# m = json.loads(manifest_path.read_text(encoding=\"utf-8\")) if manifest_path.exists() else {}\n",
        "# paths = m.setdefault(\"paths\", {})\n",
        "# paths.update({\n",
        "#     \"golden_inputs\": str(golden_inputs_path.resolve().as_posix()),\n",
        "#     \"golden_predictions_train\": str(golden_train_preds_path.resolve().as_posix()),\n",
        "# })\n",
        "# manifest_path.write_text(canonical_json_dumps(m), encoding=\"utf-8\")\n",
        "\n",
        "# print(\"✅ Golden set:\", golden_inputs_path.name, golden_train_preds_path.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "6a09e003",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved drift metrics → outputs/modeling/location_drift_train_vs_test.json  (manifest aggiornato: outputs/modeling/property/training_manifest.json)\n"
          ]
        }
      ],
      "source": [
        "# 11) Post-training drift check (location) — firma: compute_location_drift(df, target_weights, tolerance)\n",
        "from __future__ import annotations\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Import robusto della funzione (firma: df, target_weights, tolerance)\n",
        "try:\n",
        "    from shared.n03_train_model.metrics import compute_location_drift  # <- richiede (df, target_weights, tolerance)\n",
        "    _HAS_CLD = True\n",
        "except Exception:\n",
        "    _HAS_CLD = False\n",
        "\n",
        "# 2) Contesto: path & config (già definiti a inizio notebook)\n",
        "manifest_path = PROP_DIR / \"training_manifest.json\"  # es.: notebooks/outputs/modeling/property/training_manifest.json\n",
        "try:\n",
        "    manifest = json.loads(manifest_path.read_text(encoding=\"utf-8\")) if manifest_path.exists() else {}\n",
        "except Exception:\n",
        "    manifest = {}\n",
        "\n",
        "TRAIN_CFG = globals().get(\"TRAIN_CFG\", {}) or {}\n",
        "LOCATION  = globals().get(\"LOCATION\", \"location\")\n",
        "TOL       = float(TRAIN_CFG.get(\"drift_tolerance\", 0.05))  # default 5%\n",
        "\n",
        "# 3) Helpers -----------------------------------------------------------------\n",
        "def _norm_weights(d: dict) -> dict[str, float]:\n",
        "    \"\"\"Normalizza pesi (>=0) per sommare a 1.0; ignora chiavi con pesi negativi/non numerici.\"\"\"\n",
        "    clean = {str(k): float(v) for k, v in d.items() if pd.api.types.is_number(v) and float(v) >= 0.0}\n",
        "    s = float(sum(clean.values()))\n",
        "    if s <= 0:\n",
        "        return {k: 0.0 for k in clean}\n",
        "    return {k: v / s for k, v in clean.items()}\n",
        "\n",
        "def _empirical_weights(df_like: pd.DataFrame, col: str) -> dict[str, float]:\n",
        "    if not isinstance(df_like, pd.DataFrame) or col not in df_like.columns:\n",
        "        return {}\n",
        "    vc = df_like[col].dropna().astype(str).value_counts(normalize=True)\n",
        "    return {k: float(v) for k, v in vc.items()}\n",
        "\n",
        "def _fallback_drift(df_like: pd.DataFrame, target_w: dict[str, float]) -> dict:\n",
        "    \"\"\"Fallback semplice: differenze assolute + ratio + TVD/JSD.\"\"\"\n",
        "    emp = _empirical_weights(df_like, LOCATION)\n",
        "    keys = sorted(set(emp) | set(target_w))\n",
        "    p = np.array([emp.get(k, 0.0) for k in keys], dtype=float)\n",
        "    q = np.array([target_w.get(k, 0.0) for k in keys], dtype=float)\n",
        "    eps = 1e-12\n",
        "    p = np.clip(p, eps, 1.0); q = np.clip(q, eps, 1.0)\n",
        "    p /= p.sum(); q /= q.sum()\n",
        "    m = 0.5 * (p + q)\n",
        "    jsd = float(0.5 * (np.sum(p * (np.log(p) - np.log(m))) + np.sum(q * (np.log(q) - np.log(m)))))\n",
        "    tvd = float(0.5 * np.abs(p - q).sum())\n",
        "    report = {\n",
        "        \"method\": \"fallback_jsd_tvd\",\n",
        "        \"JSD\": jsd,\n",
        "        \"TVD\": tvd,\n",
        "        \"per_location\": {}\n",
        "    }\n",
        "    for k in keys:\n",
        "        emp_k = emp.get(k, 0.0); tgt_k = target_w.get(k, 0.0)\n",
        "        diff = emp_k - tgt_k\n",
        "        report[\"per_location\"][k] = {\n",
        "            \"target_weight\": tgt_k,\n",
        "            \"empirical_weight\": emp_k,\n",
        "            \"difference\": diff,\n",
        "            \"drifted\": bool(abs(diff) > TOL),\n",
        "            \"ratio\": (emp_k / tgt_k) if tgt_k > 0 else float(\"inf\")\n",
        "        }\n",
        "    return report\n",
        "\n",
        "# 4) Scegli scenario: baseline da config OPPURE train vs test ---------------\n",
        "baseline_cfg = (TRAIN_CFG.get(\"expected_profile\", {}) or {}).get(\"location_distribution\", {}) or None\n",
        "\n",
        "try:\n",
        "    if baseline_cfg:\n",
        "        # Scenario A: confronto dataset complessivo vs baseline attesa\n",
        "        if \"df\" not in globals():\n",
        "            raise RuntimeError(\"df non disponibile per il drift vs baseline.\")\n",
        "        target_w = _norm_weights(baseline_cfg)\n",
        "        if _HAS_CLD:\n",
        "            drift_result = compute_location_drift(df, target_w, TOL)\n",
        "        else:\n",
        "            drift_result = _fallback_drift(df, target_w)\n",
        "        out_path = MODEL_DIR / \"location_drift_vs_expected.json\"\n",
        "        out_key  = \"location_drift_vs_expected\"\n",
        "    else:\n",
        "        # Scenario B: train vs test — usa la distribuzione del TEST come target_weights\n",
        "        if \"df_train\" not in globals() or \"df_test\" not in globals():\n",
        "            raise RuntimeError(\"df_train/df_test non disponibili per il drift train vs test.\")\n",
        "        tgt = _empirical_weights(df_test, LOCATION)\n",
        "        target_w = _norm_weights(tgt)\n",
        "        if _HAS_CLD:\n",
        "            drift_result = compute_location_drift(df_train, target_w, TOL)\n",
        "        else:\n",
        "            drift_result = _fallback_drift(df_train, target_w)\n",
        "        out_path = MODEL_DIR / \"location_drift_train_vs_test.json\"\n",
        "        out_key  = \"location_drift_train_vs_test\"\n",
        "\n",
        "except Exception as e:\n",
        "    # Fallback totale (se qualcosa va storto nelle ramificazioni sopra)\n",
        "    try:\n",
        "        logger.info(\"compute_location_drift non disponibile/errore (%s). Uso fallback semplice.\", e)\n",
        "    except Exception:\n",
        "        print(f\"compute_location_drift non disponibile/errore ({e}). Uso fallback semplice.\")\n",
        "    if baseline_cfg and \"df\" in globals():\n",
        "        drift_result = _fallback_drift(df, _norm_weights(baseline_cfg))\n",
        "        out_path = MODEL_DIR / \"location_drift_vs_expected.json\"\n",
        "        out_key  = \"location_drift_vs_expected\"\n",
        "    elif \"df_train\" in globals() and \"df_test\" in globals():\n",
        "        drift_result = _fallback_drift(df_train, _norm_weights(_empirical_weights(df_test, LOCATION)))\n",
        "        out_path = MODEL_DIR / \"location_drift_train_vs_test.json\"\n",
        "        out_key  = \"location_drift_train_vs_test\"\n",
        "    else:\n",
        "        raise\n",
        "\n",
        "# 5) Persistenza output + aggiornamento manifest ----------------------------\n",
        "out_path.write_text(canonical_json_dumps(drift_result), encoding=\"utf-8\")\n",
        "\n",
        "m = dict(manifest.get(\"metrics\", {}))\n",
        "m[out_key] = drift_result\n",
        "manifest[\"metrics\"] = m\n",
        "manifest_path.write_text(canonical_json_dumps(manifest), encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Saved drift metrics → {out_path.as_posix()}  (manifest aggiornato: {manifest_path.as_posix()})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bbc1d70-e1f4-48ce-ac17-2592e450c444",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### ModelReportRunner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "20e93904",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[manifest] cat=8 num(total)=18 num(raw usate)=10\n",
            "[df] rows=15000 cols=44; ALL in df=18\n",
            "[sample cols] ['zone', 'urban_type', 'region', 'energy_class', 'condition', 'heating', 'view', 'public_transport_nearby', 'size_m2', 'rooms', 'bathrooms', 'year_built']\n",
            "Random split → R²(all)=0.8239  MAE=87.34  RMSE=125.74\n",
            "Random split → R²(num_raw)=0.3845  MAE=166.53  RMSE=235.07\n",
            "ΔR² (all - num_raw): +0.4394\n",
            "GSS 5× (group=location) → R²=0.8395±0.0747  MAE=68.03±15.94  RMSE=96.02±17.27\n",
            "\n",
            "Top 10 feature importance (Ordinal+CT):\n",
            "        feature  importance\n",
            "        size_m2    0.522160\n",
            "           zone    0.206055\n",
            "         region    0.091266\n",
            "     year_built    0.055060\n",
            "   energy_class    0.044937\n",
            "      condition    0.020643\n",
            "          floor    0.012104\n",
            "building_floors    0.011557\n",
            "           view    0.008557\n",
            "          rooms    0.005334\n"
          ]
        }
      ],
      "source": [
        "# === Model Report Runner (coerente con il training: OrdinalEncoder + TTR) ===\n",
        "from __future__ import annotations\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "\n",
        "# --- helper RMSE retro-compatibile\n",
        "def _rmse(y_true, y_pred):\n",
        "    try:\n",
        "        return mean_squared_error(y_true, y_pred, squared=False)\n",
        "    except TypeError:\n",
        "        return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "# ── 0) Carica manifest v2 (usa expected_features) ─────────────────────────────\n",
        "PROP_DIRS = [Path(\"notebooks/outputs/modeling/property\"), Path(\"outputs/modeling/property\")]\n",
        "PROP_DIR = next((d for d in PROP_DIRS if d.exists()), PROP_DIRS[0])\n",
        "MF_PATH = PROP_DIR / \"training_manifest.json\"\n",
        "\n",
        "mf = json.loads(MF_PATH.read_text(encoding=\"utf-8\"))\n",
        "ef = mf.get(\"expected_features\") or {}\n",
        "\n",
        "cat_cols = list(ef.get(\"categorical\") or [])\n",
        "num_cols = list(ef.get(\"numeric\") or [])\n",
        "\n",
        "# derivate presenti in expected_features ma NON nel df raw\n",
        "DERIVED = {\n",
        "    \"log_size_m2\",\"sqm_per_room\",\"baths_per_100sqm\",\n",
        "    \"elev_x_floor\",\"no_elev_high_floor\",\"rooms_per_100sqm\",\n",
        "    \"city_zone_prior\",\"region_index_prior\"\n",
        "}\n",
        "num_raw = [c for c in num_cols if c not in DERIVED]\n",
        "\n",
        "# ── 1) Scegli il DataFrame base (df -> df_train -> dataset su disco) ─────────\n",
        "if \"df\" in globals() and isinstance(df, pd.DataFrame):\n",
        "    base_df = df\n",
        "elif \"df_train\" in globals() and isinstance(df_train, pd.DataFrame):\n",
        "    base_df = df_train\n",
        "else:\n",
        "    CAND = [\n",
        "        Path(\"notebooks/outputs/dataset_generated.parquet\"),\n",
        "        Path(\"notebooks/outputs/dataset_generated.csv\"),\n",
        "        Path(\"outputs/dataset_generated.parquet\"),\n",
        "        Path(\"outputs/dataset_generated.csv\"),\n",
        "    ]\n",
        "    src = next((p for p in CAND if p.exists()), None)\n",
        "    if not src:\n",
        "        raise RuntimeError(\"Nessun dataframe disponibile: definisci df/df_train o salva dataset_generated.*\")\n",
        "    base_df = pd.read_parquet(src) if src.suffix.lower() in (\".parquet\", \".pq\") else pd.read_csv(src)\n",
        "\n",
        "# tieni SOLO le colonne che esistono davvero nel df\n",
        "cat_cols = [c for c in cat_cols if c in base_df.columns]\n",
        "num_raw  = [c for c in num_raw  if c in base_df.columns]\n",
        "ALL = cat_cols + num_raw\n",
        "\n",
        "print(f\"[manifest] cat={len(cat_cols)} num(total)={len(num_cols)} num(raw usate)={len(num_raw)}\")\n",
        "print(f\"[df] rows={len(base_df)} cols={len(base_df.columns)}; ALL in df={len(ALL)}\")\n",
        "print(f\"[sample cols] {ALL[:12]}\")\n",
        "\n",
        "assert \"valuation_k\" in base_df.columns, \"Manca il target valuation_k\"\n",
        "assert len(ALL) > 0, \"Nessuna feature trovata nel df per (cat+num_raw).\"\n",
        "\n",
        "df = base_df.copy()  # da qui in poi usiamo df\n",
        "\n",
        "# ── 2) Pipeline coerente con il training (Ordinal + imputazioni) + TTR ───────\n",
        "pre_all = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", Pipeline([\n",
        "            (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "            (\"enc\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
        "        ]), cat_cols) if cat_cols else (\"cat\", \"drop\", []),\n",
        "        (\"num\", Pipeline([\n",
        "            (\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "        ]), num_raw) if num_raw else (\"num\", \"drop\", []),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    verbose_feature_names_out=False,\n",
        ")\n",
        "\n",
        "rf_all = RandomForestRegressor(\n",
        "    n_estimators=300, random_state=42, n_jobs=-1, max_depth=None, min_samples_leaf=2\n",
        ")\n",
        "\n",
        "pipe_all = Pipeline([\n",
        "    (\"prep\", pre_all),\n",
        "    (\"ttr\", TransformedTargetRegressor(\n",
        "        regressor=rf_all,\n",
        "        func=np.log1p, inverse_func=np.expm1, check_inverse=False\n",
        "    )),\n",
        "])\n",
        "\n",
        "# ── 3) Valutazione split semplice (random) con feature RAW ───────────────────\n",
        "X_all = df[ALL].copy()\n",
        "y_nat = df[\"valuation_k\"].astype(float).to_numpy()\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X_all, y_nat, test_size=0.2, random_state=42)\n",
        "pipe_all.fit(X_tr, y_tr)\n",
        "y_hat = pipe_all.predict(X_te)\n",
        "\n",
        "r2_all  = r2_score(y_te, y_hat)\n",
        "mae_all = mean_absolute_error(y_te, y_hat)\n",
        "rmse_all = _rmse(y_te, y_hat)\n",
        "print(f\"Random split → R²(all)={r2_all:.4f}  MAE={mae_all:.2f}  RMSE={rmse_all:.2f}\")\n",
        "\n",
        "# ── 4) Solo numeriche (raw) + TTR (confronto) ────────────────────────────────\n",
        "if num_raw:\n",
        "    pre_num = ColumnTransformer(\n",
        "        [(\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))]), num_raw)],\n",
        "        remainder=\"drop\",\n",
        "        verbose_feature_names_out=False,\n",
        "    )\n",
        "    rf_num = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1, min_samples_leaf=2)\n",
        "    pipe_num = Pipeline([\n",
        "        (\"prep\", pre_num),\n",
        "        (\"ttr\", TransformedTargetRegressor(\n",
        "            regressor=rf_num, func=np.log1p, inverse_func=np.expm1, check_inverse=False\n",
        "        )),\n",
        "    ])\n",
        "    Xn_tr, Xn_te, yn_tr, yn_te = train_test_split(df[num_raw].copy(), y_nat, test_size=0.2, random_state=42)\n",
        "    pipe_num.fit(Xn_tr, yn_tr)\n",
        "    y_num = pipe_num.predict(Xn_te)\n",
        "\n",
        "    r2_num  = r2_score(yn_te, y_num)\n",
        "    mae_num = mean_absolute_error(yn_te, y_num)\n",
        "    rmse_num = _rmse(yn_te, y_num)\n",
        "    print(f\"Random split → R²(num_raw)={r2_num:.4f}  MAE={mae_num:.2f}  RMSE={rmse_num:.2f}\")\n",
        "    print(f\"ΔR² (all - num_raw): {r2_all - r2_num:+.4f}\")\n",
        "\n",
        "# ── 5) Stima più robusta: GroupShuffleSplit (group=location se presente) ─────\n",
        "if \"location\" in df.columns and len(ALL) > 0:\n",
        "    gss = GroupShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
        "    r2s, maes, rmses = [], [], []\n",
        "    groups = df[\"location\"].astype(str).to_numpy()\n",
        "    for tr_idx, te_idx in gss.split(df[ALL], y_nat, groups=groups):\n",
        "        pipe_all.fit(df.iloc[tr_idx][ALL], y_nat[tr_idx])\n",
        "        y_g = pipe_all.predict(df.iloc[te_idx][ALL])\n",
        "        r2s.append(r2_score(y_nat[te_idx], y_g))\n",
        "        maes.append(mean_absolute_error(y_nat[te_idx], y_g))\n",
        "        rmses.append(_rmse(y_nat[te_idx], y_g))\n",
        "    print(f\"GSS 5× (group=location) → R²={np.mean(r2s):.4f}±{np.std(r2s):.4f}  \"\n",
        "          f\"MAE={np.mean(maes):.2f}±{np.std(maes):.2f}  RMSE={np.mean(rmses):.2f}±{np.std(rmses):.2f}\")\n",
        "\n",
        "# ── 6) Feature importance (dal RF dentro TTR) + nomi coerenti CT ─────────────\n",
        "try:\n",
        "    try:\n",
        "        feat_names = list(pipe_all.named_steps[\"prep\"].get_feature_names_out())\n",
        "    except Exception:\n",
        "        feat_names = [*cat_cols, *num_raw]\n",
        "\n",
        "    rf_fitted = pipe_all.named_steps[\"ttr\"].regressor_\n",
        "    importances = getattr(rf_fitted, \"feature_importances_\", None)\n",
        "    if importances is None:\n",
        "        raise RuntimeError(\"feature_importances_ non disponibile sul regressore.\")\n",
        "\n",
        "    imp = np.asarray(importances, dtype=float)\n",
        "    if len(feat_names) != len(imp):\n",
        "        feat_names = [f\"f{i}\" for i in range(len(imp))]\n",
        "\n",
        "    fi = (pd.DataFrame({\"feature\": feat_names, \"importance\": imp})\n",
        "            .sort_values(\"importance\", ascending=False)\n",
        "            .reset_index(drop=True))\n",
        "\n",
        "    print(\"\\nTop 10 feature importance (Ordinal+CT):\")\n",
        "    print(fi.head(10).to_string(index=False))\n",
        "except Exception as e:\n",
        "    print(\"Feature importance non disponibile:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "814c338c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ training_manifest firmato: 18c60b1d012ff84bdc33f65b4fb25784837ddb359847681470560206a974e73f | created_utc: 2025-09-23T00:06:46Z\n"
          ]
        }
      ],
      "source": [
        "from shared.common.utils import canonical_json_dumps, sha256_hex\n",
        "from datetime import datetime, timezone\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "PROP_DIRS = [Path(\"notebooks/outputs/modeling/property\"), Path(\"outputs/modeling/property\")]\n",
        "PROP_DIR = next((d for d in PROP_DIRS if d.exists()), PROP_DIRS[0])\n",
        "manifest_path = PROP_DIR / \"training_manifest.json\"\n",
        "\n",
        "m = json.loads(manifest_path.read_text(encoding=\"utf-8\")) if manifest_path.exists() else {}\n",
        "manifest_canon = canonical_json_dumps(m)\n",
        "m[\"manifest_sha256\"] = sha256_hex(manifest_canon)\n",
        "if \"created_utc\" not in m:\n",
        "    m[\"created_utc\"] = datetime.now(timezone.utc).replace(microsecond=0).isoformat().replace(\"+00:00\",\"Z\")\n",
        "\n",
        "manifest_path.write_text(canonical_json_dumps(m), encoding=\"utf-8\")\n",
        "print(\"✅ training_manifest firmato:\", m[\"manifest_sha256\"], \"| created_utc:\", m[\"created_utc\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "af220429",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoder: OneHotEncoder\n",
            "handle_unknown: ignore\n",
            "Categorie — region (sample): ['center', 'north', 'south']\n",
            "Categorie — zone   (sample): ['center', 'periphery', 'semi_center']\n",
            "\n",
            "✅ Done.\n"
          ]
        }
      ],
      "source": [
        "# --- OHE/Ordinal: introspezione robusta (ri-estrazione post-fit + fallback) ---\n",
        "from __future__ import annotations\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd, joblib\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
        "\n",
        "# 1) carica pipeline salvata\n",
        "PIPE_PATHS = [\n",
        "    Path(\"notebooks/outputs/modeling/property/value_regressor_v2.joblib\"),\n",
        "    Path(\"outputs/modeling/property/value_regressor_v2.joblib\"),\n",
        "]\n",
        "PIPE_PATH = next((p for p in PIPE_PATHS if p.exists()), None)\n",
        "assert PIPE_PATH is not None, \"value_regressor_v2.joblib non trovato\"\n",
        "pipe = joblib.load(PIPE_PATH)\n",
        "\n",
        "# 2) trova il ColumnTransformer anche se annidato\n",
        "def find_ct(obj):\n",
        "    if isinstance(obj, ColumnTransformer): return obj\n",
        "    if isinstance(obj, Pipeline):\n",
        "        for _, st in obj.steps:\n",
        "            ct = find_ct(st)\n",
        "            if ct is not None: return ct\n",
        "    if obj.__class__.__name__ == \"TransformedTargetRegressor\":\n",
        "        reg = getattr(obj, \"regressor_\", None) or getattr(obj, \"regressor\", None)\n",
        "        if reg is not None: return find_ct(reg)\n",
        "    for attr in (\"prep\",\"preprocessor\",\"feature_processor\",\"processor\"):\n",
        "        if hasattr(obj, attr):\n",
        "            ct = find_ct(getattr(obj, attr))\n",
        "            if ct is not None: return ct\n",
        "    return None\n",
        "\n",
        "prep = find_ct(pipe)\n",
        "assert isinstance(prep, ColumnTransformer), \"Nessun ColumnTransformer trovato nella pipeline\"\n",
        "\n",
        "# 3) costruisci un mini DF per fittare SOLO il prep (se serve)\n",
        "def build_fit_sample(prep: ColumnTransformer, pipe_all: Pipeline) -> pd.DataFrame:\n",
        "    # sorgente dati: df_train / df / outputs/dataset_generated.*\n",
        "    if \"df_train\" in globals() and isinstance(df_train, pd.DataFrame):\n",
        "        base = df_train.copy()\n",
        "    elif \"df\" in globals() and isinstance(df, pd.DataFrame):\n",
        "        base = df.copy()\n",
        "    else:\n",
        "        ds_candidates = [Path(\"outputs/dataset_generated.parquet\"), Path(\"outputs/dataset_generated.csv\")]\n",
        "        ds_path = next((p for p in ds_candidates if p.exists()), None)\n",
        "        assert ds_path is not None, \"Dataset per il mini-fit non trovato\"\n",
        "        base = pd.read_parquet(ds_path) if ds_path.suffix.lower() in {\".parquet\",\".pq\"} else pd.read_csv(ds_path)\n",
        "\n",
        "    # colonne richieste dal CT\n",
        "    req = []\n",
        "    for _, _, cols in getattr(prep, \"transformers\", []):\n",
        "        if cols is None or cols == \"drop\": continue\n",
        "        req.extend(list(cols) if isinstance(cols,(list,tuple,np.ndarray)) else [cols])\n",
        "    req = list(dict.fromkeys(map(str, req)))\n",
        "\n",
        "    X = base.reindex(columns=req)\n",
        "    # applica derive se presente in pipe\n",
        "    if isinstance(pipe_all, Pipeline) and \"derive\" in pipe_all.named_steps:\n",
        "        d = pipe_all.named_steps[\"derive\"]\n",
        "        if hasattr(d, \"transform\"):\n",
        "            try: X = d.transform(X)\n",
        "            except Exception as e: print(\"[derive] skip nel mini-fit:\", e)\n",
        "\n",
        "    if len(X) > 2000:\n",
        "        X = X.sample(n=2000, random_state=42)\n",
        "    return X\n",
        "\n",
        "# 4) mini-fit del SOLO prep se non fittato\n",
        "if not hasattr(prep, \"transformers_\"):\n",
        "    X_small = build_fit_sample(prep, pipe)\n",
        "    prep.fit(X_small)\n",
        "else:\n",
        "    # serve anche X_small per eventuale fallback\n",
        "    X_small = build_fit_sample(prep, pipe)\n",
        "\n",
        "# 5) ri-estrai il ramo 'cat' e l'encoder **dopo** il fit\n",
        "def get_cat_branch_and_encoder(prep: ColumnTransformer):\n",
        "    cat_branch, cat_cols_in = None, None\n",
        "    for name, trans, cols in getattr(prep, \"transformers_\", []):\n",
        "        cols_list = list(cols) if isinstance(cols,(list,tuple,np.ndarray)) else ([cols] if cols is not None else [])\n",
        "        if name == \"cat\":\n",
        "            cat_branch, cat_cols_in = trans, cols_list\n",
        "            break\n",
        "    if cat_branch is None:  # fallback: primo ramo pipeline con encoder noto\n",
        "        for name, trans, cols in getattr(prep, \"transformers_\", []):\n",
        "            if isinstance(trans, (Pipeline, OneHotEncoder, OrdinalEncoder)):\n",
        "                cat_branch = trans\n",
        "                cat_cols_in = list(cols) if isinstance(cols,(list,tuple,np.ndarray)) else [cols]\n",
        "                break\n",
        "\n",
        "    enc = None\n",
        "    if isinstance(cat_branch, Pipeline):\n",
        "        for key in (\"encode\",\"enc\",\"ohe\",\"ordinal\",\"encoder\"):\n",
        "            if key in cat_branch.named_steps and isinstance(cat_branch.named_steps[key], (OneHotEncoder, OrdinalEncoder)):\n",
        "                enc = cat_branch.named_steps[key]\n",
        "                break\n",
        "        if enc is None:\n",
        "            for _, st in cat_branch.named_steps.items():\n",
        "                if isinstance(st, (OneHotEncoder, OrdinalEncoder)):\n",
        "                    enc = st; break\n",
        "    elif isinstance(cat_branch, (OneHotEncoder, OrdinalEncoder)):\n",
        "        enc = cat_branch\n",
        "    return cat_branch, enc, cat_cols_in\n",
        "\n",
        "cat_branch, enc, cat_cols_in = get_cat_branch_and_encoder(prep)\n",
        "assert enc is not None, \"Encoder categoriale non trovato nel ramo 'cat'\"\n",
        "\n",
        "# 6) categorie: usa categories_ se disponibile, altrimenti ricava dagli unique del mini-df\n",
        "def derive_categories_from_data(X: pd.DataFrame, cols: list[str]) -> dict[str, list]:\n",
        "    out = {}\n",
        "    for c in cols:\n",
        "        if c in X.columns:\n",
        "            vals = pd.Series(X[c]).astype(\"object\")\n",
        "            out[c] = sorted(pd.unique(vals[vals.notna()]).tolist())\n",
        "        else:\n",
        "            out[c] = []\n",
        "    return out\n",
        "\n",
        "if isinstance(enc, OneHotEncoder):\n",
        "    if hasattr(enc, \"categories_\"):\n",
        "        cats_map = { (cat_cols_in[i] if cat_cols_in and i < len(cat_cols_in) else f\"cat_{i}\"): list(c)\n",
        "                     for i, c in enumerate(enc.categories_) }\n",
        "    else:\n",
        "        # fallback: ricava categorie dal dato\n",
        "        cats_map = derive_categories_from_data(X_small, cat_cols_in or [])\n",
        "    print(\"Encoder: OneHotEncoder\")\n",
        "    print(\"handle_unknown:\", getattr(enc, \"handle_unknown\", None))\n",
        "    print(\"Categorie — region (sample):\", (cats_map.get(\"region\") or cats_map.get(\"Region\") or [])[:12])\n",
        "    print(\"Categorie — zone   (sample):\", (cats_map.get(\"zone\")   or cats_map.get(\"Zone\")   or [])[:12])\n",
        "\n",
        "elif isinstance(enc, OrdinalEncoder):\n",
        "    if hasattr(enc, \"categories_\"):\n",
        "        cats = enc.categories_\n",
        "        names = cat_cols_in or [f\"cat_{i}\" for i in range(len(cats))]\n",
        "        print(\"Encoder: OrdinalEncoder\")\n",
        "        print(\"handle_unknown:\", getattr(enc, \"handle_unknown\", None))\n",
        "        for i, cat in enumerate(cats[:min(5, len(cats))]):\n",
        "            cname = names[i] if i < len(names) else f\"cat_{i}\"\n",
        "            print(f\"  {cname}: {list(cat)[:12]}\")\n",
        "    else:\n",
        "        cats_map = derive_categories_from_data(X_small, cat_cols_in or [])\n",
        "        print(\"Encoder: OrdinalEncoder (fallback categorie dai dati)\")\n",
        "        for k, v in list(cats_map.items())[:5]:\n",
        "            print(f\"  {k}: {v[:12]}\")\n",
        "\n",
        "print(\"\\n✅ Done.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ai-oracle",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
