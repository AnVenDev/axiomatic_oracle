{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8cc119a1-28bd-47af-8a5d-38204c005bdf",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Imports & Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ba165936-c2d9-4dd5-b388-62912b63f069",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-10-07 17:02:46,437] WARNING model_trainer: dataset_config.yaml not found; proceeding with defaults.\n",
            "[2025-10-07 17:02:46,437] INFO model_trainer: Setup OK | seed=42 | outputs_dir=c:/Users/anven/OneDrive/Documenti/GitHub/axiomatic_oracle/notebooks/outputs\n"
          ]
        }
      ],
      "source": [
        "# 00) Imports & setup\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- Project roots ---\n",
        "NB_ROOT = Path.cwd()                 # .../notebooks\n",
        "PROJ_ROOT = NB_ROOT.parent           # project root\n",
        "\n",
        "if str(NB_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(NB_ROOT))\n",
        "if str(PROJ_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJ_ROOT))\n",
        "\n",
        "# --- Shared utilities ---\n",
        "from shared.common.utils import (\n",
        "    NumpyJSONEncoder,          # noqa: F401\n",
        "    optimize_dtypes,           # noqa: F401\n",
        "    log_basic_diagnostics,     # noqa: F401\n",
        "    set_global_seed,\n",
        ")\n",
        "from shared.common.config import load_config, configure_logger\n",
        "from shared.common.serving_transformers import GeoCanonizer, PriorsGuard, EnsureDerivedFeatures\n",
        "from shared.n03_train_model.preprocessing import drop_leaky_and_target, list_required_serving_derivatives\n",
        "from shared.common.sanity_checks import scale_gate_valuation_k, scale_gate_per_sqm\n",
        "from shared.common.constants import (\n",
        "    EXPECTED_VALUATION_TOTAL_KEUR_RANGE,\n",
        "    EXPECTED_PRICE_PER_SQM_EUR_RANGE,\n",
        ")\n",
        "\n",
        "# (Models will be chosen later; avoid heavy imports here)\n",
        "\n",
        "# --- Logger ---\n",
        "LOG_LEVEL = os.getenv(\"NB_LOG_LEVEL\", \"INFO\")\n",
        "logger = configure_logger(name=\"model_trainer\", level=LOG_LEVEL)\n",
        "\n",
        "# --- Load config (optional). If YAML is missing, proceed with safe defaults. ---\n",
        "CFG_PATH = NB_ROOT / \"dataset_config.yaml\"\n",
        "if CFG_PATH.exists():\n",
        "    CONFIG = load_config(str(CFG_PATH))\n",
        "    logger.info(\"Loaded config YAML: %s\", CFG_PATH.as_posix())\n",
        "else:\n",
        "    CONFIG = {}\n",
        "    logger.warning(\"dataset_config.yaml not found; proceeding with defaults.\")\n",
        "\n",
        "TRAIN_CFG = CONFIG.get(\"training\", {}) or {}\n",
        "\n",
        "# --- Global seed ---\n",
        "SEED = int(TRAIN_CFG.get(\"seed\", CONFIG.get(\"seed\", 42)))\n",
        "set_global_seed(SEED)\n",
        "\n",
        "# --- Output folders (relative to `notebooks/`) ---\n",
        "BASE_OUT = NB_ROOT / \"outputs\"\n",
        "MODEL_DIR = BASE_OUT / \"modeling\"\n",
        "FIG_DIR   = MODEL_DIR / \"figures\"\n",
        "ART_DIR   = MODEL_DIR / \"artifacts\"\n",
        "PROP_DIR  = MODEL_DIR / \"property\"  # used by downstream registry/backends\n",
        "\n",
        "for d in (BASE_OUT, MODEL_DIR, FIG_DIR, ART_DIR, PROP_DIR):\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- Dataset path (can be overridden via YAML) ---\n",
        "DATASET_PATH = Path(TRAIN_CFG.get(\"dataset_path\", BASE_OUT / \"dataset_generated.csv\"))\n",
        "\n",
        "# --- Display/QoL ---\n",
        "pd.set_option(\"display.max_columns\", 200)\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "logger.info(\"Setup OK | seed=%s | outputs_dir=%s\", SEED, BASE_OUT.as_posix())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c0f3f96-b7be-48ff-a9b2-2c9b9569cb63",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "38892f35-5a7d-488c-83d2-a99bca1e6fee",
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-10-07 17:02:46,457] INFO model_trainer: Found nb01 manifest: c:/Users/anven/OneDrive/Documenti/GitHub/axiomatic_oracle/notebooks/outputs/snapshots/manifest_20251005T102255Z.json\n",
            "[2025-10-07 17:02:46,458] INFO model_trainer: 📄 Loading dataset from: C:/Users/anven/OneDrive/Documenti/GitHub/axiomatic_oracle/notebooks/outputs/dataset_generated.csv\n",
            "[2025-10-07 17:02:46,576] INFO model_trainer: ✅ Dtypes optimized: 14.95 MB → 12.81 MB (−2.15 MB, 14.4%)\n",
            "[2025-10-07 17:02:46,576] INFO model_trainer: [UTILS] Distribution by location:\n",
            "location\n",
            "Milan       3017\n",
            "Rome        2700\n",
            "Turin       1214\n",
            "Naples      1190\n",
            "Bologna      886\n",
            "Genoa        773\n",
            "Florence     770\n",
            "Palermo      767\n",
            "Venice       596\n",
            "Bari         593\n",
            "Verona       591\n",
            "Padua        581\n",
            "Catania      444\n",
            "Cagliari     442\n",
            "Trieste      436\n",
            "[2025-10-07 17:02:46,576] INFO model_trainer: [UTILS] Valuation min: 53.70k€\n",
            "[2025-10-07 17:02:46,583] INFO model_trainer: [UTILS] Valuation max: 2403.49k€\n",
            "[2025-10-07 17:02:46,583] INFO model_trainer: [UTILS] Valuation mean: 474.27k€\n",
            "[2025-10-07 17:02:46,583] INFO model_trainer: [UTILS] Corr(size_m2, valuation_k): 0.636\n",
            "[2025-10-07 17:02:46,590] INFO model_trainer: ✅ Schema validation passed\n",
            "[2025-10-07 17:02:46,590] WARNING model_trainer: 🔴 Removing leaky features: ['price_per_sqm', 'strongly_incoherent']\n",
            "[2025-10-07 17:02:46,597] INFO model_trainer: ✅ Dataset cleaned: 43 columns remaining\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape after cleanup: (15000, 43)\n",
            "Numeric columns: ['valuation_k', 'listing_month', 'size_m2', 'rooms', 'bathrooms', 'year_built', 'age_years', 'floor', 'building_floors', 'is_top_floor', 'is_ground_floor', 'has_elevator', 'has_garden', 'has_balcony', 'garage', 'owner_occupied', 'public_transport_nearby', 'distance_to_center_km', 'parking_spot', 'cellar', 'attic', 'concierge', 'humidity_level', 'temperature_avg', 'noise_level', 'air_quality_index', 'condition_score', 'risk_score', 'luxury_score', 'env_score', 'confidence_score']\n"
          ]
        }
      ],
      "source": [
        "# 01) Load dataset from nb01 manifest (robust) + optimize + validate + immediate leakage cleanup\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from shared.common.utils import canonical_json_dumps, optimize_dtypes, log_basic_diagnostics\n",
        "from shared.common.sanity_checks import validate_dataset\n",
        "\n",
        "from shared.common.constants import VALUATION_K, PRICE_PER_SQM, PRICE_PER_SQM_CAPPED_VIOLATED\n",
        "from shared.n03_train_model.preprocessing import ML_LEAKY_FEATURES as _ML_LEAKY\n",
        "\n",
        "# --- helper: resolve relative paths against known bases (expects BASE_OUT/NB_ROOT/PROJ_ROOT already defined) ---\n",
        "def _resolve_path(p: str | Path) -> Path | None:\n",
        "    cand = Path(p)\n",
        "    if cand.exists():\n",
        "        return cand\n",
        "    # if relative, try under standard bases\n",
        "    for base in [BASE_OUT, NB_ROOT, PROJ_ROOT]:\n",
        "        try:\n",
        "            q = (base / str(p)).resolve()\n",
        "            if q.exists():\n",
        "                return q\n",
        "        except Exception:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "# 1) Find the latest nb01 manifest\n",
        "snap_dir = BASE_OUT / \"snapshots\"\n",
        "snap_dir.mkdir(parents=True, exist_ok=True)\n",
        "manifests = sorted(snap_dir.glob(\"manifest_*.json\"))\n",
        "manifest01 = None\n",
        "if manifests:\n",
        "    try:\n",
        "        manifest01 = json.loads(manifests[-1].read_text(encoding=\"utf-8\"))\n",
        "        logger.info(\"Found nb01 manifest: %s\", manifests[-1].as_posix())\n",
        "    except Exception as e:\n",
        "        logger.warning(\"Unable to read latest manifest: %s\", e)\n",
        "\n",
        "# 2) Resolve dataset path from manifest (supports multiple keys)\n",
        "data_path: Path | None = None\n",
        "if isinstance(manifest01, dict):\n",
        "    paths = (manifest01.get(\"paths\") or {})  # type: ignore\n",
        "    for k in (\"dataset\", \"dataset_path\", \"output_path\"):\n",
        "        p = paths.get(k)\n",
        "        if p:\n",
        "            rp = _resolve_path(p)\n",
        "            if rp:\n",
        "                data_path = rp\n",
        "                break\n",
        "\n",
        "# 3) Fallbacks: use DATASET_PATH (from Cell 01) or search in BASE_OUT\n",
        "if data_path is None or not data_path.exists():\n",
        "    candidates = [\n",
        "        Path(DATASET_PATH) if isinstance(DATASET_PATH, (str, Path)) else None,\n",
        "        BASE_OUT / \"dataset_generated.parquet\",\n",
        "        BASE_OUT / \"dataset_generated.csv\",\n",
        "    ]\n",
        "    candidates += sorted(BASE_OUT.glob(\"dataset_*.parquet\"))\n",
        "    candidates += sorted(BASE_OUT.glob(\"dataset_*.csv\"))\n",
        "    data_path = next((c for c in candidates if c and c.exists()), None)\n",
        "\n",
        "if not data_path or not data_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        \"Dataset not found. Check the nb01 manifest in notebooks/outputs/snapshots \"\n",
        "        \"or ensure notebooks/outputs/dataset_generated.(csv|parquet) exists.\"\n",
        "    )\n",
        "\n",
        "logger.info(\"📄 Loading dataset from: %s\", data_path.as_posix())\n",
        "\n",
        "# 4) Load parquet/csv (graceful handling if parquet engine is missing)\n",
        "if data_path.suffix.lower() in {\".parquet\", \".pq\"}:\n",
        "    try:\n",
        "        df = pd.read_parquet(data_path)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\n",
        "            f\"Failed to read Parquet at {data_path}. Ensure 'pyarrow' or 'fastparquet' is installed. Details: {e}\"\n",
        "        )\n",
        "else:\n",
        "    df = pd.read_csv(data_path)\n",
        "\n",
        "# 5) Dtype optimization (log memory saving)\n",
        "mem_before = df.memory_usage(deep=True).sum() / 1024**2\n",
        "df = optimize_dtypes(df)\n",
        "mem_after = df.memory_usage(deep=True).sum() / 1024**2\n",
        "logger.info(\n",
        "    \"✅ Dtypes optimized: %.2f MB → %.2f MB (−%.2f MB, %.1f%%)\",\n",
        "    mem_before, mem_after, mem_before - mem_after,\n",
        "    0.0 if mem_before == 0 else (mem_before - mem_after) / mem_before * 100.0\n",
        ")\n",
        "\n",
        "# 6) Quick diagnostics\n",
        "log_basic_diagnostics(df, logger)\n",
        "\n",
        "# 7) Schema validation (asset_type from nb01 config)\n",
        "asset_type = str(CONFIG.get(\"generation\", {}).get(\"asset_type\", \"property\"))\n",
        "try:\n",
        "    val_report = validate_dataset(df, asset_type=asset_type, raise_on_failure=True)\n",
        "    logger.info(\"✅ Schema validation passed\")\n",
        "except Exception as e:\n",
        "    logger.warning(\"Schema validation warning: %s\", e)\n",
        "    val_report = {\"overall_passed\": False, \"error\": str(e)}\n",
        "\n",
        "# 8) Persist validation report next to modeling outputs\n",
        "(MODEL_DIR / \"validation_nb03.json\").write_text(\n",
        "    canonical_json_dumps(val_report),\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "# 9) Immediate LEAKAGE CLEANUP (on raw df, before building X/y)\n",
        "# --- 1) Explicit removals (case-insensitive) ---\n",
        "explicit_leaky = {\n",
        "    PRICE_PER_SQM,\n",
        "    \"price_per_sqm\",\n",
        "    \"price_per_sqm_vs_region_avg\",\n",
        "    \"price_per_sqm_capped\",\n",
        "    \"valuation_k_log\",\n",
        "    PRICE_PER_SQM_CAPPED_VIOLATED,\n",
        "    \"strongly_incoherent\",\n",
        "    \"valuation_k_decile\",\n",
        "    \"valuation_rank\",\n",
        "    \"is_top_valuation\",\n",
        "}\n",
        "# union with module defaults\n",
        "explicit_leaky |= set(map(str, _ML_LEAKY))\n",
        "\n",
        "# map lowercase -> original (preserve original casing when dropping)\n",
        "lower_map = {c.lower(): c for c in df.columns}\n",
        "present_explicit = [lower_map[n.lower()] for n in explicit_leaky if n and n.lower() in lower_map]\n",
        "\n",
        "# --- 2) Pattern-based removals (regex, case-insensitive) ---\n",
        "regex_patterns = [\n",
        "    r\"price_per_sqm\",         # qualsiasi col contenente price_per_sqm\n",
        "    r\"_vs_region_avg$\",       # *_vs_region_avg\n",
        "    r\"^valuation_k_.+$\",      # derivati del target valuation_k\n",
        "    r\"^valuation(_|$)\",       # eventuali 'valuation' grezzi\n",
        "    r\"(decile|rank)\",         # indicatori di leakage \"analitico\"\n",
        "]\n",
        "\n",
        "present_regex = []\n",
        "for col in df.columns:\n",
        "    if col == VALUATION_K:  # NON rimuovere il target\n",
        "        continue\n",
        "    if any(re.search(pat, col, flags=re.IGNORECASE) for pat in regex_patterns):\n",
        "        present_regex.append(col)\n",
        "\n",
        "# --- 3) Apply removals ---\n",
        "to_drop = sorted(set(present_explicit) | set(present_regex))\n",
        "if to_drop:\n",
        "    logger.warning(\"🔴 Removing leaky features: %s\", to_drop)\n",
        "    df.drop(columns=to_drop, inplace=True, errors=\"ignore\")\n",
        "    logger.info(\"✅ Dataset cleaned: %d columns remaining\", df.shape[1])\n",
        "else:\n",
        "    logger.info(\"✅ No leaky features found in the dataset\")\n",
        "\n",
        "# --- 4) Final assertions ---\n",
        "assert not any(\"price_per_sqm\" in c.lower() for c in df.columns), \\\n",
        "    \"ERROR: columns matching 'price_per_sqm*' are still present!\"\n",
        "assert not any(\n",
        "    c.lower().startswith(\"valuation_k_\") for c in df.columns if c.lower() != VALUATION_K.lower()\n",
        "), \"ERROR: 'valuation_k_*' derivatives are still present!\"\n",
        "\n",
        "# Minimal debug\n",
        "logger.debug(\"Remaining columns: %s\", list(df.columns))\n",
        "print(f\"Shape after cleanup: {df.shape}\")\n",
        "print(f\"Numeric columns: {df.select_dtypes(include='number').columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c08c1d08-0b0e-4ed0-bbd9-94c27b0909e3",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Feature Engineering (derivations + priors + anomaly flags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "35685dd0",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-10-07 17:02:49,035] INFO model_trainer: train split: 10500 rows, 43 cols\n",
            "[2025-10-07 17:02:49,035] INFO model_trainer: valid split: 2250 rows, 43 cols\n",
            "[2025-10-07 17:02:49,035] INFO model_trainer: test split: 2250 rows, 43 cols\n",
            "[2025-10-07 17:02:57,768] INFO model_trainer: Anomaly features (train only): ['condition_minus_risk', 'size_m2', 'rooms_per_100sqm', 'no_elev_high_floor', 'floor_ratio', 'pt_importance', 'pt_x_periphery', 'city_zone_prior', 'region_index_prior', 'distance_to_center_km', 'air_quality_index', 'noise_level', 'humidity_level', 'temperature_avg']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "FEATURE ENGINEERING COMPLETED — quick peek\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>asset_id</th>\n",
              "      <th>asset_type</th>\n",
              "      <th>location</th>\n",
              "      <th>valuation_k</th>\n",
              "      <th>last_verified_ts</th>\n",
              "      <th>listing_month</th>\n",
              "      <th>region</th>\n",
              "      <th>urban_type</th>\n",
              "      <th>zone</th>\n",
              "      <th>size_m2</th>\n",
              "      <th>rooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>year_built</th>\n",
              "      <th>age_years</th>\n",
              "      <th>floor</th>\n",
              "      <th>building_floors</th>\n",
              "      <th>is_top_floor</th>\n",
              "      <th>is_ground_floor</th>\n",
              "      <th>has_elevator</th>\n",
              "      <th>has_garden</th>\n",
              "      <th>has_balcony</th>\n",
              "      <th>garage</th>\n",
              "      <th>owner_occupied</th>\n",
              "      <th>public_transport_nearby</th>\n",
              "      <th>distance_to_center_km</th>\n",
              "      <th>parking_spot</th>\n",
              "      <th>cellar</th>\n",
              "      <th>attic</th>\n",
              "      <th>concierge</th>\n",
              "      <th>energy_class</th>\n",
              "      <th>humidity_level</th>\n",
              "      <th>temperature_avg</th>\n",
              "      <th>noise_level</th>\n",
              "      <th>air_quality_index</th>\n",
              "      <th>condition_score</th>\n",
              "      <th>risk_score</th>\n",
              "      <th>luxury_score</th>\n",
              "      <th>env_score</th>\n",
              "      <th>orientation</th>\n",
              "      <th>view</th>\n",
              "      <th>condition</th>\n",
              "      <th>heating</th>\n",
              "      <th>confidence_score</th>\n",
              "      <th>condition_minus_risk</th>\n",
              "      <th>rooms_per_100sqm</th>\n",
              "      <th>no_elev_high_floor</th>\n",
              "      <th>floor_ratio</th>\n",
              "      <th>pt_importance</th>\n",
              "      <th>pt_x_periphery</th>\n",
              "      <th>pt_x_semi_center</th>\n",
              "      <th>pt_x_center</th>\n",
              "      <th>city_zone_prior</th>\n",
              "      <th>region_index_prior</th>\n",
              "      <th>city</th>\n",
              "      <th>anomaly_flag</th>\n",
              "      <th>anomaly_refined</th>\n",
              "      <th>severity_score</th>\n",
              "      <th>sample_weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>asset_000001</td>\n",
              "      <td>property</td>\n",
              "      <td>Turin</td>\n",
              "      <td>961.320007</td>\n",
              "      <td>2025-10-05 10:22:24+00:00</td>\n",
              "      <td>10</td>\n",
              "      <td>north</td>\n",
              "      <td>urban</td>\n",
              "      <td>center</td>\n",
              "      <td>170</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2014</td>\n",
              "      <td>11</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.40</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>E</td>\n",
              "      <td>49.000000</td>\n",
              "      <td>14.3</td>\n",
              "      <td>79</td>\n",
              "      <td>55</td>\n",
              "      <td>0.801</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.7</td>\n",
              "      <td>South-East</td>\n",
              "      <td>street</td>\n",
              "      <td>renovated</td>\n",
              "      <td>autonomous</td>\n",
              "      <td>0.7205</td>\n",
              "      <td>0.622</td>\n",
              "      <td>1.764706</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.05</td>\n",
              "      <td>turin</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>-0.179232</td>\n",
              "      <td>1.00106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>asset_000005</td>\n",
              "      <td>property</td>\n",
              "      <td>Cagliari</td>\n",
              "      <td>67.540001</td>\n",
              "      <td>2025-10-05 10:22:24+00:00</td>\n",
              "      <td>10</td>\n",
              "      <td>south</td>\n",
              "      <td>urban</td>\n",
              "      <td>semi_center</td>\n",
              "      <td>43</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1953</td>\n",
              "      <td>72</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2.29</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>G</td>\n",
              "      <td>62.000000</td>\n",
              "      <td>18.4</td>\n",
              "      <td>55</td>\n",
              "      <td>97</td>\n",
              "      <td>0.684</td>\n",
              "      <td>0.326</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>South-East</td>\n",
              "      <td>street</td>\n",
              "      <td>needs_renovation</td>\n",
              "      <td>autonomous</td>\n",
              "      <td>0.4620</td>\n",
              "      <td>0.358</td>\n",
              "      <td>4.651163</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.15</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.92</td>\n",
              "      <td>cagliari</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>-0.075212</td>\n",
              "      <td>1.00106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>asset_000006</td>\n",
              "      <td>property</td>\n",
              "      <td>Venice</td>\n",
              "      <td>232.050003</td>\n",
              "      <td>2025-10-05 10:22:24+00:00</td>\n",
              "      <td>10</td>\n",
              "      <td>north</td>\n",
              "      <td>urban</td>\n",
              "      <td>periphery</td>\n",
              "      <td>77</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1991</td>\n",
              "      <td>34</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6.55</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>D</td>\n",
              "      <td>34.299999</td>\n",
              "      <td>24.4</td>\n",
              "      <td>41</td>\n",
              "      <td>77</td>\n",
              "      <td>0.792</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.7</td>\n",
              "      <td>East</td>\n",
              "      <td>sea</td>\n",
              "      <td>needs_renovation</td>\n",
              "      <td>none</td>\n",
              "      <td>0.5960</td>\n",
              "      <td>0.552</td>\n",
              "      <td>2.597403</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.05</td>\n",
              "      <td>venice</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>-0.058198</td>\n",
              "      <td>1.00106</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       asset_id asset_type  location  valuation_k           last_verified_ts  \\\n",
              "1  asset_000001   property     Turin   961.320007  2025-10-05 10:22:24+00:00   \n",
              "5  asset_000005   property  Cagliari    67.540001  2025-10-05 10:22:24+00:00   \n",
              "6  asset_000006   property    Venice   232.050003  2025-10-05 10:22:24+00:00   \n",
              "\n",
              "   listing_month region urban_type         zone  size_m2  rooms  bathrooms  \\\n",
              "1             10  north      urban       center      170      3          2   \n",
              "5             10  south      urban  semi_center       43      2          1   \n",
              "6             10  north      urban    periphery       77      2          1   \n",
              "\n",
              "   year_built  age_years  floor  building_floors  is_top_floor  \\\n",
              "1        2014         11      4                7             0   \n",
              "5        1953         72      0                8             0   \n",
              "6        1991         34      0                9             0   \n",
              "\n",
              "   is_ground_floor  has_elevator  has_garden  has_balcony  garage  \\\n",
              "1                0             1           1            1       0   \n",
              "5                1             1           0            1       1   \n",
              "6                1             1           0            1       0   \n",
              "\n",
              "   owner_occupied  public_transport_nearby  distance_to_center_km  \\\n",
              "1               1                        0                   1.40   \n",
              "5               1                        1                   2.29   \n",
              "6               1                        1                   6.55   \n",
              "\n",
              "   parking_spot  cellar  attic  concierge energy_class  humidity_level  \\\n",
              "1             1       0      0          0            E       49.000000   \n",
              "5             0       0      0          0            G       62.000000   \n",
              "6             0       1      0          0            D       34.299999   \n",
              "\n",
              "   temperature_avg  noise_level  air_quality_index  condition_score  \\\n",
              "1             14.3           79                 55            0.801   \n",
              "5             18.4           55                 97            0.684   \n",
              "6             24.4           41                 77            0.792   \n",
              "\n",
              "   risk_score  luxury_score  env_score orientation    view         condition  \\\n",
              "1       0.179           0.6        0.7  South-East  street         renovated   \n",
              "5       0.326           0.4        0.0  South-East  street  needs_renovation   \n",
              "6       0.240           0.2        0.7        East     sea  needs_renovation   \n",
              "\n",
              "      heating  confidence_score  condition_minus_risk  rooms_per_100sqm  \\\n",
              "1  autonomous            0.7205                 0.622          1.764706   \n",
              "5  autonomous            0.4620                 0.358          4.651163   \n",
              "6        none            0.5960                 0.552          2.597403   \n",
              "\n",
              "   no_elev_high_floor  floor_ratio  pt_importance  pt_x_periphery  \\\n",
              "1                 0.0     0.666667            0.0               0   \n",
              "5                 0.0          0.0           1.15               0   \n",
              "6                 0.0          0.0            1.3               1   \n",
              "\n",
              "   pt_x_semi_center  pt_x_center  city_zone_prior  region_index_prior  \\\n",
              "1                 0            0              0.0                1.05   \n",
              "5                 1            0              0.0                0.92   \n",
              "6                 0            0              0.0                1.05   \n",
              "\n",
              "       city anomaly_flag  anomaly_refined  severity_score  sample_weight  \n",
              "1     turin        False            False       -0.179232        1.00106  \n",
              "5  cagliari        False            False       -0.075212        1.00106  \n",
              "6    venice        False            False       -0.058198        1.00106  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "train/valid/test shapes: (10500, 58) / (2250, 54) / (2250, 54)\n"
          ]
        }
      ],
      "source": [
        "# 04) Feature Engineering — Derivations + Priors + Anomaly flags (single cell)\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from shared.common.utils import canonical_json_dumps\n",
        "from shared.common.constants import (\n",
        "    ASSET_ID, VALUATION_K,\n",
        "    LAST_VERIFIED_TS, PREDICTION_TS, LAG_HOURS,\n",
        "    CONDITION_SCORE, RISK_SCORE,\n",
        "    SIZE_M2, ROOMS, FLOOR, BUILDING_FLOORS,\n",
        "    HAS_ELEVATOR, PUBLIC_TRANSPORT_NEARBY, ZONE, REGION,\n",
        ")\n",
        "from shared.common.serving_transformers import PriorsGuard\n",
        "\n",
        "REQUIRED_DERIVED = list_required_serving_derivatives()\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "# A) Train/Valid/Test split\n",
        "#    - group blocking on ASSET_ID (if present) + decile stratification on group medians\n",
        "#    - otherwise row-level decile stratification on target\n",
        "# --------------------------------------------------------------------------------------\n",
        "def _strat_bins(y: pd.Series, q: int = 10) -> pd.Series:\n",
        "    y_num = pd.to_numeric(y, errors=\"coerce\")\n",
        "    ranks = y_num.rank(method=\"first\")\n",
        "    unique = int(ranks.nunique())\n",
        "    if unique < 2:\n",
        "        return pd.Series(0, index=y.index, dtype=int)\n",
        "    q_eff = max(2, min(int(q), unique))\n",
        "    try:\n",
        "        bins = pd.qcut(ranks, q=q_eff, labels=False, duplicates=\"drop\")\n",
        "    except Exception:\n",
        "        bins = pd.Series(0, index=y.index, dtype=int)\n",
        "    if bins.isna().any():\n",
        "        mode_bin = int(bins.dropna().mode().iat[0]) if not bins.dropna().empty else 0\n",
        "        bins = bins.fillna(mode_bin).astype(int)\n",
        "    return bins.astype(int)\n",
        "\n",
        "def _safe_stratify(labels: pd.Series | np.ndarray, min_per_class: int = 2):\n",
        "    lab = pd.Series(labels)\n",
        "    vc = lab.value_counts()\n",
        "    if len(vc) < 2 or (vc < min_per_class).any():\n",
        "        return None\n",
        "    return lab.values\n",
        "\n",
        "def _ensure_split(df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    cfg = TRAIN_CFG if isinstance(TRAIN_CFG, dict) else {}\n",
        "    test_size = float(cfg.get(\"test_size\", 0.15))\n",
        "    val_size  = float(cfg.get(\"val_size\",  0.15))\n",
        "    n_deciles = int(cfg.get(\"n_deciles\",   10))\n",
        "    group_col = str(cfg.get(\"group_col\", ASSET_ID))\n",
        "\n",
        "    # keep rows with numeric target\n",
        "    mask_y = pd.to_numeric(df[VALUATION_K], errors=\"coerce\").notna()\n",
        "    if not mask_y.all():\n",
        "        logger.warning(\"Rows without target removed before splitting: %d\", (~mask_y).sum())\n",
        "    df_clean = df.loc[mask_y].copy()\n",
        "\n",
        "    if group_col in df_clean.columns and df_clean[group_col].notna().any():\n",
        "        df_clean[group_col] = df_clean[group_col].astype(str)\n",
        "        gstats = (\n",
        "            df_clean[[group_col, VALUATION_K]]\n",
        "            .groupby(group_col, as_index=False)[VALUATION_K]\n",
        "            .median()\n",
        "            .rename(columns={VALUATION_K: f\"{VALUATION_K}__group_median\"})\n",
        "        )\n",
        "\n",
        "        g_all = gstats[group_col].values\n",
        "        g_bins_all = _strat_bins(gstats[f\"{VALUATION_K}__group_median\"], q=n_deciles).values\n",
        "        strat_all = _safe_stratify(g_bins_all)\n",
        "\n",
        "        g_tmp, g_test = train_test_split(\n",
        "            g_all,\n",
        "            test_size=test_size,\n",
        "            random_state=SEED,\n",
        "            stratify=strat_all,\n",
        "        )\n",
        "\n",
        "        val_rel = float(val_size / max(1e-9, (1.0 - test_size)))\n",
        "        val_rel = min(max(val_rel, 0.05), 0.8)\n",
        "\n",
        "        tmp_mask = np.isin(gstats[group_col].values, g_tmp)\n",
        "        gstats_tmp = gstats.loc[tmp_mask].copy()\n",
        "        bins_tmp = _strat_bins(gstats_tmp[f\"{VALUATION_K}__group_median\"], q=n_deciles).values\n",
        "        bin_map_tmp = dict(zip(gstats_tmp[group_col].values, bins_tmp))\n",
        "        y_tmp_bins = np.array([bin_map_tmp.get(g, 0) for g in g_tmp])\n",
        "        strat_tmp = _safe_stratify(y_tmp_bins)\n",
        "\n",
        "        g_train, g_valid = train_test_split(\n",
        "            g_tmp,\n",
        "            test_size=val_rel,\n",
        "            random_state=SEED,\n",
        "            stratify=strat_tmp,\n",
        "        )\n",
        "\n",
        "        G_TRAIN, G_VALID, G_TEST = set(g_train), set(g_valid), set(g_test)\n",
        "        df_train = df_clean[df_clean[group_col].isin(G_TRAIN)].copy()\n",
        "        df_valid = df_clean[df_clean[group_col].isin(G_VALID)].copy()\n",
        "        df_test  = df_clean[df_clean[group_col].isin(G_TEST)].copy()\n",
        "    else:\n",
        "        logger.warning(\n",
        "            \"%s missing/invalid for grouping: falling back to row-level stratification.\", group_col\n",
        "        )\n",
        "        bins_all = _strat_bins(df_clean[VALUATION_K], q=n_deciles)\n",
        "        df_tmp, df_test = train_test_split(\n",
        "            df_clean,\n",
        "            test_size=test_size,\n",
        "            random_state=SEED,\n",
        "            stratify=_safe_stratify(bins_all),\n",
        "        )\n",
        "\n",
        "        val_rel = float(val_size / max(1e-9, (1.0 - test_size)))\n",
        "        val_rel = min(max(val_rel, 0.05), 0.8)\n",
        "        bins_tmp = _strat_bins(df_tmp[VALUATION_K], q=n_deciles)\n",
        "        df_train, df_valid = train_test_split(\n",
        "            df_tmp,\n",
        "            test_size=val_rel,\n",
        "            random_state=SEED,\n",
        "            stratify=_safe_stratify(bins_tmp),\n",
        "        )\n",
        "\n",
        "    # disjointness checks\n",
        "    for a_name, A in ((\"train\", df_train), (\"valid\", df_valid), (\"test\", df_test)):\n",
        "        logger.info(\"%s split: %d rows, %d cols\", a_name, len(A), A.shape[1])\n",
        "    assert len(set(df_train.index) & set(df_valid.index)) == 0\n",
        "    assert len(set(df_train.index) & set(df_test.index)) == 0\n",
        "    assert len(set(df_valid.index) & set(df_test.index)) == 0\n",
        "\n",
        "    return df_train, df_valid, df_test\n",
        "\n",
        "if not all(n in globals() for n in (\"df_train\", \"df_valid\", \"df_test\")):\n",
        "    df_train, df_valid, df_test = _ensure_split(df)\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "# B) Derivations (applied to all splits) – no target leakage\n",
        "# --------------------------------------------------------------------------------------\n",
        "def _ensure_datetime_and_lag(df_: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = df_.copy()\n",
        "    if (LAG_HOURS not in out.columns) and ({LAST_VERIFIED_TS, PREDICTION_TS} <= set(out.columns)):\n",
        "        out[LAST_VERIFIED_TS] = pd.to_datetime(out[LAST_VERIFIED_TS], utc=True, errors=\"coerce\")\n",
        "        out[PREDICTION_TS]   = pd.to_datetime(out[PREDICTION_TS],   utc=True, errors=\"coerce\")\n",
        "        lag = (out[PREDICTION_TS] - out[LAST_VERIFIED_TS]).dt.total_seconds().div(3600)\n",
        "        out[LAG_HOURS] = lag.where(lag >= 0, other=pd.NA).astype(\"Float32\")\n",
        "    elif LAG_HOURS in out.columns:\n",
        "        out[LAG_HOURS] = pd.to_numeric(out[LAG_HOURS], errors=\"coerce\").astype(\"Float32\")\n",
        "    return out\n",
        "\n",
        "def _derive_core_features(df_: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = df_.copy()\n",
        "\n",
        "    # condition_minus_risk\n",
        "    if (CONDITION_SCORE in out.columns) and (RISK_SCORE in out.columns):\n",
        "        cs = pd.to_numeric(out[CONDITION_SCORE], errors=\"coerce\")\n",
        "        rs = pd.to_numeric(out[RISK_SCORE], errors=\"coerce\")\n",
        "        out[\"condition_minus_risk\"] = (cs - rs).astype(\"Float32\")\n",
        "\n",
        "    # listing_month from prediction ts\n",
        "    if (\"listing_month\" not in out.columns) and (PREDICTION_TS in out.columns):\n",
        "        ts = pd.to_datetime(out[PREDICTION_TS], utc=True, errors=\"coerce\")\n",
        "        out[\"listing_month\"] = ts.dt.month.astype(\"Int16\")\n",
        "\n",
        "    # rooms per 100 sqm\n",
        "    s = pd.to_numeric(out.get(SIZE_M2), errors=\"coerce\").replace(0, np.nan)\n",
        "    r = pd.to_numeric(out.get(ROOMS), errors=\"coerce\")\n",
        "    out[\"rooms_per_100sqm\"] = (100.0 * r / s).astype(\"Float32\")\n",
        "\n",
        "    # penalty for high floor without elevator\n",
        "    f = pd.to_numeric(out.get(FLOOR), errors=\"coerce\")\n",
        "    e = pd.to_numeric(out.get(HAS_ELEVATOR), errors=\"coerce\").fillna(0)\n",
        "    out[\"no_elev_high_floor\"] = ((1 - e) * np.maximum(f - 1, 0)).astype(\"Float32\")\n",
        "\n",
        "    # floor ratio in [0,1]\n",
        "    bf = pd.to_numeric(out.get(BUILDING_FLOORS), errors=\"coerce\")\n",
        "    denom = (bf - 1.0).where((bf - 1.0) > 0, other=np.nan)\n",
        "    out[\"floor_ratio\"] = (f / denom).clip(lower=0.0, upper=1.0).astype(\"Float32\")\n",
        "\n",
        "    # public-transport importance (stronger in periphery)\n",
        "    if (ZONE in out.columns) and (PUBLIC_TRANSPORT_NEARBY in out.columns):\n",
        "        z_norm = out[ZONE].astype(\"string\").str.strip().str.lower()\n",
        "        mult = (\n",
        "            z_norm.map({\"center\": 1.00, \"semi_center\": 1.15, \"periphery\": 1.30})\n",
        "            .fillna(1.10)\n",
        "            .astype(\"Float32\")\n",
        "        )\n",
        "        pt = pd.to_numeric(out[PUBLIC_TRANSPORT_NEARBY], errors=\"coerce\").fillna(0).astype(\"Float32\")\n",
        "        out[\"pt_importance\"] = (pt * mult).astype(\"Float32\")\n",
        "        out[\"pt_x_periphery\"]   = (pt * (z_norm == \"periphery\").astype(\"Int8\")).astype(\"Int8\")\n",
        "        out[\"pt_x_semi_center\"] = (pt * (z_norm == \"semi_center\").astype(\"Int8\")).astype(\"Int8\")\n",
        "        out[\"pt_x_center\"]      = (pt * (z_norm == \"center\").astype(\"Int8\")).astype(\"Int8\")\n",
        "\n",
        "    return out\n",
        "\n",
        "def _apply_derivations(df_: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = _ensure_datetime_and_lag(df_)\n",
        "    out = _derive_core_features(out)\n",
        "    if VALUATION_K not in out.columns:\n",
        "        raise ValueError(f\"{VALUATION_K} missing: cannot train.\")\n",
        "    out[VALUATION_K] = pd.to_numeric(out[VALUATION_K], errors=\"coerce\").astype(\"Float32\")\n",
        "    return out\n",
        "\n",
        "for _n in (\"df_train\", \"df_valid\", \"df_test\"):\n",
        "    globals()[_n] = _apply_derivations(globals()[_n])\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "# C) Priors (city_zone_prior, region_index_prior) — consistent with generation logic\n",
        "#     + robust pre-creation of prior columns (Series) and manual fallback\n",
        "# --------------------------------------------------------------------------------------\n",
        "GEN_CFG = CONFIG.get(\"generation\", {}) if isinstance(CONFIG, dict) else {}\n",
        "_city_base = GEN_CFG.get(\"city_base_prices\", {}) or {}\n",
        "region_index_defaults = {\"north\": 1.05, \"center\": 1.00, \"south\": 0.92}\n",
        "region_index_map = GEN_CFG.get(\"region_index\", {}) or region_index_defaults\n",
        "\n",
        "if _city_base:\n",
        "    _city_base_norm = {\n",
        "        str(c).strip().lower(): {str(z).strip().lower(): float(v) for z, v in d.items()}\n",
        "        for c, d in _city_base.items()\n",
        "    }\n",
        "    all_zones = {z for d in _city_base_norm.values() for z in d}\n",
        "    zone_medians = {\n",
        "        z: float(np.nanmedian([d.get(z, np.nan) for d in _city_base_norm.values()]))\n",
        "        for z in all_zones\n",
        "    }\n",
        "    global_median = float(np.nanmedian([v for d in _city_base_norm.values() for v in d.values()]))\n",
        "else:\n",
        "    _city_base_norm, zone_medians, global_median = {}, {}, 0.0\n",
        "\n",
        "def _ensure_prior_cols(df_: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Make sure prior columns exist as Series (not scalars) and geo keys are normalized.\"\"\"\n",
        "    out = df_.copy()\n",
        "    # create as nullable float series if missing\n",
        "    for col in (\"city_zone_prior\", \"region_index_prior\"):\n",
        "        if col not in out.columns:\n",
        "            out[col] = pd.Series(pd.NA, index=out.index, dtype=\"Float32\")\n",
        "        else:\n",
        "            out[col] = pd.to_numeric(out[col], errors=\"coerce\").astype(\"Float32\")\n",
        "    # ensure city from location if missing\n",
        "    if \"city\" not in out.columns and \"location\" in out.columns:\n",
        "        out[\"city\"] = out[\"location\"]\n",
        "    # normalize geo keys\n",
        "    for c in (\"city\", \"zone\", \"region\"):\n",
        "        if c in out.columns:\n",
        "            out[c] = out[c].astype(\"string\").str.strip().str.lower()\n",
        "    return out\n",
        "\n",
        "def _fallback_priors(df_: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Manual priors computation if PriorsGuard fails.\"\"\"\n",
        "    out = _ensure_prior_cols(df_)\n",
        "    ci = out.get(\"city\", pd.Series(index=out.index, dtype=\"string\")).astype(\"string\").str.lower()\n",
        "    zo = out.get(\"zone\", pd.Series(index=out.index, dtype=\"string\")).astype(\"string\").str.lower()\n",
        "    re = out.get(\"region\", pd.Series(index=out.index, dtype=\"string\")).astype(\"string\").str.lower()\n",
        "\n",
        "    vals = []\n",
        "    for c, z in zip(ci, zo):\n",
        "        v = _city_base_norm.get(str(c), {}).get(str(z), np.nan)\n",
        "        if pd.isna(v):\n",
        "            v = zone_medians.get(str(z), global_median)\n",
        "        vals.append(v)\n",
        "    out[\"city_zone_prior\"] = pd.to_numeric(vals, errors=\"coerce\").astype(\"Float32\")\n",
        "    out[\"region_index_prior\"] = re.map(region_index_map).astype(\"Float32\")\n",
        "    return out\n",
        "\n",
        "# pre-create prior columns & normalize geo keys\n",
        "for _n in (\"df_train\", \"df_valid\", \"df_test\"):\n",
        "    globals()[_n] = _ensure_prior_cols(globals()[_n])\n",
        "\n",
        "priors = PriorsGuard(\n",
        "    city_base=_city_base_norm,\n",
        "    region_index=region_index_map,\n",
        "    zone_medians=zone_medians,\n",
        "    global_cityzone_median=global_median,\n",
        ")\n",
        "\n",
        "# apply PriorsGuard with fallback\n",
        "for _n in (\"df_train\", \"df_valid\", \"df_test\"):\n",
        "    try:\n",
        "        globals()[_n] = priors.transform(globals()[_n])\n",
        "    except Exception as e:\n",
        "        logger.warning(\"PriorsGuard.transform failed on %s (%s). Falling back to manual priors.\", _n, e)\n",
        "        globals()[_n] = _fallback_priors(globals()[_n])\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "# D) Anti-leakage guards (post-derivations/priors)\n",
        "# --------------------------------------------------------------------------------------\n",
        "for split_name, split_df in ((\"train\", df_train), (\"valid\", df_valid), (\"test\", df_test)):\n",
        "    assert not any(\"price_per_sqm\" in c.lower() for c in split_df.columns), \\\n",
        "        f\"LEAKAGE in df_{split_name}: columns matching 'price_per_sqm*' are present!\"\n",
        "    assert not any(\n",
        "        c.lower().startswith(\"valuation_k_\") for c in split_df.columns if c.lower() != VALUATION_K.lower()\n",
        "    ), f\"LEAKAGE in df_{split_name}: 'valuation_k_*' derivatives are present!\"\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "# E) Train-only anomaly flags + sample_weight (no leakage)\n",
        "# --------------------------------------------------------------------------------------\n",
        "try:\n",
        "    from shared.n02_explore_dataset.eda_core import AnomalyDetector  # type: ignore\n",
        "except Exception:\n",
        "    AnomalyDetector = None  # type: ignore\n",
        "\n",
        "num_cols = [c for c in df_train.columns if pd.api.types.is_numeric_dtype(df_train[c])]\n",
        "exclude = {VALUATION_K, \"sample_weight\"}\n",
        "\n",
        "preferred = [\n",
        "    \"condition_minus_risk\", SIZE_M2, \"rooms_per_100sqm\",\n",
        "    \"no_elev_high_floor\", \"floor_ratio\",\n",
        "    \"pt_importance\", \"pt_x_periphery\",\n",
        "    \"city_zone_prior\", \"region_index_prior\",\n",
        "    LAG_HOURS, \"distance_to_center_km\",\n",
        "    \"air_quality_index\", \"noise_level\", \"humidity_level\", \"temperature_avg\",\n",
        "]\n",
        "feat_candidates = [c for c in preferred if c in num_cols and c not in exclude]\n",
        "\n",
        "if len(feat_candidates) < 3:\n",
        "    pool = []\n",
        "    for c in num_cols:\n",
        "        if c in exclude:\n",
        "            continue\n",
        "        s = pd.to_numeric(df_train[c], errors=\"coerce\")\n",
        "        if s.nunique(dropna=True) >= 10 and np.nanvar(s.values) > 0:\n",
        "            pool.append((c, float(np.nanvar(s.values))))\n",
        "    pool.sort(key=lambda x: x[1], reverse=True)\n",
        "    feat_candidates = [c for c, _ in pool[:8]]\n",
        "\n",
        "contamination       = float(TRAIN_CFG.get(\"anomaly_contamination\", 0.03)) if isinstance(TRAIN_CFG, dict) else 0.03\n",
        "strong_z_threshold  = float(TRAIN_CFG.get(\"anomaly_strong_z\", 2.5)) if isinstance(TRAIN_CFG, dict) else 2.5\n",
        "severity_percentile = float(TRAIN_CFG.get(\"anomaly_severity_pct\", 90.0)) if isinstance(TRAIN_CFG, dict) else 90.0\n",
        "n_estimators        = int(TRAIN_CFG.get(\"anomaly_n_estimators\", 200)) if isinstance(TRAIN_CFG, dict) else 200\n",
        "\n",
        "if feat_candidates:\n",
        "    logger.info(\"Anomaly features (train only): %s\", feat_candidates)\n",
        "\n",
        "    if AnomalyDetector is not None:\n",
        "        anom = AnomalyDetector(\n",
        "            contamination=contamination,\n",
        "            strong_z_threshold=strong_z_threshold,\n",
        "            severity_percentile=severity_percentile,\n",
        "            n_estimators=n_estimators,\n",
        "            random_state=SEED,\n",
        "        )\n",
        "        df_train_ext, anom_rep = anom.detect_anomalies(\n",
        "            df_train,\n",
        "            feature_candidates=feat_candidates,\n",
        "            exclude_features=set(),\n",
        "        )\n",
        "        for col in (\"anomaly_flag\", \"anomaly_refined\", \"severity_score\"):\n",
        "            if col in df_train_ext.columns:\n",
        "                df_train[col] = df_train_ext[col]\n",
        "    else:\n",
        "        X = df_train[feat_candidates].apply(pd.to_numeric, errors=\"coerce\")\n",
        "        mu = X.mean(axis=0)\n",
        "        sd = X.std(axis=0).replace(0, np.nan)\n",
        "        z  = (X - mu) / sd\n",
        "        z_abs = z.abs()\n",
        "        z_mean = z_abs.mean(axis=1)\n",
        "        thr = np.nanpercentile(z_mean.dropna().values, severity_percentile)\n",
        "        flags_raw = (z_abs > strong_z_threshold).any(axis=1)\n",
        "        flags_ref = (z_mean >= thr)\n",
        "\n",
        "        df_train[\"anomaly_flag\"]    = flags_raw.astype(\"Int8\")\n",
        "        df_train[\"anomaly_refined\"] = flags_ref.astype(\"Int8\")\n",
        "        df_train[\"severity_score\"]  = z_mean.fillna(0).astype(\"Float32\")\n",
        "\n",
        "        anom_rep = {\n",
        "            \"method\": \"fallback_zscore\",\n",
        "            \"features\": feat_candidates,\n",
        "            \"strong_z_threshold\": strong_z_threshold,\n",
        "            \"severity_percentile\": severity_percentile,\n",
        "            \"n_anomalies_raw\": int(flags_raw.sum()),\n",
        "            \"n_anomalies_refined\": int(flags_ref.sum()),\n",
        "        }\n",
        "\n",
        "    # sample_weight (mean ≈ 1.0)\n",
        "    if \"severity_score\" in df_train.columns and df_train[\"severity_score\"].notna().any():\n",
        "        sev = pd.to_numeric(df_train[\"severity_score\"], errors=\"coerce\").clip(lower=0).astype(\"Float32\")\n",
        "        w   = 1.0 / (1.0 + sev)\n",
        "        w   = w.clip(lower=0.2, upper=1.0)\n",
        "        w   = (w * (1.0 / max(w.mean(), 1e-6))).astype(\"Float32\")\n",
        "        df_train[\"sample_weight\"] = w\n",
        "    elif \"confidence_score\" in df_train.columns and df_train[\"confidence_score\"].notna().any():\n",
        "        w = pd.to_numeric(df_train[\"confidence_score\"], errors=\"coerce\").clip(0.2, 1.0).astype(\"Float32\")\n",
        "        w = (w * (1.0 / max(w.mean(), 1e-6))).astype(\"Float32\")\n",
        "        df_train[\"sample_weight\"] = w\n",
        "    else:\n",
        "        df_train[\"sample_weight\"] = np.float32(1.0)\n",
        "\n",
        "    # persist anomaly report\n",
        "    try:\n",
        "        (ART_DIR / \"anomaly_train_report.json\").write_text(\n",
        "            canonical_json_dumps(anom_rep),\n",
        "            encoding=\"utf-8\"\n",
        "        )\n",
        "    except Exception:\n",
        "        pass\n",
        "else:\n",
        "    logger.info(\"Anomaly detection skipped: no valid candidate features.\")\n",
        "    df_train[\"sample_weight\"] = np.float32(1.0)\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "# F) Snapshot\n",
        "# --------------------------------------------------------------------------------------\n",
        "print(\"=\" * 60)\n",
        "print(\"FEATURE ENGINEERING COMPLETED — quick peek\")\n",
        "print(\"=\" * 60)\n",
        "display(df_train.head(3))\n",
        "print(f\"\\ntrain/valid/test shapes: {df_train.shape} / {df_valid.shape} / {df_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f993f856",
      "metadata": {},
      "source": [
        "### Feature Preparation & Single Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e17f0feb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature spec saved → c:\\Users\\anven\\OneDrive\\Documenti\\GitHub\\axiomatic_oracle\\notebooks\\outputs\\modeling\\artifacts\\feature_spec.json\n",
            "Using 23 categorical + 41 numeric features (53 total).\n",
            "Pipeline preview saved → c:\\Users\\anven\\OneDrive\\Documenti\\GitHub\\axiomatic_oracle\\notebooks\\outputs\\modeling\\artifacts\\pipeline_preview.json\n",
            "Prepared X/y:\n",
            "  X_train=(10500, 53), y_train=(10500,), sample_weight=yes\n",
            "  X_valid=(2250, 53), y_valid=(2250,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\anven\\AppData\\Local\\Temp\\ipykernel_19228\\1461876441.py:104: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
            "  if (pd.api.types.is_object_dtype(df_train[c]) or pd.api.types.is_categorical_dtype(df_train[c]))\n"
          ]
        }
      ],
      "source": [
        "# 05) Feature Preparation & Single Pipeline (replaces legacy A/B)\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "\n",
        "from shared.common.utils import canonical_json_dumps\n",
        "from shared.common.constants import (\n",
        "    VALUATION_K,\n",
        "    LAST_VERIFIED_TS, PREDICTION_TS,\n",
        ")\n",
        "from shared.n03_train_model.preprocessing import ML_LEAKY_FEATURES as _ML_LEAKY  # static list\n",
        "\n",
        "# -----------------------------\n",
        "# A) Choose feature columns\n",
        "# -----------------------------\n",
        "\n",
        "# 0) Anti-leakage guard (regex, case-insensitive)\n",
        "_LEAKY_PATTERNS = tuple(\n",
        "    re.compile(p, re.IGNORECASE)\n",
        "    for p in (\n",
        "        r\"^(y|label|target)$\",    # target alias\n",
        "        r\"^valuation(_|$)\",       # 'valuation', 'valuation_*'\n",
        "        r\"^valuation_k(_|$)\",     # 'valuation_k*'\n",
        "        r\"^price_per_sqm\",        # any price_per_sqm*\n",
        "        r\"_vs_region_avg$\",       # *_vs_region_avg\n",
        "        r\"(decile|rank)\",         # derived ranking/bins\n",
        "    )\n",
        ")\n",
        "def _is_name_leaky(name: str) -> bool:\n",
        "    if name in _ML_LEAKY:\n",
        "        return True\n",
        "    return any(rx.search(name) for rx in _LEAKY_PATTERNS)\n",
        "\n",
        "# 1) Colonne da escludere sempre\n",
        "EXCLUDE_ALWAYS = {\n",
        "    VALUATION_K,                  # target\n",
        "    \"sample_weight\",\n",
        "    LAST_VERIFIED_TS, PREDICTION_TS,  # timestamps grezzi\n",
        "    # artefatti & colonne analitiche\n",
        "    \"anomaly_flag\", \"anomaly_refined\",\n",
        "}\n",
        "\n",
        "# 2) Prefer explicit engineered/robust features quando presenti\n",
        "PREFERRED_NUMERIC = [\n",
        "    # engineered in previous cell\n",
        "    \"condition_minus_risk\",\n",
        "    \"rooms_per_100sqm\",\n",
        "    \"no_elev_high_floor\",\n",
        "    \"floor_ratio\",\n",
        "    \"pt_importance\", \"pt_x_periphery\", \"pt_x_semi_center\", \"pt_x_center\",\n",
        "    \"city_zone_prior\", \"region_index_prior\",\n",
        "    # common numeric predictors\n",
        "    \"size_m2\", \"rooms\", \"bathrooms\",\n",
        "    \"floor\", \"building_floors\",\n",
        "    \"distance_to_center_km\",\n",
        "    \"humidity_level\", \"temperature_avg\", \"noise_level\", \"air_quality_index\",\n",
        "    \"lag_hours\",\n",
        "]\n",
        "PREFERRED_CATEGORICAL = [\n",
        "    \"location\", \"region\", \"urban_type\", \"zone\",\n",
        "    \"energy_class\", \"orientation\", \"view\", \"condition\", \"heating\",\n",
        "    # binari che a volte arrivano come object/category\n",
        "    \"has_elevator\", \"has_garden\", \"has_balcony\", \"garage\",\n",
        "    \"owner_occupied\", \"public_transport_nearby\", \"parking_spot\",\n",
        "    \"cellar\", \"attic\", \"concierge\",\n",
        "    # time-derived discreto\n",
        "    \"listing_month\",\n",
        "]\n",
        "\n",
        "def _pick_existing(cols: List[str], df_cols: pd.Index) -> List[str]:\n",
        "    seen, out = set(), []\n",
        "    for c in cols:\n",
        "        if c in df_cols and c not in seen and (c not in EXCLUDE_ALWAYS) and (not _is_name_leaky(c)):\n",
        "            seen.add(c); out.append(c)\n",
        "    return out\n",
        "\n",
        "# Base pick (rispettando anti-leakage)\n",
        "cat_cols = _pick_existing(PREFERRED_CATEGORICAL, df_train.columns)\n",
        "num_cols = _pick_existing(PREFERRED_NUMERIC, df_train.columns)\n",
        "\n",
        "# 3) Auto-discover ulteriori candidati sicuri (no duplicati, no leakage)\n",
        "_auto_num = [\n",
        "    c for c in df_train.columns\n",
        "    if pd.api.types.is_numeric_dtype(df_train[c])\n",
        "    and c not in EXCLUDE_ALWAYS\n",
        "    and not _is_name_leaky(c)\n",
        "    and c not in num_cols\n",
        "]\n",
        "_auto_cat = [\n",
        "    c for c in df_train.columns\n",
        "    if (pd.api.types.is_object_dtype(df_train[c]) or pd.api.types.is_categorical_dtype(df_train[c]))\n",
        "    and c not in EXCLUDE_ALWAYS\n",
        "    and not _is_name_leaky(c)\n",
        "    and c not in cat_cols\n",
        "]\n",
        "\n",
        "num_cols += [c for c in _auto_num if c not in num_cols]\n",
        "cat_cols += [c for c in _auto_cat if c not in cat_cols]\n",
        "\n",
        "# 4) Lista finale (l’ordine conta per importances downstream)\n",
        "FEATURES = cat_cols + [c for c in num_cols if c not in set(cat_cols)]\n",
        "if not FEATURES:\n",
        "    raise RuntimeError(\"No features selected for training (after leakage guards).\")\n",
        "\n",
        "# Persist a small spec per riproducibilità\n",
        "feature_spec = {\n",
        "    \"categorical\": cat_cols,\n",
        "    \"numeric\": num_cols,\n",
        "    \"all_features\": FEATURES,\n",
        "}\n",
        "(ART_DIR / \"feature_spec.json\").write_text(canonical_json_dumps(feature_spec), encoding=\"utf-8\")\n",
        "print(f\"Feature spec saved → {ART_DIR / 'feature_spec.json'}\")\n",
        "print(f\"Using {len(cat_cols)} categorical + {len(num_cols)} numeric features ({len(FEATURES)} total).\")\n",
        "\n",
        "# --------------------------------\n",
        "# B) Build preprocessing pipeline\n",
        "# --------------------------------\n",
        "# Categorical: impute missing con moda + Ordinal encode (gestione unknown)\n",
        "cat_pipe = Pipeline(steps=[\n",
        "    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"encode\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
        "])\n",
        "\n",
        "# Numeric: impute con mediana + standardize\n",
        "num_pipe = Pipeline(steps=[\n",
        "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scale\", StandardScaler(with_mean=True, with_std=True)),\n",
        "])\n",
        "\n",
        "prep = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", cat_pipe, cat_cols),\n",
        "        (\"num\", num_pipe, num_cols),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    sparse_threshold=0.0,\n",
        "    n_jobs=None,\n",
        "    verbose_feature_names_out=False,\n",
        ")\n",
        "\n",
        "# --------------------------------\n",
        "# C) Estimator (single “champion”)\n",
        "# --------------------------------\n",
        "rf_params = {\n",
        "    \"n_estimators\": int(TRAIN_CFG.get(\"rf_n_estimators\", 400)) if isinstance(TRAIN_CFG, dict) else 400,\n",
        "    \"max_depth\": TRAIN_CFG.get(\"rf_max_depth\", None) if isinstance(TRAIN_CFG, dict) else None,\n",
        "    \"min_samples_leaf\": int(TRAIN_CFG.get(\"rf_min_samples_leaf\", 2)) if isinstance(TRAIN_CFG, dict) else 2,\n",
        "    \"n_jobs\": -1,\n",
        "    \"random_state\": SEED,\n",
        "}\n",
        "rf = RandomForestRegressor(**rf_params)\n",
        "\n",
        "# Train in log-space (stabile) e predici in scala naturale (k€) via TTR\n",
        "ttr = TransformedTargetRegressor(\n",
        "    regressor=Pipeline(steps=[\n",
        "        (\"prep\", prep),\n",
        "        (\"model\", rf),\n",
        "    ]),\n",
        "    func=np.log1p,\n",
        "    inverse_func=np.expm1,\n",
        "    check_inverse=False,\n",
        ")\n",
        "\n",
        "# --------------------------------\n",
        "# D) Prepare train/valid sets (aligned columns)\n",
        "# --------------------------------\n",
        "def _ensure_cols(df_part: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
        "    dfp = df_part.copy()\n",
        "    for c in cols:\n",
        "        if c not in dfp.columns:\n",
        "            dfp[c] = np.nan\n",
        "    return dfp[cols]  # keep order\n",
        "\n",
        "X_train = _ensure_cols(df_train, FEATURES)\n",
        "y_train = pd.to_numeric(df_train[VALUATION_K], errors=\"coerce\").astype(\"float64\")\n",
        "\n",
        "X_valid = _ensure_cols(df_valid, FEATURES)\n",
        "y_valid = pd.to_numeric(df_valid[VALUATION_K], errors=\"coerce\").astype(\"float64\")\n",
        "\n",
        "# sample weights (train only)\n",
        "w_train = None\n",
        "if \"sample_weight\" in df_train.columns:\n",
        "    w = pd.to_numeric(df_train[\"sample_weight\"], errors=\"coerce\").astype(\"float64\")\n",
        "    w = w.replace([np.inf, -np.inf], np.nan).fillna(1.0).clip(lower=0.0)\n",
        "    # normalize to mean≈1 per stabilità\n",
        "    mean_w = float(w.mean()) if float(w.mean()) > 0 else 1.0\n",
        "    w_train = (w / mean_w).to_numpy()\n",
        "\n",
        "# Persist a small preview per sanity\n",
        "preview = {\n",
        "    \"n_features\": len(FEATURES),\n",
        "    \"categorical\": len(cat_cols),\n",
        "    \"numeric\": len(num_cols),\n",
        "    \"rf_params\": rf_params,\n",
        "}\n",
        "(ART_DIR / \"pipeline_preview.json\").write_text(canonical_json_dumps(preview), encoding=\"utf-8\")\n",
        "print(f\"Pipeline preview saved → {ART_DIR / 'pipeline_preview.json'}\")\n",
        "\n",
        "print(\"Prepared X/y:\")\n",
        "print(f\"  X_train={X_train.shape}, y_train={y_train.shape}, sample_weight={'yes' if w_train is not None else 'no'}\")\n",
        "print(f\"  X_valid={X_valid.shape}, y_valid={y_valid.shape}\")\n",
        "\n",
        "# Expose objects for next cells\n",
        "pipeline = ttr          # main model object to fit\n",
        "preprocessor = prep     # for introspection/feature names\n",
        "chosen_features = FEATURES"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2eb712d9",
      "metadata": {},
      "source": [
        "### Train & Validation (single champion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bec884b6",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-10-07 17:05:55,496] INFO model_trainer: Passing sample_weight with key(s): ['model__sample_weight']\n",
            "[2025-10-07 17:05:55,497] INFO model_trainer: Fitting champion pipeline…\n",
            "[2025-10-07 17:05:59,245] INFO model_trainer: Fit completed.\n",
            "[2025-10-07 17:05:59,331] INFO model_trainer: Saved validation report → c:/Users/anven/OneDrive/Documenti/GitHub/axiomatic_oracle/notebooks/outputs/modeling/artifacts/eval_valid.json\n",
            "[2025-10-07 17:05:59,339] INFO model_trainer: Saved validation predictions → c:/Users/anven/OneDrive/Documenti/GitHub/axiomatic_oracle/notebooks/outputs/modeling/artifacts/predictions_valid.csv\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation metrics: {'MAE': 55.02990196764213, 'RMSE': 5837.736915453582, 'R2': 0.9250257802906402}\n",
            "Saved: c:\\Users\\anven\\OneDrive\\Documenti\\GitHub\\axiomatic_oracle\\notebooks\\outputs\\modeling\\figures\\valid_scatter_true_vs_pred.png\n",
            "Saved: c:\\Users\\anven\\OneDrive\\Documenti\\GitHub\\axiomatic_oracle\\notebooks\\outputs\\modeling\\figures\\valid_residuals_hist.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-10-07 17:06:00,922] INFO model_trainer: Saved champion model → c:/Users/anven/OneDrive/Documenti/GitHub/axiomatic_oracle/notebooks/outputs/modeling/champion_model.joblib\n"
          ]
        }
      ],
      "source": [
        "# 06) Train & Validation (single champion) — fixed sample_weight routing\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "import joblib  # type: ignore\n",
        "\n",
        "from shared.common.utils import canonical_json_dumps\n",
        "from shared.common.constants import VALUATION_K\n",
        "\n",
        "# --------------------------\n",
        "# Safety checks\n",
        "# --------------------------\n",
        "req_objs = [\"pipeline\", \"X_train\", \"y_train\", \"X_valid\", \"y_valid\"]\n",
        "missing = [o for o in req_objs if o not in globals()]\n",
        "if missing:\n",
        "    raise RuntimeError(f\"Missing objects before training: {missing}\")\n",
        "\n",
        "# --------------------------\n",
        "# Helper: robust sample_weight routing\n",
        "# --------------------------\n",
        "def _final_step_name(p: Pipeline) -> str:\n",
        "    return list(p.named_steps.keys())[-1]\n",
        "\n",
        "def _make_fit_kwargs(est, sample_weight):\n",
        "    \"\"\"\n",
        "    Route sample_weight correctly across:\n",
        "    1) Pipeline(..., ('ttr', TTR(regressor=Pipeline(..., ('model', RF))))) -> {'ttr__model__sample_weight': w}\n",
        "    2) Pipeline(..., ('ttr', TTR(regressor=RF)))                           -> {'ttr__sample_weight': w}\n",
        "    3) Pipeline(..., ('rf', RF))                                           -> {'rf__sample_weight': w}\n",
        "    4) TransformedTargetRegressor(regressor=Pipeline(..., ('model', RF)))  -> {'model__sample_weight': w}\n",
        "    5) TransformedTargetRegressor(regressor=RF)                            -> {'sample_weight': w}\n",
        "    6) Plain estimator                                                     -> {'sample_weight': w}\n",
        "    \"\"\"\n",
        "    # Case A: top-level is a Pipeline\n",
        "    if isinstance(est, Pipeline):\n",
        "        last_name = _final_step_name(est)\n",
        "        last_obj  = est.named_steps[last_name]\n",
        "\n",
        "        # A1) final is TTR\n",
        "        if isinstance(last_obj, TransformedTargetRegressor):\n",
        "            reg = getattr(last_obj, \"regressor_\", None) or getattr(last_obj, \"regressor\", None)\n",
        "            # A1a) regressor is a Pipeline -> need two-level namespacing\n",
        "            if isinstance(reg, Pipeline):\n",
        "                inner_last = _final_step_name(reg)\n",
        "                return {f\"{last_name}__{inner_last}__sample_weight\": sample_weight}\n",
        "            # A1b) regressor is a plain estimator -> pass to TTR step\n",
        "            return {f\"{last_name}__sample_weight\": sample_weight}\n",
        "\n",
        "        # A2) final is a plain estimator\n",
        "        return {f\"{last_name}__sample_weight\": sample_weight}\n",
        "\n",
        "    # Case B: top-level is a TTR\n",
        "    if isinstance(est, TransformedTargetRegressor):\n",
        "        reg = getattr(est, \"regressor_\", None) or getattr(est, \"regressor\", None)\n",
        "        if isinstance(reg, Pipeline):\n",
        "            inner_last = _final_step_name(reg)\n",
        "            return {f\"{inner_last}__sample_weight\": sample_weight}\n",
        "        return {\"sample_weight\": sample_weight}\n",
        "\n",
        "    # Case C: plain estimator\n",
        "    return {\"sample_weight\": sample_weight}\n",
        "\n",
        "# --------------------------\n",
        "# A) Fit champion pipeline\n",
        "# --------------------------\n",
        "# Build/locate training weights if available\n",
        "w_train = None\n",
        "if \"w_train\" in globals() and w_train is not None:\n",
        "    w_train = np.asarray(w_train, dtype=float).ravel()\n",
        "elif \"df_train\" in globals() and isinstance(df_train, pd.DataFrame) and \"sample_weight\" in df_train.columns:\n",
        "    # align by index if X_train has an index from df_train\n",
        "    idx = getattr(X_train, \"index\", None)\n",
        "    if idx is not None and set(idx).issubset(set(df_train.index)):\n",
        "        w_train = df_train.loc[idx, \"sample_weight\"].to_numpy(dtype=float, copy=False).ravel()\n",
        "    else:\n",
        "        w_train = df_train[\"sample_weight\"].to_numpy(dtype=float, copy=False).ravel()\n",
        "\n",
        "fit_kwargs = {}\n",
        "if w_train is not None:\n",
        "    fit_kwargs = _make_fit_kwargs(pipeline, w_train)\n",
        "    logger.info(\"Passing sample_weight with key(s): %s\", list(fit_kwargs.keys()))\n",
        "\n",
        "logger.info(\"Fitting champion pipeline…\")\n",
        "pipeline.fit(X_train, y_train, **fit_kwargs)\n",
        "logger.info(\"Fit completed.\")\n",
        "\n",
        "# --------------------------\n",
        "# B) Validation evaluation\n",
        "# --------------------------\n",
        "y_pred_valid = pipeline.predict(X_valid)\n",
        "# guard tiny negatives\n",
        "y_pred_valid = np.maximum(y_pred_valid, 0.0)\n",
        "\n",
        "mae  = float(mean_absolute_error(y_valid, y_pred_valid))\n",
        "rmse = float(mean_squared_error(y_valid, y_pred_valid))\n",
        "r2   = float(r2_score(y_valid, y_pred_valid))\n",
        "\n",
        "eval_report = {\n",
        "    \"split\": \"valid\",\n",
        "    \"metrics\": {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2},\n",
        "    \"n_valid\": int(len(y_valid)),\n",
        "    \"target\": VALUATION_K,\n",
        "}\n",
        "print(\"Validation metrics:\", eval_report[\"metrics\"])\n",
        "(ART_DIR / \"eval_valid.json\").write_text(canonical_json_dumps(eval_report), encoding=\"utf-8\")\n",
        "logger.info(\"Saved validation report → %s\", (ART_DIR / \"eval_valid.json\").as_posix())\n",
        "\n",
        "# --------------------------\n",
        "# C) Quick diagnostic plots\n",
        "# --------------------------\n",
        "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "preds_valid = pd.DataFrame({\n",
        "    \"y_true\": y_valid,\n",
        "    \"y_pred\": y_pred_valid,\n",
        "}, index=getattr(X_valid, \"index\", None))\n",
        "preds_valid.to_csv(ART_DIR / \"predictions_valid.csv\", index=True)\n",
        "logger.info(\"Saved validation predictions → %s\", (ART_DIR / \"predictions_valid.csv\").as_posix())\n",
        "\n",
        "# Scatter: y_true vs y_pred\n",
        "plt.figure(figsize=(6, 6))\n",
        "lim_lo = float(np.nanmin([preds_valid[\"y_true\"].min(), preds_valid[\"y_pred\"].min(), 0]))\n",
        "lim_hi = float(np.nanmax([preds_valid[\"y_true\"].max(), preds_valid[\"y_pred\"].max()]))\n",
        "plt.scatter(preds_valid[\"y_true\"], preds_valid[\"y_pred\"], s=8, alpha=0.5)\n",
        "plt.plot([lim_lo, lim_hi], [lim_lo, lim_hi], lw=1)  # y=x\n",
        "plt.xlabel(\"True valuation_k\")\n",
        "plt.ylabel(\"Predicted valuation_k\")\n",
        "plt.title(\"Validation — y_true vs y_pred\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"valid_scatter_true_vs_pred.png\", dpi=150)\n",
        "plt.close()\n",
        "print(\"Saved:\", FIG_DIR / \"valid_scatter_true_vs_pred.png\")\n",
        "\n",
        "# Residuals histogram\n",
        "resid = preds_valid[\"y_pred\"] - preds_valid[\"y_true\"]\n",
        "plt.figure(figsize=(7, 4))\n",
        "plt.hist(resid, bins=40)\n",
        "plt.xlabel(\"Residual (pred - true)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Validation residuals\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"valid_residuals_hist.png\", dpi=150)\n",
        "plt.close()\n",
        "print(\"Saved:\", FIG_DIR / \"valid_residuals_hist.png\")\n",
        "\n",
        "# --------------------------\n",
        "# D) Persist model artifact\n",
        "# --------------------------\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "model_path = MODEL_DIR / \"champion_model.joblib\"\n",
        "\n",
        "feature_spec_safe = globals().get(\"feature_spec\", {}) or {}\n",
        "chosen_features_safe = globals().get(\"chosen_features\", []) or []\n",
        "\n",
        "joblib.dump(\n",
        "    {\n",
        "        \"model\": pipeline,\n",
        "        \"features\": {\n",
        "            \"categorical\": feature_spec_safe.get(\"categorical\", []),\n",
        "            \"numeric\": feature_spec_safe.get(\"numeric\", []),\n",
        "            \"all\": chosen_features_safe,\n",
        "        },\n",
        "        \"seed\": int(SEED) if \"SEED\" in globals() else None,\n",
        "        \"metrics_valid\": eval_report[\"metrics\"],\n",
        "    },\n",
        "    model_path,\n",
        ")\n",
        "logger.info(\"Saved champion model → %s\", model_path.as_posix())\n",
        "\n",
        "# Expose artifacts to later cells\n",
        "champion = pipeline\n",
        "valid_report = eval_report"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "279cc11f-e303-48de-92c6-d38c9ec38bf1",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Feature Importance (on champion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "de9c6c54-9cfc-4c43-8cb6-721f5066387c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: c:\\Users\\anven\\OneDrive\\Documenti\\GitHub\\axiomatic_oracle\\notebooks\\outputs\\modeling\\figures\\feature_importance_builtin.png\n",
            "Saved: c:\\Users\\anven\\OneDrive\\Documenti\\GitHub\\axiomatic_oracle\\notebooks\\outputs\\modeling\\figures\\feature_importance_permutation.png\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>importance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>f33</td>\n",
              "      <td>0.512114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>f38</td>\n",
              "      <td>0.126428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>f3</td>\n",
              "      <td>0.065747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>f63</td>\n",
              "      <td>0.047259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>f1</td>\n",
              "      <td>0.036277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>f32</td>\n",
              "      <td>0.032433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>f4</td>\n",
              "      <td>0.030340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>f22</td>\n",
              "      <td>0.022865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>f0</td>\n",
              "      <td>0.022375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>f45</td>\n",
              "      <td>0.018183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>f44</td>\n",
              "      <td>0.017842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>f7</td>\n",
              "      <td>0.011846</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   feature  importance\n",
              "0      f33    0.512114\n",
              "1      f38    0.126428\n",
              "2       f3    0.065747\n",
              "3      f63    0.047259\n",
              "4       f1    0.036277\n",
              "5      f32    0.032433\n",
              "6       f4    0.030340\n",
              "7      f22    0.022865\n",
              "8       f0    0.022375\n",
              "9      f45    0.018183\n",
              "10     f44    0.017842\n",
              "11      f7    0.011846"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>importance</th>\n",
              "      <th>std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>size_m2</td>\n",
              "      <td>0.911539</td>\n",
              "      <td>0.023997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>distance_to_center_km</td>\n",
              "      <td>0.199011</td>\n",
              "      <td>0.006644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>zone</td>\n",
              "      <td>0.077972</td>\n",
              "      <td>0.002329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>region</td>\n",
              "      <td>0.077057</td>\n",
              "      <td>0.003963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>city</td>\n",
              "      <td>0.064862</td>\n",
              "      <td>0.004178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>energy_class</td>\n",
              "      <td>0.064219</td>\n",
              "      <td>0.002874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>location</td>\n",
              "      <td>0.062918</td>\n",
              "      <td>0.004143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>region_index_prior</td>\n",
              "      <td>0.050719</td>\n",
              "      <td>0.002218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>age_years</td>\n",
              "      <td>0.022628</td>\n",
              "      <td>0.001173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>year_built</td>\n",
              "      <td>0.021321</td>\n",
              "      <td>0.001114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>condition</td>\n",
              "      <td>0.019860</td>\n",
              "      <td>0.001047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>luxury_score</td>\n",
              "      <td>0.001931</td>\n",
              "      <td>0.000269</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  feature  importance       std\n",
              "0                 size_m2    0.911539  0.023997\n",
              "1   distance_to_center_km    0.199011  0.006644\n",
              "2                    zone    0.077972  0.002329\n",
              "3                  region    0.077057  0.003963\n",
              "4                    city    0.064862  0.004178\n",
              "5            energy_class    0.064219  0.002874\n",
              "6                location    0.062918  0.004143\n",
              "7      region_index_prior    0.050719  0.002218\n",
              "8               age_years    0.022628  0.001173\n",
              "9              year_built    0.021321  0.001114\n",
              "10              condition    0.019860  0.001047\n",
              "11           luxury_score    0.001931  0.000269"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 07) Feature importances (champion) — robust, single-pipeline\n",
        "from __future__ import annotations\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from shared.common.constants import VALUATION_K\n",
        "\n",
        "TOPN = 20  # top bars in charts\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers\n",
        "# ----------------------------\n",
        "def _extract_parts(model_like):\n",
        "    \"\"\"\n",
        "    Return (inner_pipe, preproc, final_est, final_step_name, is_ttr)\n",
        "    - inner_pipe: sklearn Pipeline with .named_steps\n",
        "    - preproc   : ColumnTransformer in step 'prep' (if present)\n",
        "    - final_est : last estimator (e.g. RandomForestRegressor)\n",
        "    - final_step_name: name of the last step (e.g. 'model'/'rf')\n",
        "    - is_ttr    : True if model_like is a TransformedTargetRegressor\n",
        "    \"\"\"\n",
        "    is_ttr = isinstance(model_like, TransformedTargetRegressor)\n",
        "    inner = (\n",
        "        model_like.regressor_ if is_ttr and hasattr(model_like, \"regressor_\")\n",
        "        else (model_like.regressor if is_ttr and hasattr(model_like, \"regressor\") else model_like)\n",
        "    )\n",
        "    if not isinstance(inner, Pipeline):\n",
        "        raise RuntimeError(\"Expected a Pipeline with a 'prep' step and a final estimator.\")\n",
        "\n",
        "    preproc = inner.named_steps.get(\"prep\", None)\n",
        "\n",
        "    # pick the last step as final estimator (prefer common names if present)\n",
        "    final_step_name = None\n",
        "    for cand in (\"model\", \"rf\", \"regressor\"):\n",
        "        if cand in inner.named_steps:\n",
        "            final_step_name = cand\n",
        "            break\n",
        "    if final_step_name is None:\n",
        "        final_step_name = list(inner.named_steps.keys())[-1]\n",
        "    final_est = inner.named_steps[final_step_name]\n",
        "    return inner, preproc, final_est, final_step_name, is_ttr\n",
        "\n",
        "\n",
        "def _feature_names_from_ct(preproc, fallback_cat: list[str], fallback_num: list[str]) -> list[str]:\n",
        "    \"\"\"Extract feature names in ColumnTransformer order (cat then num).\"\"\"\n",
        "    if preproc is None or not hasattr(preproc, \"transformers\"):\n",
        "        # fallback to provided lists\n",
        "        seen, ordered = set(), []\n",
        "        for c in list(fallback_cat) + list(fallback_num):\n",
        "            if c not in seen:\n",
        "                seen.add(c); ordered.append(c)\n",
        "        return ordered\n",
        "\n",
        "    cat_cols_ct, num_cols_ct = [], []\n",
        "    for name, est, cols in preproc.transformers:\n",
        "        if name == \"cat\":\n",
        "            cat_cols_ct = list(cols) if isinstance(cols, (list, tuple, np.ndarray, pd.Index)) else list(fallback_cat)\n",
        "        elif name == \"num\":\n",
        "            num_cols_ct = list(cols) if isinstance(cols, (list, tuple, np.ndarray, pd.Index)) else list(fallback_num)\n",
        "\n",
        "    seen, ordered = set(), []\n",
        "    for c in list(cat_cols_ct) + list(num_cols_ct):\n",
        "        if c not in seen:\n",
        "            seen.add(c); ordered.append(c)\n",
        "    return ordered\n",
        "\n",
        "\n",
        "def _ensure_cols(df_part: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
        "    \"\"\"Add missing columns as NaN; return with exact column order.\"\"\"\n",
        "    dfp = df_part.copy()\n",
        "    missing = [c for c in cols if c not in dfp.columns]\n",
        "    for c in missing:\n",
        "        dfp[c] = np.nan\n",
        "    return dfp[cols]\n",
        "\n",
        "\n",
        "def _to_parquet_optional(df: pd.DataFrame, path: Path):\n",
        "    \"\"\"Try writing Parquet; skip quietly if engine is missing.\"\"\"\n",
        "    try:\n",
        "        df.to_parquet(path, index=False)\n",
        "    except Exception as e:\n",
        "        logger.info(\"Parquet export skipped for %s: %s\", path, e)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Choose champion (single pipeline)\n",
        "# ----------------------------\n",
        "if \"champion\" in globals() and champion is not None:\n",
        "    chosen = champion\n",
        "elif \"pipeline\" in globals() and pipeline is not None:\n",
        "    chosen = pipeline\n",
        "else:\n",
        "    raise RuntimeError(\"No trained pipeline found. Expected variable 'champion' or 'pipeline'.\")\n",
        "\n",
        "inner_pipe, preproc, final_est, final_step, is_ttr = _extract_parts(chosen)\n",
        "\n",
        "# Feature lists from our spec created in the previous cell\n",
        "if \"feature_spec\" not in globals():\n",
        "    raise RuntimeError(\"feature_spec is missing (expected from the previous cell).\")\n",
        "\n",
        "cat_fallback = list(feature_spec.get(\"categorical\", []))\n",
        "num_fallback = list(feature_spec.get(\"numeric\", []))\n",
        "feat_in_use  = _feature_names_from_ct(preproc, cat_fallback, num_fallback)\n",
        "\n",
        "# Build aligned X_test\n",
        "if \"df_test\" not in globals():\n",
        "    raise RuntimeError(\"df_test is missing.\")\n",
        "X_tst_use = _ensure_cols(df_test, feat_in_use)\n",
        "\n",
        "# ----------------------------\n",
        "# Built-in importance (if available)\n",
        "# ----------------------------\n",
        "builtin_imp = None\n",
        "fi_raw = getattr(final_est, \"feature_importances_\", None)\n",
        "if fi_raw is not None:\n",
        "    try:\n",
        "        imp = np.asarray(fi_raw)\n",
        "        if imp.ndim == 1 and imp.size > 0:\n",
        "            feat_names = list(feat_in_use) if len(feat_in_use) == imp.shape[0] else [f\"f{i}\" for i in range(int(imp.shape[0]))]\n",
        "            builtin_imp = (\n",
        "                pd.DataFrame({\"feature\": feat_names, \"importance\": imp.astype(float, copy=False)})\n",
        "                .sort_values(\"importance\", ascending=False)\n",
        "                .reset_index(drop=True)\n",
        "            )\n",
        "            builtin_imp.to_csv(MODEL_DIR / \"feature_importance_builtin.csv\", index=False)\n",
        "            _to_parquet_optional(builtin_imp, MODEL_DIR / \"feature_importance_builtin.parquet\")\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            _top = min(TOPN, len(builtin_imp))\n",
        "            ax = builtin_imp.head(_top).plot(kind=\"bar\", x=\"feature\", y=\"importance\", legend=False, rot=45)\n",
        "            ax.set_title(f\"Built-in Feature Importance (top {_top})\")\n",
        "            ax.set_ylabel(\"Importance\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(FIG_DIR / \"feature_importance_builtin.png\", dpi=150)\n",
        "            plt.close()\n",
        "            print(\"Saved:\", FIG_DIR / \"feature_importance_builtin.png\")\n",
        "        else:\n",
        "            print(\"⚠️ feature_importances_ present but empty → skipping built-in.\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Unable to compute built-in importance: {e}\")\n",
        "else:\n",
        "    print(\"ℹ️ feature_importances_ not available on the final estimator → skipping built-in.\")\n",
        "\n",
        "# ----------------------------\n",
        "# Permutation importance\n",
        "# ----------------------------\n",
        "# y scale:\n",
        "#  - if using TTR, predictions are on natural scale → pass natural y\n",
        "#  - if the pipeline was trained on log1p(y) directly, pass log1p(y)\n",
        "if is_ttr:\n",
        "    y_perm = df_test[VALUATION_K].to_numpy(dtype=\"float64\", copy=False)\n",
        "else:\n",
        "    y_perm = np.log1p(df_test[VALUATION_K].to_numpy(dtype=\"float64\", copy=False))\n",
        "\n",
        "perm = permutation_importance(\n",
        "    estimator=chosen,           # TTR or Pipeline\n",
        "    X=X_tst_use,\n",
        "    y=y_perm,\n",
        "    n_repeats=8,\n",
        "    random_state=SEED if \"SEED\" in globals() else 42,\n",
        "    n_jobs=-1,\n",
        "    scoring=\"r2\",\n",
        ")\n",
        "\n",
        "feat_names_pi = list(feat_in_use) if len(feat_in_use) == perm.importances_mean.shape[0] \\\n",
        "                else [f\"f{i}\" for i in range(perm.importances_mean.shape[0])]\n",
        "\n",
        "perm_imp = (\n",
        "    pd.DataFrame({\n",
        "        \"feature\": feat_names_pi,\n",
        "        \"importance\": perm.importances_mean.astype(float, copy=False),\n",
        "        \"std\": perm.importances_std.astype(float, copy=False),\n",
        "    })\n",
        "    .sort_values(\"importance\", ascending=False)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "perm_imp.to_csv(MODEL_DIR / \"feature_importance_permutation.csv\", index=False)\n",
        "_to_parquet_optional(perm_imp, MODEL_DIR / \"feature_importance_permutation.parquet\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "_top = min(TOPN, len(perm_imp))\n",
        "ax = perm_imp.head(_top).plot(kind=\"bar\", x=\"feature\", y=\"importance\", yerr=\"std\", legend=False, rot=45)\n",
        "ax.set_title(f\"Permutation Importance (top {_top})\")\n",
        "ax.set_ylabel(\"Importance (mean ΔR²)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"feature_importance_permutation.png\", dpi=150)\n",
        "plt.close()\n",
        "print(\"Saved:\", FIG_DIR / \"feature_importance_permutation.png\")\n",
        "\n",
        "# Quick preview\n",
        "if builtin_imp is not None:\n",
        "    display(builtin_imp.head(12))\n",
        "display(perm_imp.head(12))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca8ec440",
      "metadata": {},
      "source": [
        "### Segment Valuations & Predictions Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a72e0ffa",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-10-07 17:06:28,280] INFO model_trainer: Added 1 missing columns to TEST (imputed as NaN): ['severity_score']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: c:\\Users\\anven\\OneDrive\\Documenti\\GitHub\\axiomatic_oracle\\notebooks\\outputs\\modeling\\figures\\residuals_test_hist.png\n",
            "Saved: c:\\Users\\anven\\OneDrive\\Documenti\\GitHub\\axiomatic_oracle\\notebooks\\outputs\\modeling\\figures\\residuals_vs_pred_test.png\n",
            "Saved: c:\\Users\\anven\\OneDrive\\Documenti\\GitHub\\axiomatic_oracle\\notebooks\\outputs\\modeling\\metrics_by_decile.csv\n",
            "Saved: c:\\Users\\anven\\OneDrive\\Documenti\\GitHub\\axiomatic_oracle\\notebooks\\outputs\\modeling\\metrics_by_location.csv\n",
            "Saved: c:\\Users\\anven\\OneDrive\\Documenti\\GitHub\\axiomatic_oracle\\notebooks\\outputs\\modeling\\predictions_test.csv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>asset_id</th>\n",
              "      <th>location</th>\n",
              "      <th>valuation_k</th>\n",
              "      <th>y_pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>asset_000000</td>\n",
              "      <td>Venice</td>\n",
              "      <td>237.570007</td>\n",
              "      <td>208.332203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>asset_000002</td>\n",
              "      <td>Bari</td>\n",
              "      <td>374.720001</td>\n",
              "      <td>318.625580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>asset_000003</td>\n",
              "      <td>Genoa</td>\n",
              "      <td>572.309998</td>\n",
              "      <td>677.990431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>asset_000013</td>\n",
              "      <td>Verona</td>\n",
              "      <td>176.839996</td>\n",
              "      <td>217.349668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>asset_000022</td>\n",
              "      <td>Cagliari</td>\n",
              "      <td>118.820000</td>\n",
              "      <td>152.746103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>asset_000035</td>\n",
              "      <td>Milan</td>\n",
              "      <td>794.280029</td>\n",
              "      <td>816.461326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>asset_000051</td>\n",
              "      <td>Milan</td>\n",
              "      <td>224.869995</td>\n",
              "      <td>186.147949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>asset_000058</td>\n",
              "      <td>Milan</td>\n",
              "      <td>1378.780029</td>\n",
              "      <td>1188.465781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>asset_000079</td>\n",
              "      <td>Venice</td>\n",
              "      <td>467.339996</td>\n",
              "      <td>398.150185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>asset_000088</td>\n",
              "      <td>Milan</td>\n",
              "      <td>1228.660034</td>\n",
              "      <td>1205.232037</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        asset_id  location  valuation_k       y_pred\n",
              "0   asset_000000    Venice   237.570007   208.332203\n",
              "2   asset_000002      Bari   374.720001   318.625580\n",
              "3   asset_000003     Genoa   572.309998   677.990431\n",
              "13  asset_000013    Verona   176.839996   217.349668\n",
              "22  asset_000022  Cagliari   118.820000   152.746103\n",
              "35  asset_000035     Milan   794.280029   816.461326\n",
              "51  asset_000051     Milan   224.869995   186.147949\n",
              "58  asset_000058     Milan  1378.780029  1188.465781\n",
              "79  asset_000079    Venice   467.339996   398.150185\n",
              "88  asset_000088     Milan  1228.660034  1205.232037"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 08) Segment valuations & save predictions — robust, single-pipeline\n",
        "from __future__ import annotations\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from shared.common.constants import VALUATION_K, ASSET_ID, LOCATION\n",
        "\n",
        "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------------- helpers ----------------\n",
        "def _expm1_safe(z, cap: float = 12.0):\n",
        "    z = np.asarray(z, dtype=np.float64)\n",
        "    z = np.clip(z, -20.0, cap)  # cap to avoid overflow on extreme log-preds\n",
        "    out = np.expm1(z)\n",
        "    out[out < 0] = 0.0\n",
        "    return out\n",
        "\n",
        "def _to_parquet_optional(df: pd.DataFrame, path: Path):\n",
        "    \"\"\"Try Parquet; skip quietly if engine is missing.\"\"\"\n",
        "    try:\n",
        "        df.to_parquet(path, index=False)\n",
        "    except Exception as e:\n",
        "        logger.info(\"Parquet export skipped for %s: %s\", path, e)\n",
        "\n",
        "def _ensure_cols(df_part: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
        "    dfp = df_part.copy()\n",
        "    miss = [c for c in cols if c not in dfp.columns]\n",
        "    if miss:\n",
        "        for c in miss:\n",
        "            dfp[c] = np.nan\n",
        "        logger.info(\"Added %d missing columns to TEST (imputed as NaN): %s\", len(miss), miss[:10])\n",
        "    return dfp[cols]\n",
        "\n",
        "def _extract_parts(model_like):\n",
        "    \"\"\"\n",
        "    Return (inner_pipe, preproc, final_est, final_step_name, is_ttr)\n",
        "    - inner_pipe: sklearn Pipeline with .named_steps\n",
        "    - preproc   : ColumnTransformer in step 'prep' (if present)\n",
        "    - final_est : last estimator (e.g. RandomForestRegressor)\n",
        "    - final_step_name: name of last step\n",
        "    - is_ttr    : True if model_like is a TransformedTargetRegressor\n",
        "    \"\"\"\n",
        "    is_ttr = isinstance(model_like, TransformedTargetRegressor)\n",
        "    inner = (\n",
        "        model_like.regressor_ if is_ttr and hasattr(model_like, \"regressor_\")\n",
        "        else (model_like.regressor if is_ttr and hasattr(model_like, \"regressor\") else model_like)\n",
        "    )\n",
        "    if not isinstance(inner, Pipeline):\n",
        "        raise RuntimeError(\"Expected a Pipeline with a 'prep' step and a final estimator.\")\n",
        "    preproc = inner.named_steps.get(\"prep\", None)\n",
        "    # choose last step as final estimator unless a common alias is present\n",
        "    final_step_name = None\n",
        "    for cand in (\"model\", \"rf\", \"regressor\"):\n",
        "        if cand in inner.named_steps:\n",
        "            final_step_name = cand\n",
        "            break\n",
        "    if final_step_name is None:\n",
        "        final_step_name = list(inner.named_steps.keys())[-1]\n",
        "    final_est = inner.named_steps[final_step_name]\n",
        "    return inner, preproc, final_est, final_step_name, is_ttr\n",
        "\n",
        "def _feature_names_from_ct(preproc, fallback_cat: list[str], fallback_num: list[str]) -> list[str]:\n",
        "    \"\"\"Extract feature names in ColumnTransformer order (cat then num).\"\"\"\n",
        "    if preproc is None or not hasattr(preproc, \"transformers\"):\n",
        "        seen, ordered = set(), []\n",
        "        for c in list(fallback_cat) + list(fallback_num):\n",
        "            if c not in seen:\n",
        "                seen.add(c); ordered.append(c)\n",
        "        return ordered\n",
        "\n",
        "    cat_cols_ct, num_cols_ct = [], []\n",
        "    for name, est, cols in preproc.transformers:\n",
        "        if name == \"cat\":\n",
        "            cat_cols_ct = list(cols) if isinstance(cols, (list, tuple, np.ndarray, pd.Index)) else list(fallback_cat)\n",
        "        elif name == \"num\":\n",
        "            num_cols_ct = list(cols) if isinstance(cols, (list, tuple, np.ndarray, pd.Index)) else list(fallback_num)\n",
        "\n",
        "    seen, ordered = set(), []\n",
        "    for c in list(cat_cols_ct) + list(num_cols_ct):\n",
        "        if c not in seen:\n",
        "            seen.add(c); ordered.append(c)\n",
        "    return ordered\n",
        "\n",
        "def _predict_nat_from_champion(champ, X_df: pd.DataFrame, cols: list[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Predict on TEST in natural scale (k€), handling:\n",
        "      - TransformedTargetRegressor (natural output)\n",
        "      - Plain pipeline trained on log1p(y) (expm1 with safety)\n",
        "    \"\"\"\n",
        "    LOG_CAP = float(TRAIN_CFG.get(\"log_cap_clip\", 12.0)) if \"TRAIN_CFG\" in globals() else 12.0\n",
        "    X_use = _ensure_cols(X_df, cols)\n",
        "    if isinstance(champ, TransformedTargetRegressor):\n",
        "        return np.asarray(champ.predict(X_use), dtype=np.float64)\n",
        "    # pipeline trained on log1p target\n",
        "    log_pred = champ.predict(X_use)\n",
        "    return _expm1_safe(log_pred, cap=LOG_CAP)\n",
        "\n",
        "def _rmse(y_true, y_pred):\n",
        "    try:\n",
        "        return mean_squared_error(y_true, y_pred, squared=False)\n",
        "    except TypeError:\n",
        "        return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "def _group_apply_safe(df: pd.DataFrame, key: str, func):\n",
        "    gb = df.groupby(key, observed=True)\n",
        "    try:\n",
        "        out = gb.apply(func, include_groups=False)  # pandas >= 2.2\n",
        "    except TypeError:\n",
        "        out = gb[[\"y_true\", \"y_pred\"]].apply(func)  # pandas < 2.2\n",
        "    return out.reset_index()\n",
        "\n",
        "# ---------------- choose champion & feature order ----------------\n",
        "if \"champion\" not in globals() or champion is None:\n",
        "    raise RuntimeError(\"Missing trained 'champion' pipeline from the training cell.\")\n",
        "\n",
        "if \"feature_spec\" not in globals():\n",
        "    raise RuntimeError(\"feature_spec is missing (expected from the Feature Preparation cell).\")\n",
        "\n",
        "inner_pipe, preproc, final_est, final_step, is_ttr = _extract_parts(champion)\n",
        "cat_fallback = list(feature_spec.get(\"categorical\", []))\n",
        "num_fallback = list(feature_spec.get(\"numeric\", []))\n",
        "feat_in_use  = _feature_names_from_ct(preproc, cat_fallback, num_fallback)\n",
        "\n",
        "# ---------------- 1) TEST predictions & residual plots ----------------\n",
        "if \"df_test\" not in globals():\n",
        "    raise RuntimeError(\"df_test is missing — cannot compute residuals.\")\n",
        "\n",
        "y_true_t = df_test[VALUATION_K].to_numpy(dtype=\"float64\", copy=False)\n",
        "X_tst_use = _ensure_cols(df_test, feat_in_use)\n",
        "y_pred_t = _predict_nat_from_champion(champion, X_tst_use, feat_in_use)\n",
        "\n",
        "# sanitize\n",
        "mask = np.isfinite(y_true_t) & np.isfinite(y_pred_t)\n",
        "valid_n = int(mask.sum())\n",
        "if valid_n < max(30, int(0.3 * len(y_true_t))):\n",
        "    bad_idx = np.where(~mask)[0][:10].tolist()\n",
        "    raise RuntimeError(\n",
        "        f\"Non-finite TEST predictions/targets: valid {valid_n}/{len(mask)}. \"\n",
        "        f\"Example bad idx: {bad_idx}. Check feature alignment and prediction scale.\"\n",
        "    )\n",
        "\n",
        "y_true_t = y_true_t[mask]\n",
        "y_pred_t = y_pred_t[mask]\n",
        "residuals = y_true_t - y_pred_t\n",
        "\n",
        "# residual hist\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.hist(residuals, bins=60, density=True)\n",
        "plt.title(\"Residuals (TEST)\")\n",
        "plt.xlabel(\"y − ŷ (k€)\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.tight_layout()\n",
        "out_res = FIG_DIR / \"residuals_test_hist.png\"\n",
        "plt.savefig(out_res, dpi=150, bbox_inches=\"tight\")\n",
        "plt.close()\n",
        "print(\"Saved:\", out_res)\n",
        "\n",
        "# residual vs pred\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(y_pred_t, residuals, s=10, alpha=0.6)\n",
        "plt.axhline(0.0, linestyle=\"--\")\n",
        "plt.title(\"Residuals vs Predictions (TEST)\")\n",
        "plt.xlabel(\"ŷ (k€)\")\n",
        "plt.ylabel(\"y − ŷ (k€)\")\n",
        "plt.tight_layout()\n",
        "out_sc = FIG_DIR / \"residuals_vs_pred_test.png\"\n",
        "plt.savefig(out_sc, dpi=150, bbox_inches=\"tight\")\n",
        "plt.close()\n",
        "print(\"Saved:\", out_sc)\n",
        "\n",
        "# ---------------- 2) Segmented metrics & predictions export ----------------\n",
        "# Build clean metric dataframe on valid rows\n",
        "idx_valid = df_test.index[mask]\n",
        "cols_keep = []\n",
        "if ASSET_ID in df_test.columns: cols_keep.append(ASSET_ID)\n",
        "if LOCATION in df_test.columns: cols_keep.append(LOCATION)\n",
        "\n",
        "dfm = df_test.loc[idx_valid, cols_keep].copy()\n",
        "dfm[\"y_true\"] = y_true_t\n",
        "dfm[\"y_pred\"] = y_pred_t\n",
        "\n",
        "# deciles on natural target\n",
        "try:\n",
        "    dfm[\"decile\"] = pd.qcut(dfm[\"y_true\"], q=10, labels=False, duplicates=\"drop\")\n",
        "except Exception:\n",
        "    dfm[\"decile\"] = 0\n",
        "\n",
        "def _agg_metrics(g: pd.DataFrame) -> pd.Series:\n",
        "    return pd.Series({\n",
        "        \"n\": int(len(g)),\n",
        "        \"MAE\": float(mean_absolute_error(g[\"y_true\"], g[\"y_pred\"])),\n",
        "        \"RMSE\": float(_rmse(g[\"y_true\"], g[\"y_pred\"])),\n",
        "        \"R2\": float(r2_score(g[\"y_true\"], g[\"y_pred\"])) if len(g) > 1 else np.nan,\n",
        "    })\n",
        "\n",
        "# by decile\n",
        "dec_rep = _group_apply_safe(dfm, \"decile\", _agg_metrics)\n",
        "dec_rep.to_csv(MODEL_DIR / \"metrics_by_decile.csv\", index=False)\n",
        "_to_parquet_optional(dec_rep, MODEL_DIR / \"metrics_by_decile.parquet\")\n",
        "print(\"Saved:\", MODEL_DIR / \"metrics_by_decile.csv\")\n",
        "\n",
        "# by location (if present)\n",
        "if LOCATION in dfm.columns:\n",
        "    loc_rep = _group_apply_safe(dfm, LOCATION, _agg_metrics)\n",
        "else:\n",
        "    loc_rep = pd.DataFrame([{\n",
        "        LOCATION: \"NA\",\n",
        "        \"n\": int(len(dfm)),\n",
        "        \"MAE\": float(mean_absolute_error(dfm[\"y_true\"], dfm[\"y_pred\"])) if len(dfm) else np.nan,\n",
        "        \"RMSE\": float(_rmse(dfm[\"y_true\"], dfm[\"y_pred\"])) if len(dfm) else np.nan,\n",
        "        \"R2\": float(r2_score(dfm[\"y_true\"], dfm[\"y_pred\"])) if len(dfm) > 1 else np.nan,\n",
        "    }])\n",
        "\n",
        "loc_rep.to_csv(MODEL_DIR / \"metrics_by_location.csv\", index=False)\n",
        "_to_parquet_optional(loc_rep, MODEL_DIR / \"metrics_by_location.parquet\")\n",
        "print(\"Saved:\", MODEL_DIR / \"metrics_by_location.csv\")\n",
        "\n",
        "# predictions export\n",
        "pred_cols = []\n",
        "if ASSET_ID in dfm.columns: pred_cols.append(ASSET_ID)\n",
        "if LOCATION in dfm.columns: pred_cols.append(LOCATION)\n",
        "pred_df = dfm[pred_cols + [\"y_true\", \"y_pred\"]].rename(columns={\"y_true\": VALUATION_K})\n",
        "\n",
        "pred_df.to_csv(MODEL_DIR / \"predictions_test.csv\", index=False, encoding=\"utf-8\")\n",
        "_to_parquet_optional(pred_df, MODEL_DIR / \"predictions_test.parquet\")\n",
        "print(\"Saved:\", MODEL_DIR / \"predictions_test.csv\")\n",
        "\n",
        "display(pred_df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bf1f075-89a4-4e49-a30c-2031f17ab419",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Model Persistence & Manifest Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "701beb69",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Saved serving pipeline (no-refit): value_regressor_v2.joblib\n",
            "✅ Saved feature_order: feature_order.json\n",
            "✅ Saved meta: value_regressor_v2_meta.json\n",
            "✅ Saved manifest: training_manifest.json\n",
            "✅ Saved worst-k: worst_k.json\n",
            "✅ Saved drift metrics → location_drift_train_vs_test.json  (manifest updated)\n"
          ]
        }
      ],
      "source": [
        "# 09) Model persistence & training manifest — single champion, no shim\n",
        "from __future__ import annotations\n",
        "\n",
        "import os, json, hashlib\n",
        "from datetime import datetime, timezone\n",
        "from pathlib import Path\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "\n",
        "# NOTE: path standardizzato (no \"notebooks.shared\")\n",
        "from shared.n03_train_model import metrics\n",
        "from shared.common.constants import VALUATION_K, ASSET_ID, LOCATION\n",
        "from shared.common.utils import canonical_json_dumps\n",
        "from shared.common.serving_transformers import GeoCanonizer, PriorsGuard, EnsureDerivedFeatures\n",
        "from shared.n03_train_model.preprocessing import list_required_serving_derivatives\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Thread limits (stability on some Windows/Python builds)\n",
        "# ---------------------------------------------------------------------------\n",
        "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
        "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
        "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
        "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Pre-flight checks & helpers\n",
        "# ---------------------------------------------------------------------------\n",
        "if \"champion\" not in globals() or champion is None:\n",
        "    raise RuntimeError(\"Missing trained 'champion' pipeline. Run the training cell first.\")\n",
        "\n",
        "def _extract_parts(model_like):\n",
        "    \"\"\"Return (inner_pipe, preproc, final_est, final_step_name, is_ttr).\"\"\"\n",
        "    from sklearn.compose import TransformedTargetRegressor\n",
        "    is_ttr = isinstance(model_like, TransformedTargetRegressor)\n",
        "    inner = (\n",
        "        model_like.regressor_ if is_ttr and hasattr(model_like, \"regressor_\")\n",
        "        else (model_like.regressor if is_ttr and hasattr(model_like, \"regressor\") else model_like)\n",
        "    )\n",
        "    if not isinstance(inner, Pipeline):\n",
        "        raise RuntimeError(\"Expected champion to be a Pipeline (possibly wrapped by TTR).\")\n",
        "    preproc = inner.named_steps.get(\"prep\", None)\n",
        "    # pick the last step unless a common alias exists\n",
        "    final_step_name = None\n",
        "    for cand in (\"model\", \"rf\", \"regressor\"):\n",
        "        if cand in inner.named_steps:\n",
        "            final_step_name = cand\n",
        "            break\n",
        "    if final_step_name is None:\n",
        "        final_step_name = list(inner.named_steps.keys())[-1]\n",
        "    final_est = inner.named_steps[final_step_name]\n",
        "    return inner, preproc, final_est, final_step_name, is_ttr\n",
        "\n",
        "def _feature_names_from_ct(preproc, fallback_cat: list[str], fallback_num: list[str]) -> list[str]:\n",
        "    \"\"\"ColumnTransformer order (cat then num).\"\"\"\n",
        "    if preproc is None or not hasattr(preproc, \"transformers\"):\n",
        "        seen, ordered = set(), []\n",
        "        for c in list(fallback_cat) + list(fallback_num):\n",
        "            if c not in seen:\n",
        "                seen.add(c); ordered.append(c)\n",
        "        return ordered\n",
        "\n",
        "    cat_cols_ct, num_cols_ct = [], []\n",
        "    for name, est, cols in preproc.transformers:\n",
        "        if name == \"cat\":\n",
        "            cat_cols_ct = list(cols) if isinstance(cols, (list, tuple, np.ndarray, pd.Index)) else list(fallback_cat)\n",
        "        elif name == \"num\":\n",
        "            num_cols_ct = list(cols) if isinstance(cols, (list, tuple, np.ndarray, pd.Index)) else list(fallback_num)\n",
        "\n",
        "    seen, ordered = set(), []\n",
        "    for c in list(cat_cols_ct) + list(num_cols_ct):\n",
        "        if c not in seen:\n",
        "            seen.add(c); ordered.append(c)\n",
        "    return ordered\n",
        "\n",
        "def _sha256_file(p: Path, chunk: int = 1 << 20) -> str:\n",
        "    h = hashlib.sha256()\n",
        "    with p.open(\"rb\") as f:\n",
        "        for ch in iter(lambda: f.read(chunk), b\"\"):\n",
        "            h.update(ch)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def _to_parquet_optional(df: pd.DataFrame, path: Path):\n",
        "    try:\n",
        "        df.to_parquet(path, index=False)\n",
        "    except Exception as e:\n",
        "        logger.info(\"Parquet export skipped for %s: %s\", path, e)\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Resolve champion parts & expected feature order\n",
        "# ---------------------------------------------------------------------------\n",
        "inner_pipe, preproc, final_est, final_step_name, is_ttr = _extract_parts(champion)\n",
        "\n",
        "# Prefer explicit feature_spec from the “Feature Preparation” cell\n",
        "if \"feature_spec\" not in globals():\n",
        "    raise RuntimeError(\"feature_spec is missing (expected from the Feature Preparation cell).\")\n",
        "\n",
        "cat_cols = list(feature_spec.get(\"categorical\", []))\n",
        "num_cols = list(feature_spec.get(\"numeric\", []))\n",
        "feature_order = _feature_names_from_ct(preproc, cat_cols, num_cols)\n",
        "\n",
        "# sanity: check the champion is fitted\n",
        "try:\n",
        "    check_is_fitted(inner_pipe)\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Champion pipeline does not look fitted: {e}\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Build serving pipeline (no refit) — Geo → Priors → Derived → core\n",
        "# ---------------------------------------------------------------------------\n",
        "# Pull priors from CONFIG (same source used in generation & FE)\n",
        "GEN_CFG = CONFIG.get(\"generation\", {}) if isinstance(CONFIG, dict) else {}\n",
        "_city_base_raw = GEN_CFG.get(\"city_base_prices\", {}) or {}\n",
        "region_index_defaults = {\"north\": 1.05, \"center\": 1.00, \"south\": 0.92}\n",
        "_region_index = GEN_CFG.get(\"region_index\", {}) or region_index_defaults\n",
        "\n",
        "CITY_BASE = {\n",
        "    str(c).strip().lower(): {str(z).strip().lower(): float(v) for z, v in d.items()}\n",
        "    for c, d in _city_base_raw.items()\n",
        "}\n",
        "_ZONE_KEYS = set(z for d in CITY_BASE.values() for z in d.keys())\n",
        "_ZONE_MED = {z: float(np.nanmedian([d.get(z, np.nan) for d in CITY_BASE.values()])) for z in _ZONE_KEYS} if CITY_BASE else {}\n",
        "_GLOBAL_CITYZONE_MED = float(np.nanmedian([v for d in CITY_BASE.values() for v in d.values()])) if CITY_BASE else 0.0\n",
        "\n",
        "# quali derivate servono in serving (train==serve)\n",
        "REQUIRED_DERIVED = globals().get(\"REQUIRED_DERIVED\") or list_required_serving_derivatives()\n",
        "\n",
        "serving_pipe = Pipeline(steps=[\n",
        "    (\"canon_geo\",   GeoCanonizer()),\n",
        "    (\"priors_guard\",PriorsGuard(\n",
        "        city_base=CITY_BASE,\n",
        "        region_index=_region_index,\n",
        "        zone_medians=_ZONE_MED,\n",
        "        global_cityzone_median=_GLOBAL_CITYZONE_MED,\n",
        "    )),\n",
        "    (\"derive\",      EnsureDerivedFeatures(\n",
        "        city_base=CITY_BASE,\n",
        "        region_index=_region_index,\n",
        "        required_cols=list(REQUIRED_DERIVED),\n",
        "    )),\n",
        "    (\"core\",        champion),  # <- trained model/pipeline (TTR log1p/expm1 → output in k€)\n",
        "])\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Paths\n",
        "# ---------------------------------------------------------------------------\n",
        "BASE_OUT = NB_ROOT / \"outputs\" if \"NB_ROOT\" in globals() else Path(\"outputs\")\n",
        "MODEL_DIR = BASE_OUT / \"modeling\"\n",
        "PROP_DIR  = MODEL_DIR / \"property\"\n",
        "FIG_DIR   = MODEL_DIR / \"figures\"\n",
        "ART_DIR   = MODEL_DIR / \"artifacts\"\n",
        "for d in (MODEL_DIR, PROP_DIR, FIG_DIR, ART_DIR):\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Save artifacts: feature_order, pipeline, meta, manifest\n",
        "# ---------------------------------------------------------------------------\n",
        "FEATURES_FILE = PROP_DIR / \"feature_order.json\"\n",
        "FEATURES_FILE.write_text(json.dumps(feature_order, ensure_ascii=False, separators=(\",\", \":\")), encoding=\"utf-8\")\n",
        "feature_order_sha256 = hashlib.sha256(FEATURES_FILE.read_bytes()).hexdigest()\n",
        "\n",
        "pipe_path = PROP_DIR / \"value_regressor_v2.joblib\"\n",
        "joblib.dump(serving_pipe, pipe_path)\n",
        "_loaded = joblib.load(pipe_path)\n",
        "try:\n",
        "    # check that the reloaded serving pipeline has a fitted core\n",
        "    _inner_loaded, *_ = _extract_parts(_loaded.named_steps[\"core\"])\n",
        "    check_is_fitted(_inner_loaded)\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Saved pipeline failed fitted check: {e}\")\n",
        "pipeline_sha = _sha256_file(pipe_path)\n",
        "\n",
        "meta_path = PROP_DIR / \"value_regressor_v2_meta.json\"\n",
        "model_meta = {\n",
        "    \"asset_type\": \"property\",\n",
        "    \"task\": \"value_regressor\",\n",
        "    \"model_version\": \"v2\",\n",
        "    \"model_class\": type(final_est).__name__,\n",
        "    \"trained_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),\n",
        "    \"schema_version\": \"2.0\",\n",
        "    \"pipeline_sha256\": pipeline_sha,\n",
        "    \"feature_order_sha256\": feature_order_sha256,\n",
        "    \"n_features\": int(len(feature_order)),\n",
        "    \"features_categorical\": [c for c in feature_order if c in set(cat_cols)],\n",
        "    \"features_numeric\": [c for c in feature_order if c in set(num_cols)],\n",
        "    \"target_name\": VALUATION_K,\n",
        "    \"unit\": \"k_eur\",\n",
        "    # MUST: esplicita TTR (train in log1p, serve in expm1 → output in k€)\n",
        "    \"ttr\": {\"forward\": \"log1p\", \"inverse\": \"expm1\"},\n",
        "    # MUST: il pipeline include i transformers di serving\n",
        "    \"includes_preprocessing\": True,\n",
        "    # MUST: derivate che il serving garantirà\n",
        "    \"required_derived_features\": list(REQUIRED_DERIVED),\n",
        "    \"feature_order_path\": str(FEATURES_FILE.resolve().as_posix()),\n",
        "    \"pipeline_path\": str(pipe_path.resolve().as_posix()),\n",
        "    # (utile per debugging/telemetria)\n",
        "    \"serving_stages\": [\"GeoCanonizer\", \"PriorsGuard\", \"EnsureDerivedFeatures\", \"core_model\"],\n",
        "}\n",
        "meta_path.write_text(json.dumps(model_meta, ensure_ascii=False, separators=(\",\", \":\")), encoding=\"utf-8\")\n",
        "\n",
        "manifest_path = PROP_DIR / \"training_manifest.json\"\n",
        "paths = {\n",
        "    \"pipeline\": str(pipe_path.resolve().as_posix()),\n",
        "    \"manifest\": str(manifest_path.resolve().as_posix()),\n",
        "    \"feature_order\": str(FEATURES_FILE.resolve().as_posix()),\n",
        "}\n",
        "# include metrics if a dict named `metrics` was produced in the Train & Validation cell\n",
        "metrics_blob = (metrics if \"metrics\" in globals() and isinstance(metrics, dict) else {})\n",
        "\n",
        "manifest = {\n",
        "    \"generated_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),\n",
        "    \"schema_version\": \"2.0\",\n",
        "    \"asset_type\": \"property\",\n",
        "    \"task\": \"value_regressor\",\n",
        "    \"paths\": paths,\n",
        "    \"model_meta\": {\n",
        "        \"model_version\": model_meta[\"model_version\"],\n",
        "        \"model_class\": model_meta[\"model_class\"],\n",
        "        \"pipeline_sha256\": model_meta[\"pipeline_sha256\"],\n",
        "        \"feature_order_sha256\": feature_order_sha256,\n",
        "        # duplicate criticals for BE\n",
        "        \"target_name\": model_meta[\"target_name\"],\n",
        "        \"unit\": model_meta[\"unit\"],\n",
        "        \"includes_preprocessing\": model_meta[\"includes_preprocessing\"],\n",
        "        \"ttr\": model_meta[\"ttr\"],\n",
        "        \"required_derived_features\": model_meta[\"required_derived_features\"],\n",
        "    },\n",
        "    \"metrics\": metrics_blob,\n",
        "    \"feature_order\": feature_order,\n",
        "    \"expected_features\": {\n",
        "        \"categorical\": model_meta[\"features_categorical\"],\n",
        "        \"numeric\": model_meta[\"features_numeric\"],\n",
        "    },\n",
        "}\n",
        "manifest_path.write_text(json.dumps(manifest, ensure_ascii=False, separators=(\",\", \":\")), encoding=\"utf-8\")\n",
        "\n",
        "print(\"✅ Saved serving pipeline (no-refit):\", pipe_path.name)\n",
        "print(\"✅ Saved feature_order:\", FEATURES_FILE.name)\n",
        "print(\"✅ Saved meta:\", meta_path.name)\n",
        "print(\"✅ Saved manifest:\", manifest_path.name)\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Worst-k slice (10%) on final TEST dataframe `dfm` with y_true/y_pred\n",
        "# ---------------------------------------------------------------------------\n",
        "def _worst_k(y_true: np.ndarray, y_pred: np.ndarray, k: float = 0.10) -> dict:\n",
        "    err = np.abs(y_true - y_pred).astype(float)\n",
        "    n = max(1, int(len(err) * k))\n",
        "    top = np.partition(err, -n)[-n:]\n",
        "    return {\n",
        "        \"worst_k\": float(k),\n",
        "        \"worst_k_mean_abs_err\": float(top.mean()) if len(top) else float(\"nan\"),\n",
        "        \"worst_k_max_abs_err\": float(top.max()) if len(top) else float(\"nan\"),\n",
        "        \"worst_k_count\": int(n),\n",
        "    }\n",
        "\n",
        "# Prefer using dfm from the previous “Segment metrics & predictions” cell\n",
        "if \"dfm\" in globals() and isinstance(dfm, pd.DataFrame) and {\"y_true\",\"y_pred\"} <= set(dfm.columns):\n",
        "    y_true_np = dfm[\"y_true\"].to_numpy()\n",
        "    y_pred_np = dfm[\"y_pred\"].to_numpy()\n",
        "else:\n",
        "    # Minimal fallback: recompute natural predictions on TEST (aligned with champion)\n",
        "    if \"df_test\" not in globals():\n",
        "        raise RuntimeError(\"dfm is missing and df_test not available to recompute worst-k.\")\n",
        "    from sklearn.compose import TransformedTargetRegressor\n",
        "    def _ensure_cols(df_part: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
        "        dfp = df_part.copy()\n",
        "        miss = [c for c in cols if c not in dfp.columns]\n",
        "        for c in miss: dfp[c] = np.nan\n",
        "        return dfp[cols]\n",
        "    X_use = _ensure_cols(df_test, feature_order)\n",
        "    if is_ttr:\n",
        "        y_pred_np = np.asarray(champion.predict(X_use), dtype=np.float64)\n",
        "    else:\n",
        "        LOG_CAP = float(TRAIN_CFG.get(\"log_cap_clip\", 12.0)) if \"TRAIN_CFG\" in globals() else 12.0\n",
        "        z = np.asarray(champion.predict(X_use), dtype=np.float64)\n",
        "        z = np.clip(z, -20.0, LOG_CAP)\n",
        "        y_pred_np = np.expm1(z); y_pred_np[y_pred_np < 0] = 0.0\n",
        "    y_true_np = df_test[VALUATION_K].to_numpy(dtype=np.float64, copy=False)\n",
        "\n",
        "wk = _worst_k(y_true_np, y_pred_np, k=0.10)\n",
        "wk_path = MODEL_DIR / \"worst_k.json\"\n",
        "wk_path.write_text(canonical_json_dumps(wk), encoding=\"utf-8\")\n",
        "\n",
        "# update manifest with worst-k\n",
        "m = json.loads(manifest_path.read_text(encoding=\"utf-8\"))\n",
        "m.setdefault(\"metrics\", {})[\"worst_k_10pct\"] = wk\n",
        "manifest_path.write_text(canonical_json_dumps(m), encoding=\"utf-8\")\n",
        "print(\"✅ Saved worst-k:\", wk_path.name)\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Post-training location drift (baseline vs expected OR train vs test)\n",
        "# ---------------------------------------------------------------------------\n",
        "try:\n",
        "    from shared.n03_train_model.metrics import compute_location_drift\n",
        "    _HAS_CLD = True\n",
        "except Exception:\n",
        "    _HAS_CLD = False\n",
        "\n",
        "TRAIN_CFG = globals().get(\"TRAIN_CFG\", {}) or {}\n",
        "TOL = float(TRAIN_CFG.get(\"drift_tolerance\", 0.05))  # default 5%\n",
        "\n",
        "def _norm_weights(d: dict) -> dict[str, float]:\n",
        "    clean = {str(k): float(v) for k, v in (d or {}).items() if pd.api.types.is_number(v) and float(v) >= 0.0}\n",
        "    s = float(sum(clean.values()))\n",
        "    if s <= 0:\n",
        "        return {k: 0.0 for k in clean}\n",
        "    return {k: v / s for k, v in clean.items()}\n",
        "\n",
        "def _empirical_weights(df_like: pd.DataFrame, col: str) -> dict[str, float]:\n",
        "    if not isinstance(df_like, pd.DataFrame) or col not in df_like.columns:\n",
        "        return {}\n",
        "    vc = df_like[col].dropna().astype(str).value_counts(normalize=True)\n",
        "    return {k: float(v) for k, v in vc.items()}\n",
        "\n",
        "def _fallback_drift(df_like: pd.DataFrame, target_w: dict[str, float]) -> dict:\n",
        "    emp = _empirical_weights(df_like, LOCATION)\n",
        "    keys = sorted(set(emp) | set(target_w))\n",
        "    p = np.array([emp.get(k, 0.0) for k in keys], dtype=float)\n",
        "    q = np.array([target_w.get(k, 0.0) for k in keys], dtype=float)\n",
        "    eps = 1e-12\n",
        "    p = np.clip(p, eps, 1.0); q = np.clip(q, eps, 1.0)\n",
        "    p /= p.sum(); q /= q.sum()\n",
        "    m_mid = 0.5 * (p + q)\n",
        "    jsd = float(0.5 * (np.sum(p * (np.log(p) - np.log(m_mid))) + np.sum(q * (np.log(q) - np.log(m_mid)))))\n",
        "    tvd = float(0.5 * np.abs(p - q).sum())\n",
        "    report = {\"method\": \"fallback_jsd_tvd\", \"JSD\": jsd, \"TVD\": tvd, \"per_location\": {}}\n",
        "    for k in keys:\n",
        "        emp_k = emp.get(k, 0.0); tgt_k = target_w.get(k, 0.0)\n",
        "        diff = emp_k - tgt_k\n",
        "        report[\"per_location\"][k] = {\n",
        "            \"target_weight\": tgt_k,\n",
        "            \"empirical_weight\": emp_k,\n",
        "            \"difference\": diff,\n",
        "            \"drifted\": bool(abs(diff) > TOL),\n",
        "            \"ratio\": (emp_k / tgt_k) if tgt_k > 0 else float(\"inf\")\n",
        "        }\n",
        "    return report\n",
        "\n",
        "baseline_cfg = (TRAIN_CFG.get(\"expected_profile\", {}) or {}).get(\"location_distribution\", {}) or None\n",
        "\n",
        "if baseline_cfg and \"df\" in globals():\n",
        "    # Compare entire dataset vs expected baseline\n",
        "    target_w = _norm_weights(baseline_cfg)\n",
        "    drift_result = compute_location_drift(df, target_w, TOL) if _HAS_CLD else _fallback_drift(df, target_w)\n",
        "    out_path = MODEL_DIR / \"location_drift_vs_expected.json\"\n",
        "    out_key  = \"location_drift_vs_expected\"\n",
        "elif \"df_train\" in globals() and \"df_test\" in globals():\n",
        "    # Train vs Test — use TEST distribution as target\n",
        "    tgt = _empirical_weights(df_test, LOCATION)\n",
        "    target_w = _norm_weights(tgt)\n",
        "    drift_result = compute_location_drift(df_train, target_w, TOL) if _HAS_CLD else _fallback_drift(df_train, target_w)\n",
        "    out_path = MODEL_DIR / \"location_drift_train_vs_test.json\"\n",
        "    out_key  = \"location_drift_train_vs_test\"\n",
        "else:\n",
        "    raise RuntimeError(\"No data available to compute post-training location drift.\")\n",
        "\n",
        "out_path.write_text(canonical_json_dumps(drift_result), encoding=\"utf-8\")\n",
        "\n",
        "m = json.loads(manifest_path.read_text(encoding=\"utf-8\"))\n",
        "m.setdefault(\"metrics\", {})[out_key] = drift_result\n",
        "manifest_path.write_text(canonical_json_dumps(m), encoding=\"utf-8\")\n",
        "print(f\"✅ Saved drift metrics → {out_path.name}  (manifest updated)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bbc1d70-e1f4-48ce-ac17-2592e450c444",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### ModelReportRunner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "af220429",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ℹ️ Removed overlaps from categorical (kept numeric priority): ['attic', 'cellar', 'concierge', 'garage', 'has_balcony', 'has_elevator', 'has_garden', 'listing_month', 'owner_occupied', 'parking_spot', 'public_transport_nearby']\n",
            "[manifest] cat=11  num(total)=41  num(raw used here)=30\n",
            "[df] rows=15000  cols=43; usable features in df=41\n",
            "[sample features] ['location', 'region', 'urban_type', 'zone', 'energy_class', 'orientation', 'view', 'condition', 'heating', 'asset_id', 'asset_type', 'has_elevator']\n",
            "Random split → R²(all)=0.9300  MAE=52.95  RMSE=75.85\n",
            "Random split → R²(num_raw)=0.7015  MAE=113.71  RMSE=156.60\n",
            "ΔR² (all − num_raw): +0.2285\n",
            "GSS 5× (group=location) → R²=0.8270±0.0596  MAE=69.09±19.84  RMSE=100.07±26.79\n",
            "\n",
            "Top 10 feature importance (Ordinal+CT):\n",
            "              feature  importance\n",
            "              size_m2    0.499311\n",
            "distance_to_center_km    0.146674\n",
            "               region    0.089758\n",
            "             location    0.087149\n",
            "                 zone    0.060673\n",
            "         energy_class    0.031942\n",
            "           year_built    0.017298\n",
            "            age_years    0.016990\n",
            "            condition    0.012877\n",
            "     confidence_score    0.004349\n",
            "Saved: outputs\\modeling\\report_runner_feature_importance.csv\n",
            "✅ training_manifest signed: 189650b155ab4339a5f40e9fd34d8ab84eff0cf13bea6d74196e687bad72dd45 | created_utc: 2025-10-07T15:06:45Z\n",
            "Encoder: OrdinalEncoder\n",
            "handle_unknown: use_encoded_value\n",
            "  location: ['Bari', 'Bologna', 'Cagliari', 'Catania', 'Florence', 'Genoa', 'Milan', 'Naples', 'Padua', 'Palermo', 'Rome', 'Trieste']\n",
            "  region: ['center', 'north', 'south']\n",
            "  urban_type: ['urban']\n",
            "  zone: ['center', 'periphery', 'semi_center']\n",
            "  energy_class: ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
            "\n",
            "✅ Model Report Runner completed.\n"
          ]
        }
      ],
      "source": [
        "# 10) Model Report Runner — consistent with training (OrdinalEncoder + TTR) + manifest signing + encoder introspection\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "\n",
        "# ------------------------------ helpers -------------------------------------\n",
        "def _rmse(y_true, y_pred):\n",
        "    try:\n",
        "        return mean_squared_error(y_true, y_pred, squared=False)\n",
        "    except TypeError:\n",
        "        # ancient sklearn fallback\n",
        "        return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "def _to_parquet_optional(df: pd.DataFrame, path: Path):\n",
        "    try:\n",
        "        df.to_parquet(path, index=False)\n",
        "    except Exception as e:\n",
        "        print(f\"[info] Parquet export skipped for {path.name}: {e}\")\n",
        "\n",
        "def _unique_preserve_order(seq):\n",
        "    out, seen = [], set()\n",
        "    for x in seq:\n",
        "        x = str(x)\n",
        "        if x not in seen:\n",
        "            seen.add(x); out.append(x)\n",
        "    return out\n",
        "\n",
        "# -------------------------- 0) Load v2 manifest -----------------------------\n",
        "PROP_DIRS = [Path(\"notebooks/outputs/modeling/property\"), Path(\"outputs/modeling/property\")]\n",
        "PROP_DIR = next((d for d in PROP_DIRS if d.exists()), PROP_DIRS[0])\n",
        "MF_PATH = PROP_DIR / \"training_manifest.json\"\n",
        "assert MF_PATH.exists(), f\"training_manifest.json not found at {MF_PATH}\"\n",
        "\n",
        "mf = json.loads(MF_PATH.read_text(encoding=\"utf-8\"))\n",
        "ef = mf.get(\"expected_features\") or {}\n",
        "\n",
        "cat_cols = list(ef.get(\"categorical\") or [])\n",
        "num_cols = list(ef.get(\"numeric\") or [])\n",
        "\n",
        "# Derived features that might appear in manifest but not be present in the raw dataset\n",
        "DERIVED = {\n",
        "    # legacy variants\n",
        "    \"log_size_m2\", \"sqm_per_room\", \"baths_per_100sqm\", \"elev_x_floor\",\n",
        "    # current engineered features (from our FE cell)\n",
        "    \"rooms_per_100sqm\", \"no_elev_high_floor\", \"floor_ratio\",\n",
        "    \"pt_importance\", \"pt_x_periphery\", \"pt_x_semi_center\", \"pt_x_center\",\n",
        "    \"city_zone_prior\", \"region_index_prior\",\n",
        "}\n",
        "# For the runner we only use features that truly exist in the chosen base DF.\n",
        "# (We’re not re-deriving here—this is a quick report runner.)\n",
        "num_raw = [c for c in num_cols if c not in DERIVED]\n",
        "\n",
        "# ------------------ 1) Choose base dataframe (df → df_train → disk) --------\n",
        "if \"df\" in globals() and isinstance(df, pd.DataFrame):\n",
        "    base_df = df.copy()\n",
        "elif \"df_train\" in globals() and isinstance(df_train, pd.DataFrame):\n",
        "    base_df = df_train.copy()\n",
        "else:\n",
        "    CAND = [\n",
        "        Path(\"notebooks/outputs/dataset_generated.parquet\"),\n",
        "        Path(\"notebooks/outputs/dataset_generated.csv\"),\n",
        "        Path(\"outputs/dataset_generated.parquet\"),\n",
        "        Path(\"outputs/dataset_generated.csv\"),\n",
        "    ]\n",
        "    src = next((p for p in CAND if p.exists()), None)\n",
        "    if not src:\n",
        "        raise RuntimeError(\"No dataframe found: define df/df_train or ensure dataset_generated.* exists.\")\n",
        "    base_df = pd.read_parquet(src) if src.suffix.lower() in (\".parquet\", \".pq\") else pd.read_csv(src)\n",
        "\n",
        "# --- Deduplicate duplicated column names (keep first) ---\n",
        "dup_mask = base_df.columns.duplicated(keep=\"first\")\n",
        "if dup_mask.any():\n",
        "    dups = base_df.columns[dup_mask].tolist()\n",
        "    print(f\"⚠️ Dropping duplicated columns (keeping first occurrence): {dups[:12]}{'…' if len(dups)>12 else ''}\")\n",
        "    base_df = base_df.loc[:, ~dup_mask]\n",
        "\n",
        "# Keep only features that actually exist in the DF and make them unique (order preserved)\n",
        "cat_cols = _unique_preserve_order([c for c in cat_cols if c in base_df.columns])\n",
        "num_raw  = _unique_preserve_order([c for c in num_raw  if c in base_df.columns])\n",
        "\n",
        "# Remove overlaps (prefer numeric if a name appears in both)\n",
        "overlap = set(cat_cols) & set(num_raw)\n",
        "if overlap:\n",
        "    cat_cols = [c for c in cat_cols if c not in overlap]\n",
        "    print(f\"ℹ️ Removed overlaps from categorical (kept numeric priority): {sorted(overlap)[:12]}{'…' if len(overlap)>12 else ''}\")\n",
        "\n",
        "ALL = cat_cols + num_raw\n",
        "\n",
        "print(f\"[manifest] cat={len(cat_cols)}  num(total)={len(num_cols)}  num(raw used here)={len(num_raw)}\")\n",
        "print(f\"[df] rows={len(base_df)}  cols={len(base_df.columns)}; usable features in df={len(ALL)}\")\n",
        "print(f\"[sample features] {ALL[:12]}\")\n",
        "\n",
        "assert \"valuation_k\" in base_df.columns, \"Target 'valuation_k' missing in dataframe\"\n",
        "assert len(ALL) > 0, \"No usable features found (cat+num_raw) for the runner.\"\n",
        "\n",
        "df_runner = base_df.copy()\n",
        "\n",
        "# --------- 2) Pipeline consistent with training (Ordinal + imputers) + TTR ---\n",
        "pre_all = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", Pipeline([\n",
        "            (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "            (\"enc\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
        "        ]), cat_cols) if cat_cols else (\"cat\", \"drop\", []),\n",
        "        (\"num\", Pipeline([\n",
        "            (\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "        ]), num_raw) if num_raw else (\"num\", \"drop\", []),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    verbose_feature_names_out=False,\n",
        ")\n",
        "\n",
        "rf_all = RandomForestRegressor(\n",
        "    n_estimators=300, random_state=42, n_jobs=-1, max_depth=None, min_samples_leaf=2\n",
        ")\n",
        "\n",
        "pipe_all = Pipeline([\n",
        "    (\"prep\", pre_all),\n",
        "    (\"ttr\", TransformedTargetRegressor(\n",
        "        regressor=rf_all,\n",
        "        func=np.log1p, inverse_func=np.expm1, check_inverse=False\n",
        "    )),\n",
        "])\n",
        "\n",
        "# ---------------- 3) Simple random split evaluation (raw features) ----------\n",
        "X_all = df_runner[ALL].copy()\n",
        "y_nat = df_runner[\"valuation_k\"].astype(float).to_numpy()\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X_all, y_nat, test_size=0.20, random_state=42)\n",
        "pipe_all.fit(X_tr, y_tr)\n",
        "y_hat = pipe_all.predict(X_te)\n",
        "\n",
        "r2_all  = r2_score(y_te, y_hat)\n",
        "mae_all = mean_absolute_error(y_te, y_hat)\n",
        "rmse_all = _rmse(y_te, y_hat)\n",
        "print(f\"Random split → R²(all)={r2_all:.4f}  MAE={mae_all:.2f}  RMSE={rmse_all:.2f}\")\n",
        "\n",
        "# ---------------- 4) Numeric-only baseline (optional comparison) ------------\n",
        "if num_raw:\n",
        "    pre_num = ColumnTransformer(\n",
        "        [(\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))]), num_raw)],\n",
        "        remainder=\"drop\",\n",
        "        verbose_feature_names_out=False,\n",
        "    )\n",
        "    rf_num = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1, min_samples_leaf=2)\n",
        "    pipe_num = Pipeline([\n",
        "        (\"prep\", pre_num),\n",
        "        (\"ttr\", TransformedTargetRegressor(\n",
        "            regressor=rf_num, func=np.log1p, inverse_func=np.expm1, check_inverse=False\n",
        "        )),\n",
        "    ])\n",
        "    Xn_tr, Xn_te, yn_tr, yn_te = train_test_split(df_runner[num_raw].copy(), y_nat, test_size=0.20, random_state=42)\n",
        "    pipe_num.fit(Xn_tr, yn_tr)\n",
        "    y_num = pipe_num.predict(Xn_te)\n",
        "\n",
        "    r2_num  = r2_score(yn_te, y_num)\n",
        "    mae_num = mean_absolute_error(yn_te, y_num)\n",
        "    rmse_num = _rmse(yn_te, y_num)\n",
        "    print(f\"Random split → R²(num_raw)={r2_num:.4f}  MAE={mae_num:.2f}  RMSE={rmse_num:.2f}\")\n",
        "    print(f\"ΔR² (all − num_raw): {r2_all - r2_num:+.4f}\")\n",
        "\n",
        "# ----------- 5) More robust estimate: GroupShuffleSplit by location ---------\n",
        "if \"location\" in df_runner.columns and len(ALL) > 0:\n",
        "    gss = GroupShuffleSplit(n_splits=5, test_size=0.20, random_state=42)\n",
        "    r2s, maes, rmses = [], [], []\n",
        "    groups = df_runner[\"location\"].astype(str).to_numpy()\n",
        "    for tr_idx, te_idx in gss.split(df_runner[ALL], y_nat, groups=groups):\n",
        "        pipe_all.fit(df_runner.iloc[tr_idx][ALL], y_nat[tr_idx])\n",
        "        y_g = pipe_all.predict(df_runner.iloc[te_idx][ALL])\n",
        "        r2s.append(r2_score(y_nat[te_idx], y_g))\n",
        "        maes.append(mean_absolute_error(y_nat[te_idx], y_g))\n",
        "        rmses.append(_rmse(y_nat[te_idx], y_g))\n",
        "    print(\"GSS 5× (group=location) → \"\n",
        "          f\"R²={np.mean(r2s):.4f}±{np.std(r2s):.4f}  \"\n",
        "          f\"MAE={np.mean(maes):.2f}±{np.std(maes):.2f}  \"\n",
        "          f\"RMSE={np.mean(rmses):.2f}±{np.std(rmses):.2f}\")\n",
        "\n",
        "# ------------- 6) RF feature importance from the TTR-internal RF ------------\n",
        "try:\n",
        "    try:\n",
        "        feat_names = list(pipe_all.named_steps[\"prep\"].get_feature_names_out())\n",
        "    except Exception:\n",
        "        feat_names = [*cat_cols, *num_raw]\n",
        "\n",
        "    rf_fitted = pipe_all.named_steps[\"ttr\"].regressor_\n",
        "    importances = getattr(rf_fitted, \"feature_importances_\", None)\n",
        "    if importances is None:\n",
        "        raise RuntimeError(\"feature_importances_ not available on the regressor.\")\n",
        "\n",
        "    imp = np.asarray(importances, dtype=float)\n",
        "    if len(feat_names) != len(imp):\n",
        "        feat_names = [f\"f{i}\" for i in range(len(imp))]\n",
        "\n",
        "    fi_df = (pd.DataFrame({\"feature\": feat_names, \"importance\": imp})\n",
        "               .sort_values(\"importance\", ascending=False)\n",
        "               .reset_index(drop=True))\n",
        "\n",
        "    print(\"\\nTop 10 feature importance (Ordinal+CT):\")\n",
        "    print(fi_df.head(10).to_string(index=False))\n",
        "\n",
        "    # optional save next to modeling dir\n",
        "    MODEL_DIRS = [Path(\"notebooks/outputs/modeling\"), Path(\"outputs/modeling\")]\n",
        "    MODEL_DIR = next((d for d in MODEL_DIRS if d.exists()), MODEL_DIRS[-1])\n",
        "    fi_csv = MODEL_DIR / \"report_runner_feature_importance.csv\"\n",
        "    fi_parq = MODEL_DIR / \"report_runner_feature_importance.parquet\"\n",
        "    fi_df.to_csv(fi_csv, index=False)\n",
        "    _to_parquet_optional(fi_df, fi_parq)\n",
        "    print(\"Saved:\", fi_csv)\n",
        "except Exception as e:\n",
        "    print(\"Feature importance not available:\", e)\n",
        "\n",
        "# ---------------- Manifest signing (SHA-256 + created_utc) -------------------\n",
        "from shared.common.utils import canonical_json_dumps, sha256_hex\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "manifest_path = MF_PATH\n",
        "m = json.loads(manifest_path.read_text(encoding=\"utf-8\")) if manifest_path.exists() else {}\n",
        "manifest_canon = canonical_json_dumps(m)\n",
        "m[\"manifest_sha256\"] = sha256_hex(manifest_canon)\n",
        "if \"created_utc\" not in m:\n",
        "    m[\"created_utc\"] = datetime.now(timezone.utc).replace(microsecond=0).isoformat().replace(\"+00:00\",\"Z\")\n",
        "\n",
        "manifest_path.write_text(canonical_json_dumps(m), encoding=\"utf-8\")\n",
        "print(\"✅ training_manifest signed:\", m[\"manifest_sha256\"], \"| created_utc:\", m[\"created_utc\"])\n",
        "\n",
        "# -------- Encoder introspection (robust: finds CT even inside serving pipe) --\n",
        "import joblib\n",
        "\n",
        "PIPE_PATHS = [\n",
        "    PROP_DIR / \"value_regressor_v2.joblib\",\n",
        "    Path(\"outputs/modeling/property/value_regressor_v2.joblib\"),\n",
        "]\n",
        "PIPE_PATH = next((p for p in PIPE_PATHS if p.exists()), None)\n",
        "assert PIPE_PATH is not None, \"value_regressor_v2.joblib not found\"\n",
        "pipe_saved = joblib.load(PIPE_PATH)\n",
        "\n",
        "def find_ct(obj):\n",
        "    # direct CT\n",
        "    if isinstance(obj, ColumnTransformer):\n",
        "        return obj\n",
        "    # TTR → inspect regressor/regressor_\n",
        "    if obj.__class__.__name__ == \"TransformedTargetRegressor\":\n",
        "        reg = getattr(obj, \"regressor_\", None) or getattr(obj, \"regressor\", None)\n",
        "        if reg is not None:\n",
        "            ct = find_ct(reg); \n",
        "            if ct is not None: return ct\n",
        "    # Pipeline → walk steps (handles 'core' wrapper)\n",
        "    if isinstance(obj, Pipeline):\n",
        "        for _, st in obj.steps:\n",
        "            ct = find_ct(st)\n",
        "            if ct is not None: return ct\n",
        "    # Named attributes commonly used\n",
        "    for attr in (\"core\",\"prep\",\"preprocessor\",\"feature_processor\",\"processor\"):\n",
        "        if hasattr(obj, attr):\n",
        "            ct = find_ct(getattr(obj, attr))\n",
        "            if ct is not None: return ct\n",
        "    return None\n",
        "\n",
        "prep = find_ct(pipe_saved)\n",
        "assert isinstance(prep, ColumnTransformer), \"No ColumnTransformer found inside the saved pipeline.\"\n",
        "\n",
        "# Build a small fit sample if CT wasn't fitted yet (rare, but safe)\n",
        "def build_fit_sample(prep: ColumnTransformer) -> pd.DataFrame:\n",
        "    # prefer df_train / df; else fallback to dataset on disk\n",
        "    if \"df_train\" in globals() and isinstance(df_train, pd.DataFrame):\n",
        "        base = df_train.copy()\n",
        "    elif \"df\" in globals() and isinstance(df, pd.DataFrame):\n",
        "        base = df.copy()\n",
        "    else:\n",
        "        ds_candidates = [\n",
        "            Path(\"notebooks/outputs/dataset_generated.parquet\"),\n",
        "            Path(\"notebooks/outputs/dataset_generated.csv\"),\n",
        "            Path(\"outputs/dataset_generated.parquet\"),\n",
        "            Path(\"outputs/dataset_generated.csv\"),\n",
        "        ]\n",
        "        ds_path = next((p for p in ds_candidates if p.exists()), None)\n",
        "        assert ds_path is not None, \"Dataset for mini-fit not found\"\n",
        "        base = pd.read_parquet(ds_path) if ds_path.suffix.lower() in {\".parquet\",\".pq\"} else pd.read_csv(ds_path)\n",
        "\n",
        "    req = []\n",
        "    for _, _, cols in getattr(prep, \"transformers\", []):\n",
        "        if cols is None or cols == \"drop\":\n",
        "            continue\n",
        "        req.extend(list(cols) if isinstance(cols,(list,tuple,np.ndarray,pd.Index)) else [cols])\n",
        "    req = list(dict.fromkeys(map(str, req)))\n",
        "\n",
        "    X = base.reindex(columns=req)\n",
        "    if len(X) > 2000:\n",
        "        X = X.sample(n=2000, random_state=42)\n",
        "    return X\n",
        "\n",
        "if not hasattr(prep, \"transformers_\"):\n",
        "    X_small = build_fit_sample(prep)\n",
        "    try:\n",
        "        prep.fit(X_small)\n",
        "    except Exception as e:\n",
        "        print(\"[prep] fit failed on mini sample:\", e)\n",
        "else:\n",
        "    X_small = build_fit_sample(prep)\n",
        "\n",
        "def get_cat_branch_and_encoder(prep: ColumnTransformer):\n",
        "    cat_branch, cat_cols_in = None, None\n",
        "    if hasattr(prep, \"transformers_\"):\n",
        "        for name, trans, cols in prep.transformers_:\n",
        "            if name == \"cat\":\n",
        "                cat_branch = trans\n",
        "                cat_cols_in = list(cols) if isinstance(cols,(list,tuple,np.ndarray,pd.Index)) else ([cols] if cols is not None else [])\n",
        "                break\n",
        "        # fallback: first pipeline with a known encoder\n",
        "        if cat_branch is None:\n",
        "            for _, trans, cols in prep.transformers_:\n",
        "                if isinstance(trans, (Pipeline, OneHotEncoder, OrdinalEncoder)):\n",
        "                    cat_branch = trans\n",
        "                    cat_cols_in = list(cols) if isinstance(cols,(list,tuple,np.ndarray,pd.Index)) else [cols]\n",
        "                    break\n",
        "    enc = None\n",
        "    if isinstance(cat_branch, Pipeline):\n",
        "        for key in (\"enc\",\"encode\",\"ohe\",\"ordinal\",\"encoder\"):\n",
        "            if key in cat_branch.named_steps and isinstance(cat_branch.named_steps[key], (OneHotEncoder, OrdinalEncoder)):\n",
        "                enc = cat_branch.named_steps[key]\n",
        "                break\n",
        "        if enc is None:\n",
        "            for _, st in cat_branch.named_steps.items():\n",
        "                if isinstance(st, (OneHotEncoder, OrdinalEncoder)):\n",
        "                    enc = st; break\n",
        "    elif isinstance(cat_branch, (OneHotEncoder, OrdinalEncoder)):\n",
        "        enc = cat_branch\n",
        "    return cat_branch, enc, (cat_cols_in or [])\n",
        "\n",
        "cat_branch, enc, cat_cols_in = get_cat_branch_and_encoder(prep)\n",
        "assert enc is not None, \"Categorical encoder not found in the 'cat' branch.\"\n",
        "\n",
        "def derive_categories_from_data(X: pd.DataFrame, cols: list[str]) -> dict[str, list]:\n",
        "    out = {}\n",
        "    for c in cols:\n",
        "        if c in X.columns:\n",
        "            vals = pd.Series(X[c]).astype(\"object\")\n",
        "            out[c] = sorted(pd.unique(vals[vals.notna()]).tolist())\n",
        "        else:\n",
        "            out[c] = []\n",
        "    return out\n",
        "\n",
        "if isinstance(enc, OneHotEncoder):\n",
        "    if hasattr(enc, \"categories_\"):\n",
        "        cats_map = { (cat_cols_in[i] if i < len(cat_cols_in) else f\"cat_{i}\"): list(c)\n",
        "                     for i, c in enumerate(enc.categories_) }\n",
        "    else:\n",
        "        cats_map = derive_categories_from_data(X_small, cat_cols_in)\n",
        "    print(\"Encoder: OneHotEncoder\")\n",
        "    print(\"handle_unknown:\", getattr(enc, \"handle_unknown\", None))\n",
        "    # light preview\n",
        "    for probe in (\"region\",\"Region\",\"zone\",\"Zone\"):\n",
        "        if probe in cats_map:\n",
        "            print(f\"categories — {probe} (sample):\", cats_map[probe][:12]); break\n",
        "\n",
        "elif isinstance(enc, OrdinalEncoder):\n",
        "    if hasattr(enc, \"categories_\"):\n",
        "        cats = enc.categories_\n",
        "        names = cat_cols_in or [f\"cat_{i}\" for i in range(len(cats))]\n",
        "        print(\"Encoder: OrdinalEncoder\")\n",
        "        print(\"handle_unknown:\", getattr(enc, \"handle_unknown\", None))\n",
        "        for i, cat in enumerate(cats[:min(5, len(cats))]):\n",
        "            cname = names[i] if i < len(names) else f\"cat_{i}\"\n",
        "            print(f\"  {cname}: {list(cat)[:12]}\")\n",
        "    else:\n",
        "        cats_map = derive_categories_from_data(X_small, cat_cols_in)\n",
        "        print(\"Encoder: OrdinalEncoder (fallback categories inferred from data)\")\n",
        "        for k, v in list(cats_map.items())[:5]:\n",
        "            print(f\"  {k}: {v[:12]}\")\n",
        "\n",
        "print(\"\\n✅ Model Report Runner completed.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
