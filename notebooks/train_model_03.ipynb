{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8cc119a1-28bd-47af-8a5d-38204c005bdf",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Imports & Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ba165936-c2d9-4dd5-b388-62912b63f069",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-08-21 08:48:10,038] WARNING model_trainer: dataset_config.yaml NON trovato: uso fallback di default.\n",
            "[2025-08-21 08:48:10,043] INFO model_trainer: Setup OK | seed=42 | outputs_dir=c:/Users/Utente/Desktop/Projects/ai_oracle_rwa/notebooks/outputs\n"
          ]
        }
      ],
      "source": [
        "# 00) Import & setup — esecuzione da dentro `notebooks/`\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Siamo già in notebooks/, quindi `shared/` è un pacchetto sibling\n",
        "NB_ROOT = Path.cwd()                 # .../notebooks\n",
        "PROJ_ROOT = NB_ROOT.parent           # project root\n",
        "\n",
        "if str(NB_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(NB_ROOT))\n",
        "if str(PROJ_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJ_ROOT))\n",
        "\n",
        "# ── shared imports (coerenti con i tuoi notebook)\n",
        "from notebooks.shared.common.utils import (\n",
        "    NumpyJSONEncoder,\n",
        "    optimize_dtypes,\n",
        "    log_basic_diagnostics,\n",
        "    set_global_seed,\n",
        ")\n",
        "from notebooks.shared.common.config import load_config, configure_logger\n",
        "from notebooks.shared.common.constants import (\n",
        "    VALUATION_K,\n",
        "    LAST_VERIFIED_TS, PREDICTION_TS, LAG_HOURS,\n",
        "    CONDITION_SCORE, RISK_SCORE,\n",
        ")\n",
        "\n",
        "# sklearn (alcuni import usati in celle successive)\n",
        "from sklearn.ensemble import RandomForestRegressor  # noqa: F401\n",
        "\n",
        "# ── Logger\n",
        "LOG_LEVEL = os.getenv(\"NB_LOG_LEVEL\", \"INFO\")\n",
        "logger = configure_logger(name=\"model_trainer\", level=LOG_LEVEL)\n",
        "\n",
        "# ── Config (opzionale): se non esiste, lavoriamo in fallback senza YAML\n",
        "CFG_PATH = NB_ROOT / \"dataset_config.yaml\"\n",
        "if CFG_PATH.exists():\n",
        "    CONFIG = load_config(str(CFG_PATH))\n",
        "    logger.info(\"Config YAML caricato: %s\", CFG_PATH.as_posix())\n",
        "else:\n",
        "    CONFIG = {}\n",
        "    logger.warning(\"dataset_config.yaml NON trovato: uso fallback di default.\")\n",
        "\n",
        "TRAIN_CFG = CONFIG.get(\"training\", {}) or {}\n",
        "\n",
        "# ── Seed globale\n",
        "SEED = int(TRAIN_CFG.get(\"seed\", CONFIG.get(\"seed\", 42)))\n",
        "set_global_seed(SEED)\n",
        "\n",
        "# ── Cartelle output (da usare SEMPRE relative a `notebooks/`)\n",
        "BASE_OUT = NB_ROOT / \"outputs\"                  # <- 👈 corretto: siamo già in notebooks/\n",
        "MODEL_DIR = BASE_OUT / \"modeling\"\n",
        "FIG_DIR   = MODEL_DIR / \"figures\"\n",
        "ART_DIR   = MODEL_DIR / \"artifacts\"\n",
        "PROP_DIR  = MODEL_DIR / \"property\"              # cartella “servita” dal backend/registry\n",
        "\n",
        "for d in (BASE_OUT, MODEL_DIR, FIG_DIR, ART_DIR, PROP_DIR):\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ── Dataset path canonico (override da YAML se presente)\n",
        "DATASET_PATH = Path(TRAIN_CFG.get(\"dataset_path\", BASE_OUT / \"dataset_generated.csv\"))\n",
        "\n",
        "# QoL\n",
        "pd.set_option(\"display.max_columns\", 200)\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "logger.info(\"Setup OK | seed=%s | outputs_dir=%s\", SEED, BASE_OUT.as_posix())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c0f3f96-b7be-48ff-a9b2-2c9b9569cb63",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "38892f35-5a7d-488c-83d2-a99bca1e6fee",
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-08-21 08:48:10,114] INFO model_trainer: Manifest nb01 trovato: c:/Users/Utente/Desktop/Projects/ai_oracle_rwa/notebooks/outputs/snapshots/manifest_20250821T064041Z.json\n",
            "[2025-08-21 08:48:10,116] INFO model_trainer: 📄 Caricamento dataset da: outputs/dataset_generated.csv\n",
            "[2025-08-21 08:48:10,504] INFO model_trainer: ✅ Dtypes optimized: 14.95 MB → 12.80 MB (−2.15 MB, 14.4%)\n",
            "[2025-08-21 08:48:10,510] INFO model_trainer: [UTILS] Distribuzione per location:\n",
            "location\n",
            "Milan       3017\n",
            "Rome        2700\n",
            "Turin       1214\n",
            "Naples      1190\n",
            "Bologna      886\n",
            "Genoa        773\n",
            "Florence     770\n",
            "Palermo      767\n",
            "Venice       596\n",
            "Bari         593\n",
            "Verona       591\n",
            "Padua        581\n",
            "Catania      444\n",
            "Cagliari     442\n",
            "Trieste      436\n",
            "[2025-08-21 08:48:10,512] INFO model_trainer: [UTILS] Prezzo min: 47.39k€\n",
            "[2025-08-21 08:48:10,514] INFO model_trainer: [UTILS] Prezzo max: 2304.21k€\n",
            "[2025-08-21 08:48:10,515] INFO model_trainer: [UTILS] Prezzo medio: 472.44k€\n",
            "[2025-08-21 08:48:10,519] INFO model_trainer: [UTILS] Corr size_m2 vs valuation_k: 0.630\n",
            "[2025-08-21 08:48:10,569] INFO model_trainer: ✅ Schema validation passed\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "895"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 01) Carica dataset dal manifest di nb01 (robusto) + ottimizza + valida\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from notebooks.shared.common.utils import canonical_json_dumps\n",
        "from notebooks.shared.common.sanity_checks import validate_dataset\n",
        "\n",
        "# --- helper: risolvi path relativo verso posizioni note ---\n",
        "def _resolve_path(p: str | Path) -> Path | None:\n",
        "    cand = Path(p)\n",
        "    if cand.exists():\n",
        "        return cand\n",
        "    # se è relativo, prova sotto base output e progetto\n",
        "    for base in [BASE_OUT, NB_ROOT, PROJ_ROOT]:\n",
        "        q = (base / str(p)).resolve()\n",
        "        if q.exists():\n",
        "            return q\n",
        "    return None\n",
        "\n",
        "# 1) Trova ultimo manifest di nb01\n",
        "snap_dir = BASE_OUT / \"snapshots\"\n",
        "snap_dir.mkdir(parents=True, exist_ok=True)\n",
        "manifests = sorted(snap_dir.glob(\"manifest_*.json\"))\n",
        "manifest01 = None\n",
        "if manifests:\n",
        "    try:\n",
        "        manifest01 = json.loads(manifests[-1].read_text(encoding=\"utf-8\"))\n",
        "        logger.info(\"Manifest nb01 trovato: %s\", manifests[-1].as_posix())\n",
        "    except Exception as e:\n",
        "        logger.warning(\"Impossibile leggere il manifest più recente: %s\", e)\n",
        "\n",
        "# 2) Determina data_path dal manifest (supporta varie chiavi)\n",
        "data_path: Path | None = None\n",
        "if isinstance(manifest01, dict):\n",
        "    paths = (manifest01.get(\"paths\") or {})  # type: ignore\n",
        "    for k in (\"dataset_path\", \"dataset\", \"output_path\"):\n",
        "        p = paths.get(k)\n",
        "        if p:\n",
        "            rp = _resolve_path(p)\n",
        "            if rp:\n",
        "                data_path = rp\n",
        "                break\n",
        "\n",
        "# 3) Fallback: usa DATASET_PATH (da Cella 01) o cerca in BASE_OUT\n",
        "if data_path is None or not data_path.exists():\n",
        "    candidates = [\n",
        "        Path(DATASET_PATH) if isinstance(DATASET_PATH, (str, Path)) else None,\n",
        "        BASE_OUT / \"dataset_generated.parquet\",\n",
        "        BASE_OUT / \"dataset_generated.csv\",\n",
        "    ]\n",
        "    # estendi con eventuali file simili\n",
        "    candidates += sorted(BASE_OUT.glob(\"dataset_*.parquet\"))\n",
        "    candidates += sorted(BASE_OUT.glob(\"dataset_*.csv\"))\n",
        "    data_path = next((c for c in candidates if c and c.exists()), None)\n",
        "\n",
        "if not data_path or not data_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        \"Dataset non trovato. Verifica manifest nb01 in notebooks/outputs/snapshots \"\n",
        "        \"oppure che esista notebooks/outputs/dataset_generated.(csv|parquet).\"\n",
        "    )\n",
        "\n",
        "logger.info(\"📄 Caricamento dataset da: %s\", data_path.as_posix())\n",
        "\n",
        "# 4) Caricamento parquet/csv\n",
        "if data_path.suffix.lower() in {\".parquet\", \".pq\"}:\n",
        "    df = pd.read_parquet(data_path)\n",
        "else:\n",
        "    df = pd.read_csv(data_path)\n",
        "\n",
        "# 5) Ottimizzazione dtypes (log risparmio)\n",
        "mem_before = df.memory_usage(deep=True).sum() / 1024**2\n",
        "df = optimize_dtypes(df)\n",
        "mem_after = df.memory_usage(deep=True).sum() / 1024**2\n",
        "logger.info(\n",
        "    \"✅ Dtypes optimized: %.2f MB → %.2f MB (−%.2f MB, %.1f%%)\",\n",
        "    mem_before, mem_after, mem_before - mem_after,\n",
        "    0.0 if mem_before == 0 else (mem_before - mem_after) / mem_before * 100.0\n",
        ")\n",
        "\n",
        "# 6) Diagnostica rapida\n",
        "log_basic_diagnostics(df, logger)\n",
        "\n",
        "# 7) Validazione schema (asset_type da config, come nb01)\n",
        "asset_type = str(CONFIG.get(\"generation\", {}).get(\"asset_type\", \"property\"))\n",
        "try:\n",
        "    val_report = validate_dataset(df, asset_type=asset_type, raise_on_failure=True)\n",
        "    logger.info(\"✅ Schema validation passed\")\n",
        "except Exception as e:\n",
        "    logger.warning(\"Schema validation warning: %s\", e)\n",
        "    val_report = {\"overall_passed\": False, \"error\": str(e)}\n",
        "\n",
        "# 8) Persistenza report vicino ai modeling outputs\n",
        "(MODEL_DIR / \"validation_nb03.json\").write_text(\n",
        "    canonical_json_dumps(val_report),\n",
        "    encoding=\"utf-8\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f92a3f27",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-08-21 08:48:10,616] WARNING model_trainer: 🔴 RIMOZIONE FEATURES LEAKY: ['price_per_sqm', 'strongly_incoherent']\n",
            "[2025-08-21 08:48:10,625] INFO model_trainer: ✅ Dataset pulito: 43 colonne rimanenti\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape dopo pulizia: (15000, 43)\n",
            "Colonne numeriche: ['valuation_k', 'listing_month', 'size_m2', 'rooms', 'bathrooms', 'year_built', 'age_years', 'floor', 'building_floors', 'is_top_floor', 'is_ground_floor', 'has_elevator', 'has_garden', 'has_balcony', 'garage', 'owner_occupied', 'public_transport_nearby', 'distance_to_center_km', 'parking_spot', 'cellar', 'attic', 'concierge', 'humidity_level', 'temperature_avg', 'noise_level', 'air_quality_index', 'condition_score', 'risk_score', 'luxury_score', 'env_score', 'confidence_score']\n"
          ]
        }
      ],
      "source": [
        "# 02) PULIZIA LEAKAGE IMMEDIATA (robusta)\n",
        "from __future__ import annotations\n",
        "\n",
        "import re\n",
        "\n",
        "from notebooks.shared.common.constants import VALUATION_K\n",
        "from notebooks.shared.common.constants import PRICE_PER_SQM, PRICE_PER_SQM_CAPPED_VIOLATED\n",
        "from notebooks.shared.n03_train_model.preprocessing import ML_LEAKY_FEATURES as _ML_LEAKY\n",
        "\n",
        "# --- 1) Rimozione esplicita (case-insensitive) ---\n",
        "explicit_leaky = {\n",
        "    PRICE_PER_SQM,\n",
        "    \"price_per_sqm\",\n",
        "    \"price_per_sqm_vs_region_avg\",\n",
        "    \"price_per_sqm_capped\",\n",
        "    \"valuation_k_log\",\n",
        "    PRICE_PER_SQM_CAPPED_VIOLATED,\n",
        "    \"strongly_incoherent\",\n",
        "    \"valuation_k_decile\",\n",
        "    \"valuation_rank\",\n",
        "    \"is_top_valuation\",\n",
        "}\n",
        "explicit_leaky |= set(map(str, _ML_LEAKY))\n",
        "\n",
        "# mappa lowercase -> originale\n",
        "lower_map = {c.lower(): c for c in df.columns}\n",
        "present_explicit = [lower_map[n.lower()] for n in explicit_leaky if n and n.lower() in lower_map]\n",
        "\n",
        "# --- 2) Rimozione pattern-based (regex, case-insensitive) ---\n",
        "regex_patterns = [\n",
        "    r\"price_per_sqm\",       # qualunque col contenga price_per_sqm\n",
        "    r\"^valuation_k_.+$\",    # derivate del target\n",
        "]\n",
        "present_regex = []\n",
        "for col in df.columns:\n",
        "    if col == VALUATION_K:\n",
        "        continue\n",
        "    if any(re.search(pat, col, flags=re.IGNORECASE) for pat in regex_patterns):\n",
        "        present_regex.append(col)\n",
        "\n",
        "# --- 3) Applica rimozione ---\n",
        "to_drop = sorted(set(present_explicit) | set(present_regex))\n",
        "if to_drop:\n",
        "    logger.warning(\"🔴 RIMOZIONE FEATURES LEAKY: %s\", to_drop)\n",
        "    df.drop(columns=to_drop, inplace=True, errors=\"ignore\")\n",
        "    logger.info(\"✅ Dataset pulito: %d colonne rimanenti\", df.shape[1])\n",
        "else:\n",
        "    logger.info(\"✅ Nessuna feature leaky trovata nel dataset\")\n",
        "\n",
        "# --- 4) Verifiche finali ---\n",
        "assert not any(\"price_per_sqm\" in c.lower() for c in df.columns), \"ERRORE: colonne 'price_per_sqm*' ancora presenti!\"\n",
        "assert not any(c.lower().startswith(\"valuation_k_\") for c in df.columns if c.lower() != VALUATION_K.lower()), \\\n",
        "       \"ERRORE: derivate 'valuation_k_*' ancora presenti!\"\n",
        "\n",
        "# Debug essenziale\n",
        "logger.debug(\"Colonne rimanenti: %s\", list(df.columns))\n",
        "print(f\"Shape dopo pulizia: {df.shape}\")\n",
        "print(f\"Colonne numeriche: {df.select_dtypes(include='number').columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c08c1d08-0b0e-4ed0-bbd9-94c27b0909e3",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Preparation (derivations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8e773326-72f1-480d-8473-b685af965df1",
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-08-21 08:48:10,657] WARNING model_trainer: Impossibile derivare lag_hours: mancano last_verified_ts o prediction_ts\n",
            "[2025-08-21 08:48:10,702] INFO model_trainer: Creato feature derivata: condition_minus_risk\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "DATASET PULITO - PRIME 3 RIGHE\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>asset_id</th>\n",
              "      <th>asset_type</th>\n",
              "      <th>location</th>\n",
              "      <th>valuation_k</th>\n",
              "      <th>last_verified_ts</th>\n",
              "      <th>listing_month</th>\n",
              "      <th>region</th>\n",
              "      <th>urban_type</th>\n",
              "      <th>zone</th>\n",
              "      <th>size_m2</th>\n",
              "      <th>rooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>year_built</th>\n",
              "      <th>age_years</th>\n",
              "      <th>floor</th>\n",
              "      <th>building_floors</th>\n",
              "      <th>is_top_floor</th>\n",
              "      <th>is_ground_floor</th>\n",
              "      <th>has_elevator</th>\n",
              "      <th>has_garden</th>\n",
              "      <th>has_balcony</th>\n",
              "      <th>garage</th>\n",
              "      <th>owner_occupied</th>\n",
              "      <th>public_transport_nearby</th>\n",
              "      <th>distance_to_center_km</th>\n",
              "      <th>parking_spot</th>\n",
              "      <th>cellar</th>\n",
              "      <th>attic</th>\n",
              "      <th>concierge</th>\n",
              "      <th>energy_class</th>\n",
              "      <th>humidity_level</th>\n",
              "      <th>temperature_avg</th>\n",
              "      <th>noise_level</th>\n",
              "      <th>air_quality_index</th>\n",
              "      <th>condition_score</th>\n",
              "      <th>risk_score</th>\n",
              "      <th>luxury_score</th>\n",
              "      <th>env_score</th>\n",
              "      <th>orientation</th>\n",
              "      <th>view</th>\n",
              "      <th>condition</th>\n",
              "      <th>heating</th>\n",
              "      <th>confidence_score</th>\n",
              "      <th>condition_minus_risk</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>asset_000000</td>\n",
              "      <td>property</td>\n",
              "      <td>Venice</td>\n",
              "      <td>251.649994</td>\n",
              "      <td>2025-08-21 06:40:21+00:00</td>\n",
              "      <td>8</td>\n",
              "      <td>north</td>\n",
              "      <td>urban</td>\n",
              "      <td>semi_center</td>\n",
              "      <td>77</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1957</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2.19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>E</td>\n",
              "      <td>47.700001</td>\n",
              "      <td>14.0</td>\n",
              "      <td>45</td>\n",
              "      <td>50</td>\n",
              "      <td>0.847</td>\n",
              "      <td>0.148</td>\n",
              "      <td>0.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>South-East</td>\n",
              "      <td>garden</td>\n",
              "      <td>good</td>\n",
              "      <td>centralized</td>\n",
              "      <td>0.7435</td>\n",
              "      <td>0.699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>asset_000001</td>\n",
              "      <td>property</td>\n",
              "      <td>Turin</td>\n",
              "      <td>329.179993</td>\n",
              "      <td>2025-08-21 06:40:21+00:00</td>\n",
              "      <td>8</td>\n",
              "      <td>north</td>\n",
              "      <td>urban</td>\n",
              "      <td>semi_center</td>\n",
              "      <td>88</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2020</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2.50</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>E</td>\n",
              "      <td>53.500000</td>\n",
              "      <td>23.5</td>\n",
              "      <td>71</td>\n",
              "      <td>86</td>\n",
              "      <td>0.818</td>\n",
              "      <td>0.185</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.3</td>\n",
              "      <td>South</td>\n",
              "      <td>street</td>\n",
              "      <td>renovated</td>\n",
              "      <td>autonomous</td>\n",
              "      <td>0.5290</td>\n",
              "      <td>0.633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>asset_000002</td>\n",
              "      <td>property</td>\n",
              "      <td>Bari</td>\n",
              "      <td>167.149994</td>\n",
              "      <td>2025-08-21 06:40:21+00:00</td>\n",
              "      <td>8</td>\n",
              "      <td>south</td>\n",
              "      <td>urban</td>\n",
              "      <td>semi_center</td>\n",
              "      <td>91</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1957</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.53</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>G</td>\n",
              "      <td>63.900002</td>\n",
              "      <td>23.0</td>\n",
              "      <td>78</td>\n",
              "      <td>143</td>\n",
              "      <td>0.701</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>West</td>\n",
              "      <td>park</td>\n",
              "      <td>renovated</td>\n",
              "      <td>centralized</td>\n",
              "      <td>0.4105</td>\n",
              "      <td>0.381</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       asset_id asset_type location  valuation_k           last_verified_ts  \\\n",
              "0  asset_000000   property   Venice   251.649994  2025-08-21 06:40:21+00:00   \n",
              "1  asset_000001   property    Turin   329.179993  2025-08-21 06:40:21+00:00   \n",
              "2  asset_000002   property     Bari   167.149994  2025-08-21 06:40:21+00:00   \n",
              "\n",
              "   listing_month region urban_type         zone  size_m2  rooms  bathrooms  \\\n",
              "0              8  north      urban  semi_center       77      3          3   \n",
              "1              8  north      urban  semi_center       88      2          3   \n",
              "2              8  south      urban  semi_center       91      6          1   \n",
              "\n",
              "   year_built  age_years  floor  building_floors  is_top_floor  \\\n",
              "0        1957         68      0                3             0   \n",
              "1        2020          5      3                5             0   \n",
              "2        1957         68      0                5             0   \n",
              "\n",
              "   is_ground_floor  has_elevator  has_garden  has_balcony  garage  \\\n",
              "0                1             0           0            1       1   \n",
              "1                0             1           0            0       1   \n",
              "2                1             1           0            0       1   \n",
              "\n",
              "   owner_occupied  public_transport_nearby  distance_to_center_km  \\\n",
              "0               1                        0                   2.19   \n",
              "1               1                        1                   2.50   \n",
              "2               1                        1                   1.53   \n",
              "\n",
              "   parking_spot  cellar  attic  concierge energy_class  humidity_level  \\\n",
              "0             0       0      0          0            E       47.700001   \n",
              "1             0       0      0          0            E       53.500000   \n",
              "2             0       0      0          0            G       63.900002   \n",
              "\n",
              "   temperature_avg  noise_level  air_quality_index  condition_score  \\\n",
              "0             14.0           45                 50            0.847   \n",
              "1             23.5           71                 86            0.818   \n",
              "2             23.0           78                143            0.701   \n",
              "\n",
              "   risk_score  luxury_score  env_score orientation    view  condition  \\\n",
              "0       0.148           0.4        1.0  South-East  garden       good   \n",
              "1       0.185           0.2        0.3       South  street  renovated   \n",
              "2        0.32           0.2        0.0        West    park  renovated   \n",
              "\n",
              "       heating  confidence_score  condition_minus_risk  \n",
              "0  centralized            0.7435                 0.699  \n",
              "1   autonomous            0.5290                 0.633  \n",
              "2  centralized            0.4105                 0.381  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Shape: (15000, 44)\n",
            "Target (valuation_k) range: [47.39, 2304.21]\n"
          ]
        }
      ],
      "source": [
        "# 04) FEATURE DERIVATE\n",
        "from __future__ import annotations\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    from notebooks.shared.common.constants import PRICE_PER_SQM  # type: ignore\n",
        "except Exception:\n",
        "    PRICE_PER_SQM = \"price_per_sqm\"\n",
        "\n",
        "# 1) LAG_HOURS se mancante (da timestamp UTC)\n",
        "if (LAG_HOURS not in df.columns) and ({LAST_VERIFIED_TS, PREDICTION_TS} <= set(df.columns)):\n",
        "    # parse tollerante (accetta naive → le rende UTC)\n",
        "    df[LAST_VERIFIED_TS] = pd.to_datetime(df[LAST_VERIFIED_TS], utc=True, errors=\"coerce\")\n",
        "    df[PREDICTION_TS]   = pd.to_datetime(df[PREDICTION_TS],   utc=True, errors=\"coerce\")\n",
        "\n",
        "    lag = (df[PREDICTION_TS] - df[LAST_VERIFIED_TS]).dt.total_seconds().div(3600)\n",
        "    # valori negativi o assurdi → NaN; poi cast a float32\n",
        "    lag = lag.where(lag >= 0, other=pd.NA)\n",
        "    df[LAG_HOURS] = lag.astype(\"Float32\")\n",
        "    logger.info(\"Creato %s da %s & %s\", LAG_HOURS, LAST_VERIFIED_TS, PREDICTION_TS)\n",
        "\n",
        "elif LAG_HOURS in df.columns:\n",
        "    df[LAG_HOURS] = pd.to_numeric(df[LAG_HOURS], errors=\"coerce\").astype(\"Float32\")\n",
        "else:\n",
        "    logger.warning(\"Impossibile derivare %s: mancano %s o %s\", LAG_HOURS, LAST_VERIFIED_TS, PREDICTION_TS)\n",
        "\n",
        "# 2) condition_minus_risk (utile e non-leaky)\n",
        "if (CONDITION_SCORE in df.columns) and (RISK_SCORE in df.columns):\n",
        "    df[CONDITION_SCORE] = pd.to_numeric(df[CONDITION_SCORE], errors=\"coerce\").astype(\"Float32\")\n",
        "    df[RISK_SCORE]      = pd.to_numeric(df[RISK_SCORE],      errors=\"coerce\").astype(\"Float32\")\n",
        "    df[\"condition_minus_risk\"] = (df[CONDITION_SCORE] - df[RISK_SCORE]).astype(\"Float32\")\n",
        "    logger.info(\"Creato feature derivata: condition_minus_risk\")\n",
        "else:\n",
        "    logger.debug(\"condition_minus_risk non creato: mancano %s o %s\", CONDITION_SCORE, RISK_SCORE)\n",
        "\n",
        "if (\"listing_month\" not in df.columns) and (PREDICTION_TS in df.columns):\n",
        "    if pd.api.types.is_datetime64_any_dtype(df[PREDICTION_TS]) or pd.api.types.is_object_dtype(df[PREDICTION_TS]):\n",
        "        try:\n",
        "            ts = pd.to_datetime(df[PREDICTION_TS], utc=True, errors=\"coerce\")\n",
        "            df[\"listing_month\"] = ts.dt.month.astype(\"Int16\")\n",
        "            logger.info(\"Creato listing_month da %s\", PREDICTION_TS)\n",
        "        except Exception:\n",
        "            logger.debug(\"listing_month non creato (parse fallita)\")\n",
        "\n",
        "# 4) Target: check + coercizione numerica\n",
        "if VALUATION_K not in df.columns:\n",
        "    raise ValueError(f\"{VALUATION_K} mancante: impossibile allenare.\")\n",
        "df[VALUATION_K] = pd.to_numeric(df[VALUATION_K], errors=\"coerce\").astype(\"Float32\")\n",
        "\n",
        "# 5) Verifica finale assenza leakage\n",
        "assert not any(\"price_per_sqm\" in c.lower() for c in df.columns), \"LEAKAGE: colonne 'price_per_sqm*' presenti!\"\n",
        "assert not any(c.lower().startswith(\"valuation_k_\") for c in df.columns if c != VALUATION_K), \\\n",
        "       \"LEAKAGE: derivate 'valuation_k_*' ancora presenti!\"\n",
        "\n",
        "# 6) Snapshot\n",
        "print(\"=\" * 60)\n",
        "print(\"DATASET PULITO - PRIME 3 RIGHE\")\n",
        "print(\"=\" * 60)\n",
        "display(df.head(3))\n",
        "print(f\"\\nShape: {df.shape}\")\n",
        "print(f\"Target (valuation_k) range: [{df[VALUATION_K].min():.2f}, {df[VALUATION_K].max():.2f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fca06807",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-08-21 08:48:10,982] INFO model_trainer: Correlations saved: c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\artifacts\\target_correlations.json (JSON) | c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\artifacts\\target_correlations.csv (CSV)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "TOP 15 CORRELAZIONI POSITIVE (Pearson) CON IL TARGET\n",
            "============================================================\n",
            "size_m2                 0.630403\n",
            "luxury_score            0.379311\n",
            "confidence_score        0.280364\n",
            "concierge               0.195561\n",
            "year_built              0.178468\n",
            "condition_score         0.110013\n",
            "condition_minus_risk    0.109104\n",
            "floor                   0.051119\n",
            "is_top_floor            0.049285\n",
            "garage                  0.048877\n",
            "has_balcony             0.045449\n",
            "has_garden              0.039074\n",
            "attic                   0.019873\n",
            "air_quality_index       0.007599\n",
            "env_score               0.002760\n",
            "Name: valuation_k, dtype: float64\n",
            "\n",
            "============================================================\n",
            "TOP 15 CORRELAZIONI NEGATIVE (Pearson) CON IL TARGET\n",
            "============================================================\n",
            "humidity_level           0.000525\n",
            "bathrooms               -0.000900\n",
            "noise_level             -0.001052\n",
            "has_elevator            -0.002555\n",
            "owner_occupied          -0.003677\n",
            "rooms                   -0.006291\n",
            "temperature_avg         -0.006449\n",
            "cellar                  -0.008075\n",
            "building_floors         -0.012635\n",
            "parking_spot            -0.031793\n",
            "is_ground_floor         -0.061774\n",
            "risk_score              -0.106360\n",
            "age_years               -0.178468\n",
            "distance_to_center_km   -0.354913\n",
            "listing_month                 NaN\n",
            "Name: valuation_k, dtype: float64\n",
            "\n",
            "✅ Nessuna correlazione sospetta (>|r| > 0.95 )\n",
            "\n",
            "============================================================\n",
            "TOP 10 CORRELAZIONI SPEARMAN (assolute) CON IL TARGET\n",
            "============================================================\n",
            "size_m2                  0.698423\n",
            "distance_to_center_km   -0.425904\n",
            "luxury_score             0.398142\n",
            "confidence_score         0.286909\n",
            "concierge                0.223665\n",
            "year_built               0.167587\n",
            "age_years               -0.167587\n",
            "condition_score          0.111171\n",
            "condition_minus_risk     0.109649\n",
            "risk_score              -0.105991\n",
            "Name: valuation_k, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# 05) ANALISI CORRELAZIONI CON IL TARGET (no-leakage, robusta)\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "SUSPICIOUS_THR = 0.95\n",
        "corr_json_path = ART_DIR / \"target_correlations.json\"\n",
        "corr_csv_path  = ART_DIR / \"target_correlations.csv\"\n",
        "\n",
        "# 0) Safety: il target deve essere numerico (coercizzato in cella 04)\n",
        "if VALUATION_K not in df.columns:\n",
        "    logger.error(\"Target %s non trovato nel dataset\", VALUATION_K)\n",
        "    corr_json_path.write_text(canonical_json_dumps({\"error\": \"target missing\"}), encoding=\"utf-8\")\n",
        "else:\n",
        "    # 1) Colonne numeriche (post-pulizia) + sanity\n",
        "    numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
        "    if VALUATION_K not in numeric_cols:\n",
        "        logger.warning(\"Il target non risulta numerico: provo a forzare il cast.\")\n",
        "        df[VALUATION_K] = pd.to_numeric(df[VALUATION_K], errors=\"coerce\")\n",
        "        if not pd.api.types.is_numeric_dtype(df[VALUATION_K]):\n",
        "            raise TypeError(f\"{VALUATION_K} non numerico; impossibile calcolare correlazioni.\")\n",
        "\n",
        "    if len(numeric_cols) < 2:\n",
        "        logger.warning(\"Poche colonne numeriche per calcolare correlazioni.\")\n",
        "        corr_json_path.write_text(canonical_json_dumps({\"error\": \"not enough numeric columns\"}), encoding=\"utf-8\")\n",
        "    else:\n",
        "        # 2) Pearson\n",
        "        corr_mat = df[numeric_cols].corr(method=\"pearson\")\n",
        "        if VALUATION_K not in corr_mat.columns:\n",
        "            raise RuntimeError(\"Correlazione Pearson non calcolabile sul target (tutti NaN?).\")\n",
        "\n",
        "        correlations = corr_mat[VALUATION_K].drop(labels=[VALUATION_K], errors=\"ignore\").sort_values(ascending=False)\n",
        "\n",
        "        print(\"=\" * 60)\n",
        "        print(\"TOP 15 CORRELAZIONI POSITIVE (Pearson) CON IL TARGET\")\n",
        "        print(\"=\" * 60)\n",
        "        print(correlations.head(15))\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"TOP 15 CORRELAZIONI NEGATIVE (Pearson) CON IL TARGET\")\n",
        "        print(\"=\" * 60)\n",
        "        print(correlations.tail(15))\n",
        "\n",
        "        suspicious = correlations[correlations.abs() > SUSPICIOUS_THR]\n",
        "        if not suspicious.empty:\n",
        "            print(\"\\n🔴 ATTENZIONE: Correlazioni sospette |r| >\", SUSPICIOUS_THR)\n",
        "            for feat, corr in suspicious.items():\n",
        "                print(f\"  - {feat}: {corr:.4f}\")\n",
        "            logger.warning(\"Possibile leakage o duplicati semantici: %s\", list(suspicious.index))\n",
        "        else:\n",
        "            print(\"\\n✅ Nessuna correlazione sospetta (>|r| >\", SUSPICIOUS_THR, \")\")\n",
        "\n",
        "        # 3) Report strutturato + CSV\n",
        "        corr_df = pd.DataFrame(\n",
        "            {\"feature\": correlations.index, \"correlation_pearson\": correlations.values}\n",
        "        )\n",
        "        payload = {\n",
        "            \"meta\": {\n",
        "                \"method\": \"pearson\",\n",
        "                \"n_numeric_features\": int(len(numeric_cols) - 1),\n",
        "                \"target\": VALUATION_K,\n",
        "                \"suspicious_threshold\": SUSPICIOUS_THR,\n",
        "            },\n",
        "            \"correlations\": corr_df.to_dict(\"records\"),\n",
        "            \"suspicious\": suspicious.to_dict() if not suspicious.empty else {},\n",
        "        }\n",
        "        corr_json_path.write_text(\n",
        "            canonical_json_dumps(payload), encoding=\"utf-8\"\n",
        "        )\n",
        "        corr_df.to_csv(corr_csv_path, index=False)\n",
        "        logger.info(\"Correlations saved: %s (JSON) | %s (CSV)\", corr_json_path, corr_csv_path)\n",
        "\n",
        "        # 4) Spearman (best-effort, robusto a monotonia non lineare)\n",
        "        try:\n",
        "            spearman = df[numeric_cols].corr(method=\"spearman\")[VALUATION_K].drop(labels=[VALUATION_K], errors=\"ignore\")\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(\"TOP 10 CORRELAZIONI SPEARMAN (assolute) CON IL TARGET\")\n",
        "            print(\"=\" * 60)\n",
        "            print(spearman.reindex(spearman.abs().sort_values(ascending=False).index).head(10))\n",
        "        except Exception as e:\n",
        "            logger.debug(\"Spearman correlation failed: %s\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "69b68095-70f4-4b5d-ad0d-7b47ba5f9b7a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape prima dello split: (15000, 44)\n",
            "Colonne totali: 44\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-08-21 08:48:16,326] INFO model_trainer: train: 10500 rows, 44 cols\n",
            "[2025-08-21 08:48:16,327] INFO model_trainer: valid: 2250 rows, 44 cols\n",
            "[2025-08-21 08:48:16,328] INFO model_trainer: test: 2250 rows, 44 cols\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Split completato:\n",
            "  Train: (10500, 44)\n",
            "  Valid: (2250, 44)\n",
            "  Test:  (2250, 44)\n",
            "  Group column: asset_id\n"
          ]
        }
      ],
      "source": [
        "# 06) SPLIT TRAIN/VALID/TEST (strat. su decili) + blocco duplicati per gruppo (default: ASSET_ID)\n",
        "from __future__ import annotations\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# costanti (PRICE_PER_SQM può non essere importato in questo notebook)\n",
        "try:\n",
        "    from notebooks.shared.common.constants import VALUATION_K, ASSET_ID, PRICE_PER_SQM  # type: ignore\n",
        "except Exception:\n",
        "    from notebooks.shared.common.constants import VALUATION_K, ASSET_ID  # type: ignore\n",
        "    PRICE_PER_SQM = \"price_per_sqm\"\n",
        "\n",
        "TARGET = VALUATION_K\n",
        "\n",
        "# --- Verifica preliminare\n",
        "print(f\"Dataset shape prima dello split: {df.shape}\")\n",
        "print(f\"Colonne totali: {len(df.columns)}\")\n",
        "\n",
        "# --- 0) Parametri da config (con fallback)\n",
        "if \"TRAIN_CFG\" not in globals() or not isinstance(TRAIN_CFG, dict):\n",
        "    TRAIN_CFG = {}\n",
        "TEST_SIZE = float(TRAIN_CFG.get(\"test_size\", 0.15))\n",
        "VAL_SIZE  = float(TRAIN_CFG.get(\"val_size\",  0.15))\n",
        "N_DECILES = int(TRAIN_CFG.get(\"n_deciles\",   10))\n",
        "GROUP_COL = str(TRAIN_CFG.get(\"group_col\", ASSET_ID))  # puoi mettere 'location' in config se vuoi\n",
        "\n",
        "if not (0.01 <= TEST_SIZE <= 0.9) or not (0.01 <= VAL_SIZE <= 0.9):\n",
        "    logger.warning(\"test_size/val_size fuori range → fallback 0.15/0.15\")\n",
        "    TEST_SIZE, VAL_SIZE = 0.15, 0.15\n",
        "\n",
        "# --- 1) Pulizia target per lo split\n",
        "mask_y = pd.to_numeric(df[TARGET], errors=\"coerce\").notna()\n",
        "if not mask_y.all():\n",
        "    logger.warning(\"Righe senza target rimosse dallo split: %d\", (~mask_y).sum())\n",
        "df_clean = df.loc[mask_y].copy()\n",
        "\n",
        "# --- helper: stratificazione per decili “robusta”\n",
        "def _strat_bins(y: pd.Series, q: int = 10) -> pd.Series:\n",
        "    \"\"\"Decili robusti sul target (usa rank per duplicati). Fallback a singola classe.\"\"\"\n",
        "    y_num = pd.to_numeric(y, errors=\"coerce\")\n",
        "    ranks = y_num.rank(method=\"first\")\n",
        "    unique = int(ranks.nunique())\n",
        "    if unique < 2:\n",
        "        return pd.Series(0, index=y.index, dtype=int)\n",
        "    q_eff = max(2, min(int(q), unique))\n",
        "    try:\n",
        "        bins = pd.qcut(ranks, q=q_eff, labels=False, duplicates=\"drop\")\n",
        "    except Exception:\n",
        "        bins = pd.Series(0, index=y.index, dtype=int)\n",
        "    # riempi eventuali NaN con la moda\n",
        "    if bins.isna().any():\n",
        "        mode_bin = int(bins.dropna().mode().iat[0]) if not bins.dropna().empty else 0\n",
        "        bins = bins.fillna(mode_bin).astype(int)\n",
        "    return bins.astype(int)\n",
        "\n",
        "def _safe_stratify(labels: pd.Series | np.ndarray, min_per_class: int = 2):\n",
        "    \"\"\"Ritorna labels se idonee alla stratificazione, altrimenti None.\"\"\"\n",
        "    lab = pd.Series(labels)\n",
        "    vc = lab.value_counts()\n",
        "    if len(vc) < 2 or (vc < min_per_class).any():\n",
        "        return None\n",
        "    return lab.values\n",
        "\n",
        "# --- 2) Split con blocco duplicati per GROUP_COL (default: ASSET_ID), fallback classico se manca\n",
        "if GROUP_COL in df_clean.columns and df_clean[GROUP_COL].notna().any():\n",
        "    # mediana target per gruppo → decili a livello gruppo\n",
        "    df_clean[GROUP_COL] = df_clean[GROUP_COL].astype(str)\n",
        "    gstats = (\n",
        "        df_clean[[GROUP_COL, TARGET]]\n",
        "        .groupby(GROUP_COL, as_index=False)[TARGET]\n",
        "        .median()\n",
        "        .rename(columns={TARGET: f\"{TARGET}__group_median\"})\n",
        "    )\n",
        "\n",
        "    g_all = gstats[GROUP_COL].values\n",
        "    g_bins_all = _strat_bins(gstats[f\"{TARGET}__group_median\"], q=N_DECILES).values\n",
        "    strat_all = _safe_stratify(g_bins_all)\n",
        "\n",
        "    # primo split: test groups\n",
        "    g_tmp, g_test = train_test_split(\n",
        "        g_all,\n",
        "        test_size=TEST_SIZE,\n",
        "        random_state=SEED,\n",
        "        stratify=strat_all,\n",
        "    )\n",
        "\n",
        "    # secondo split: valid dal residuo\n",
        "    val_rel = VAL_SIZE / max(1e-9, (1.0 - TEST_SIZE))\n",
        "    val_rel = float(min(max(val_rel, 0.05), 0.8))\n",
        "\n",
        "    tmp_mask = np.isin(gstats[GROUP_COL].values, g_tmp)\n",
        "    gstats_tmp = gstats.loc[tmp_mask].copy()\n",
        "    # decili solo sui gruppi rimasti\n",
        "    bins_tmp = _strat_bins(gstats_tmp[f\"{TARGET}__group_median\"], q=N_DECILES).values\n",
        "    # mappa gruppo→bin per stratify\n",
        "    bin_map_tmp = dict(zip(gstats_tmp[GROUP_COL].values, bins_tmp))\n",
        "    y_tmp_bins = np.array([bin_map_tmp.get(g, 0) for g in g_tmp])\n",
        "    strat_tmp = _safe_stratify(y_tmp_bins)\n",
        "\n",
        "    g_train, g_valid = train_test_split(\n",
        "        g_tmp,\n",
        "        test_size=val_rel,\n",
        "        random_state=SEED,\n",
        "        stratify=strat_tmp,\n",
        "    )\n",
        "\n",
        "    G_TRAIN, G_VALID, G_TEST = set(g_train), set(g_valid), set(g_test)\n",
        "    df_train = df_clean[df_clean[GROUP_COL].isin(G_TRAIN)].copy()\n",
        "    df_valid = df_clean[df_clean[GROUP_COL].isin(G_VALID)].copy()\n",
        "    df_test  = df_clean[df_clean[GROUP_COL].isin(G_TEST)].copy()\n",
        "\n",
        "    # overlap check\n",
        "    def _overlap(a, b):\n",
        "        return set(a[GROUP_COL].astype(str)) & set(b[GROUP_COL].astype(str))\n",
        "    ov_tv = _overlap(df_train, df_valid)\n",
        "    ov_tt = _overlap(df_train, df_test)\n",
        "    ov_vt = _overlap(df_valid, df_test)\n",
        "    assert len(ov_tv) == 0 and len(ov_tt) == 0 and len(ov_vt) == 0, (\n",
        "        f\"Overlap {GROUP_COL} tra split! \"\n",
        "        f\"train∩valid={list(ov_tv)[:5]}, train∩test={list(ov_tt)[:5]}, valid∩test={list(ov_vt)[:5]}\"\n",
        "    )\n",
        "\n",
        "else:\n",
        "    logger.warning(\n",
        "        \"%s assente/non valido: uso fallback senza grouping (possibile leakage se ci sono duplicati).\",\n",
        "        GROUP_COL,\n",
        "    )\n",
        "\n",
        "    # stratify riga-level su decili target\n",
        "    bins_all = _strat_bins(df_clean[TARGET], q=N_DECILES)\n",
        "    df_tmp, df_test = train_test_split(\n",
        "        df_clean,\n",
        "        test_size=TEST_SIZE,\n",
        "        random_state=SEED,\n",
        "        stratify=_safe_stratify(bins_all),\n",
        "    )\n",
        "\n",
        "    val_rel = VAL_SIZE / max(1e-9, (1.0 - TEST_SIZE))\n",
        "    val_rel = float(min(max(val_rel, 0.05), 0.8))\n",
        "    bins_tmp = _strat_bins(df_tmp[TARGET], q=N_DECILES)\n",
        "    df_train, df_valid = train_test_split(\n",
        "        df_tmp,\n",
        "        test_size=val_rel,\n",
        "        random_state=SEED,\n",
        "        stratify=_safe_stratify(bins_tmp),\n",
        "    )\n",
        "\n",
        "# --- 3) Log e verifiche generali\n",
        "for name, part in ((\"train\", df_train), (\"valid\", df_valid), (\"test\", df_test)):\n",
        "    logger.info(\"%s: %d rows, %d cols\", name, len(part), part.shape[1])\n",
        "\n",
        "# partition disjoint per index\n",
        "assert len(set(df_train.index) & set(df_valid.index)) == 0\n",
        "assert len(set(df_train.index) & set(df_test.index)) == 0\n",
        "assert len(set(df_valid.index) & set(df_test.index)) == 0\n",
        "\n",
        "# --- 4) Airbag anti-leakage sugli split\n",
        "for split_name, split_df in [(\"train\", df_train), (\"valid\", df_valid), (\"test\", df_test)]:\n",
        "    if any(\"price_per_sqm\" in c.lower() for c in split_df.columns):\n",
        "        logger.error(\"🔴 LEAKAGE: colonne 'price_per_sqm*' in df_%s!\", split_name)\n",
        "\n",
        "print(\"\\n✅ Split completato:\")\n",
        "print(f\"  Train: {df_train.shape}\")\n",
        "print(f\"  Valid: {df_valid.shape}\")\n",
        "print(f\"  Test:  {df_test.shape}\")\n",
        "print(f\"  Group column: {GROUP_COL}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "61650dba",
      "metadata": {},
      "outputs": [],
      "source": [
        "# A) PRE-CHAIN GLOBALE: canonizza geo + crea/riempi prior/derivate minime\n",
        "from notebooks.shared.common.constants import SIZE_M2\n",
        "\n",
        "\n",
        "try:\n",
        "    from notebooks.shared.common.config import ASSET_CONFIG  # type: ignore\n",
        "    _PROP = ASSET_CONFIG[\"property\"]\n",
        "    _CITY_BASE = {c.lower(): {z.lower(): v for z, v in d.items()}\n",
        "                  for c, d in (_PROP.get(\"city_base_prices\") or {}).items()}\n",
        "    _REGION_INDEX = {k.lower(): v for k, v in (_PROP.get(\"region_index\") or {\n",
        "        \"north\": 1.05, \"center\": 1.00, \"south\": 0.92\n",
        "    }).items()}\n",
        "except Exception:\n",
        "    _CITY_BASE = {}\n",
        "    _REGION_INDEX = {\"north\": 1.05, \"center\": 1.00, \"south\": 0.92}\n",
        "\n",
        "# mediane di fallback per zona e globale\n",
        "_ZONE_KEYS = set(z for d in _CITY_BASE.values() for z in d.keys())\n",
        "_ZONE_MED = {z: float(np.nanmedian([d.get(z, np.nan) for d in _CITY_BASE.values()])) for z in _ZONE_KEYS} if _CITY_BASE else {}\n",
        "_GLOBAL_CITYZONE_MED = float(np.nanmedian([v for d in _CITY_BASE.values() for v in d.values()])) if _CITY_BASE else 0.0\n",
        "\n",
        "def _canon_geo(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    if \"city\" not in out.columns and \"location\" in out.columns:\n",
        "        out[\"city\"] = out[\"location\"]\n",
        "    if \"zone\" not in out.columns:\n",
        "        out[\"zone\"] = \"semi_center\"\n",
        "    if \"region\" not in out.columns:\n",
        "        out[\"region\"] = \"center\"\n",
        "    for col in (\"city\",\"zone\",\"region\"):\n",
        "        if col in out.columns:\n",
        "            out[col] = out[col].astype(str).str.strip().str.lower()\n",
        "    return out\n",
        "\n",
        "def _ensure_priors_and_min_derivatives(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Crea/riempi: no_elev_high_floor, rooms_per_100sqm, city_zone_prior, region_index_prior.\"\"\"\n",
        "    out = _canon_geo(df)\n",
        "\n",
        "    # no_elev_high_floor = penalità (niente ascensore & piano > 1)\n",
        "    f = pd.to_numeric(out.get(\"floor\"), errors=\"coerce\")\n",
        "    e = pd.to_numeric(out.get(\"has_elevator\"), errors=\"coerce\").fillna(0)\n",
        "    out[\"no_elev_high_floor\"] = ((1 - e) * np.maximum(f - 1, 0)).astype(\"float64\")\n",
        "\n",
        "    # rooms_per_100sqm\n",
        "    s = pd.to_numeric(out.get(SIZE_M2), errors=\"coerce\").replace(0, np.nan)\n",
        "    r = pd.to_numeric(out.get(\"rooms\"), errors=\"coerce\")\n",
        "    out[\"rooms_per_100sqm\"] = (100.0 * r / s).astype(\"float64\")\n",
        "\n",
        "    # city_zone_prior (CITY_BASE con fallback zona→globale)\n",
        "    ci = out.get(\"city\", pd.Series(index=out.index, dtype=str)).astype(str).str.lower()\n",
        "    zo = out.get(\"zone\", pd.Series(index=out.index, dtype=str)).astype(str).str.lower()\n",
        "    vals = []\n",
        "    for c, z in zip(ci, zo):\n",
        "        v = _CITY_BASE.get(c, {}).get(z, np.nan)\n",
        "        if pd.isna(v):\n",
        "            v = _ZONE_MED.get(z, _GLOBAL_CITYZONE_MED)\n",
        "        vals.append(v)\n",
        "    out[\"city_zone_prior\"] = np.asarray(vals, dtype=\"float64\")\n",
        "\n",
        "    # region_index_prior (macroarea)\n",
        "    out[\"region_index_prior\"] = out[\"region\"].astype(str).str.lower().map(_REGION_INDEX).astype(\"float64\")\n",
        "\n",
        "    return out\n",
        "\n",
        "# applica a tutti gli split (non è leakage: sono feature ex-ante, no target)\n",
        "for _name in (\"df_train\",\"df_valid\",\"df_test\"):\n",
        "    if _name in globals() and isinstance(globals()[_name], pd.DataFrame):\n",
        "        globals()[_name] = _ensure_priors_and_min_derivatives(globals()[_name])\n",
        "\n",
        "try:\n",
        "    n_nan_cz = int(pd.to_numeric(df_train[\"city_zone_prior\"], errors=\"coerce\").isna().sum())\n",
        "    n_nan_ri = int(pd.to_numeric(df_train[\"region_index_prior\"], errors=\"coerce\").isna().sum())\n",
        "    n_nan_ne = int(pd.to_numeric(df_train[\"no_elev_high_floor\"], errors=\"coerce\").isna().sum())\n",
        "    n_nan_rr = int(pd.to_numeric(df_train[\"rooms_per_100sqm\"], errors=\"coerce\").isna().sum())\n",
        "    (ART_DIR / \"prechain_checks.txt\").write_text(\n",
        "        f\"train NaN city_zone_prior={n_nan_cz}, region_index_prior={n_nan_ri}, \"\n",
        "        f\"no_elev_high_floor={n_nan_ne}, rooms_per_100sqm={n_nan_rr}\\n\",\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "except Exception:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f60aec06-522b-4e18-ad3b-71c1f0e62d1b",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Anomaly Flags (train only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3379aac2-ab77-48cc-a77d-693a20d1104c",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-08-21 08:48:23,003] INFO model_trainer: Anomaly features (train only): ['condition_minus_risk', 'size_m2', 'luxury_score', 'env_score', 'distance_to_center_km', 'air_quality_index', 'noise_level', 'humidity_level', 'temperature_avg']\n",
            "[2025-08-21 08:48:31,248] INFO model_trainer: Anomalie raw: 315 | refined: 276\n",
            "[2025-08-21 08:48:31,262] INFO model_trainer: sample_weight da severity_score (mean=1.000, min=0.802, max=1.001)\n"
          ]
        }
      ],
      "source": [
        "# 04) Flags di outlier/anomalie SOLO sul TRAIN → feature/pesi (no leakage)\n",
        "from __future__ import annotations\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- costanti con import robusto\n",
        "try:\n",
        "    from notebooks.shared.common.constants import ENV_SCORE, LUXURY_SCORE, SIZE_M2, VALUATION_K  # type: ignore\n",
        "except Exception:\n",
        "    ENV_SCORE, LUXURY_SCORE, SIZE_M2, VALUATION_K = \"env_score\", \"luxury_score\", \"size_m2\", \"valuation_k\"\n",
        "\n",
        "try:\n",
        "    from notebooks.shared.common.constants import LAG_HOURS  # type: ignore\n",
        "except Exception:\n",
        "    LAG_HOURS = \"lag_hours\"\n",
        "\n",
        "# feature “leaky”/derivate definite nell’EDA (se presenti)\n",
        "try:\n",
        "    from notebooks.shared.n02_explore_dataset.eda_core import AnomalyDetector, LEAKY_FEATURES, TARGET_DERIVED_FEATURES  # type: ignore\n",
        "except Exception:\n",
        "    AnomalyDetector = None  # fallback sotto\n",
        "    LEAKY_FEATURES = {\"price_per_sqm\", \"price_per_sqm_capped\", \"price_per_sqm_vs_region_avg\", \"valuation_k_log\"}\n",
        "    TARGET_DERIVED_FEATURES = {\"_viz_price_per_sqm\", \"valuation_k_decile\", \"valuation_rank\", \"is_top_valuation\"}\n",
        "\n",
        "# Parametri con override da config\n",
        "if \"TRAIN_CFG\" not in globals() or not isinstance(TRAIN_CFG, dict):\n",
        "    TRAIN_CFG = {}\n",
        "contamination       = float(TRAIN_CFG.get(\"anomaly_contamination\", 0.03))\n",
        "strong_z_threshold  = float(TRAIN_CFG.get(\"anomaly_strong_z\", 2.5))\n",
        "severity_percentile = float(TRAIN_CFG.get(\"anomaly_severity_pct\", 90.0))\n",
        "n_estimators        = int(TRAIN_CFG.get(\"anomaly_n_estimators\", 200))\n",
        "\n",
        "# 4.1 Scelta feature candidate (numeric, no target/leaky/derived)\n",
        "num_cols = [c for c in df_train.columns if pd.api.types.is_numeric_dtype(df_train[c])]\n",
        "exclude  = set(LEAKY_FEATURES) | set(TARGET_DERIVED_FEATURES) | {VALUATION_K, \"price_per_sqm\"}\n",
        "\n",
        "prefer   = [\n",
        "    \"condition_minus_risk\", SIZE_M2, LUXURY_SCORE, ENV_SCORE,\n",
        "    \"building_age_years\", \"distance_to_center_km\", LAG_HOURS,\n",
        "    \"air_quality_index\", \"noise_level\", \"humidity_level\", \"temperature_avg\",\n",
        "]\n",
        "feat_cand = [c for c in prefer if c in df_train.columns and c in num_cols and c not in exclude]\n",
        "\n",
        "# fallback: scegli le prime N numeriche con var > 0 e almeno 10 valori unici\n",
        "if len(feat_cand) < 3:\n",
        "    cand_pool = []\n",
        "    for c in num_cols:\n",
        "        if c in exclude or c == \"sample_weight\":\n",
        "            continue\n",
        "        s = pd.to_numeric(df_train[c], errors=\"coerce\")\n",
        "        if s.nunique(dropna=True) >= 10 and np.nanvar(s.values) > 0:\n",
        "            cand_pool.append((c, float(np.nanvar(s.values))))\n",
        "    cand_pool.sort(key=lambda x: x[1], reverse=True)\n",
        "    feat_cand = [c for c, _ in cand_pool[:8]]  # max 8\n",
        "\n",
        "if feat_cand:\n",
        "    logger.info(\"Anomaly features (train only): %s\", feat_cand)\n",
        "\n",
        "    # 4.2 Rilevamento anomalie (classe ufficiale → fallback z-score)\n",
        "    if AnomalyDetector is not None:\n",
        "        anom = AnomalyDetector(\n",
        "            contamination=contamination,\n",
        "            strong_z_threshold=strong_z_threshold,\n",
        "            severity_percentile=severity_percentile,\n",
        "            n_estimators=n_estimators,\n",
        "            random_state=SEED,\n",
        "        )\n",
        "        df_train_anom, anom_rep = anom.detect_anomalies(\n",
        "            df_train,\n",
        "            feature_candidates=feat_cand,\n",
        "            exclude_features=set(),  # già esclusi a monte\n",
        "        )\n",
        "    else:\n",
        "        # --- Fallback semplice: z-score medio + percentile su features candidate\n",
        "        X = df_train[feat_cand].copy()\n",
        "        X = X.apply(pd.to_numeric, errors=\"coerce\")\n",
        "        mu = X.mean(axis=0)\n",
        "        sd = X.std(axis=0).replace(0, np.nan)\n",
        "        z  = (X - mu) / sd\n",
        "        z_abs = z.abs()\n",
        "        z_mean = z_abs.mean(axis=1)  # severità media\n",
        "        thr = np.nanpercentile(z_mean.dropna().values, severity_percentile)\n",
        "        flags_raw = (z_abs > strong_z_threshold).any(axis=1)\n",
        "        flags_ref = (z_mean >= thr)\n",
        "\n",
        "        df_train_anom = df_train.copy()\n",
        "        df_train_anom[\"anomaly_flag\"]    = flags_raw.astype(np.int8)\n",
        "        df_train_anom[\"anomaly_refined\"] = flags_ref.astype(np.int8)\n",
        "        df_train_anom[\"severity_score\"]  = z_mean.fillna(0).astype(\"float32\")\n",
        "\n",
        "        n_raw = int(flags_raw.sum())\n",
        "        n_ref = int(flags_ref.sum())\n",
        "        anom_rep = {\n",
        "            \"method\": \"fallback_zscore\",\n",
        "            \"features\": feat_cand,\n",
        "            \"strong_z_threshold\": strong_z_threshold,\n",
        "            \"severity_percentile\": severity_percentile,\n",
        "            \"n_anomalies_raw\": n_raw,\n",
        "            \"n_anomalies_refined\": n_ref,\n",
        "        }\n",
        "\n",
        "    logger.info(\"Anomalie raw: %s | refined: %s\",\n",
        "                anom_rep.get(\"n_anomalies_raw\"), anom_rep.get(\"n_anomalies_refined\"))\n",
        "\n",
        "    # 4.3 trasferisci colonne utili SOLO su train (no leakage)\n",
        "    for col in (\"anomaly_flag\", \"anomaly_refined\", \"severity_score\"):\n",
        "        if col in df_train_anom.columns:\n",
        "            df_train.loc[df_train_anom.index, col] = df_train_anom[col]\n",
        "\n",
        "    # 4.4 salva report\n",
        "    (ART_DIR / \"anomaly_train_report.json\").write_text(\n",
        "        canonical_json_dumps(anom_rep),\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "else:\n",
        "    logger.info(\"Anomaly detection skipped: nessuna feature candidata valida.\")\n",
        "\n",
        "# 4.5 sample_weight (fallback = 1.0) — SOLO TRAIN\n",
        "if \"severity_score\" in df_train.columns and df_train[\"severity_score\"].notna().any():\n",
        "    sev = pd.to_numeric(df_train[\"severity_score\"], errors=\"coerce\").clip(lower=0).astype(\"float32\")\n",
        "    w   = 1.0 / (1.0 + sev)              # decresce con severità\n",
        "    w   = w.clip(lower=0.2, upper=1.0)   # evita pesi troppo piccoli\n",
        "    w   = w * (1.0 / max(w.mean(), 1e-6))  # normalize mean≈1.0\n",
        "    df_train[\"sample_weight\"] = w.astype(\"float32\")\n",
        "    logger.info(\"sample_weight da severity_score (mean=%.3f, min=%.3f, max=%.3f)\",\n",
        "                float(w.mean()), float(w.min()), float(w.max()))\n",
        "elif \"confidence_score\" in df_train.columns and df_train[\"confidence_score\"].notna().any():\n",
        "    w = pd.to_numeric(df_train[\"confidence_score\"], errors=\"coerce\").clip(0.2, 1.0).astype(\"float32\")\n",
        "    w = w * (1.0 / max(w.mean(), 1e-6))\n",
        "    df_train[\"sample_weight\"] = w\n",
        "    logger.info(\"sample_weight da confidence_score (mean=%.3f, min=%.3f, max=%.3f)\",\n",
        "                float(w.mean()), float(w.min()), float(w.max()))\n",
        "else:\n",
        "    df_train[\"sample_weight\"] = np.float32(1.0)\n",
        "    logger.info(\"sample_weight uniforme (1.0) — nessuna metrica disponibile.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c915f13-189c-4380-9c72-a30680dfdd1b",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Feature Preparation & Pipelines A/B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26fd68d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 08-bis) Stratified splits su decili del target (train/valid/test) + distribuzioni\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "VALUATION_K = \"valuation_k\"\n",
        "SEED = int(globals().get(\"SEED\", 42))\n",
        "\n",
        "def _load_base_df():\n",
        "    # priorità: df già in RAM; in alternativa carica da outputs/\n",
        "    for name in (\"df_full\", \"df\", \"df_raw\", \"df_train_full\"):\n",
        "        if name in globals() and isinstance(globals()[name], pd.DataFrame) and VALUATION_K in globals()[name].columns:\n",
        "            return globals()[name].copy()\n",
        "    for p in [Path(\"outputs/dataset_generated.parquet\"), Path(\"outputs/dataset_generated.csv\")]:\n",
        "        if p.exists():\n",
        "            return pd.read_parquet(p) if p.suffix.lower() != \".csv\" else pd.read_csv(p)\n",
        "    raise RuntimeError(\"Dataset base non trovato per lo split stratificato.\")\n",
        "\n",
        "df_all = _load_base_df()\n",
        "df_all = df_all.loc[pd.to_numeric(df_all[VALUATION_K], errors=\"coerce\").notna()].copy()\n",
        "\n",
        "y_all = pd.to_numeric(df_all[VALUATION_K], errors=\"coerce\").to_numpy(dtype=\"float64\")\n",
        "\n",
        "# Decili robusti\n",
        "try:\n",
        "    d_all = pd.qcut(y_all, q=10, labels=False, duplicates=\"drop\")\n",
        "    if d_all.isna().any():\n",
        "        raise ValueError(\"qcut produced NaNs\")\n",
        "    d_all = d_all.to_numpy(dtype=int)\n",
        "except Exception:\n",
        "    qs = np.quantile(y_all[~np.isnan(y_all)], np.linspace(0, 1, 11))\n",
        "    qs = np.unique(qs)\n",
        "    if len(qs) < 3:\n",
        "        d_all = np.zeros_like(y_all, dtype=int)\n",
        "    else:\n",
        "        d_all = np.clip(np.digitize(y_all, qs[1:-1], right=True), 0, 9)\n",
        "\n",
        "# Split 70/15/15 con stratify\n",
        "idx_all = np.arange(len(df_all))\n",
        "idx_tr, idx_tmp, y_tr, y_tmp, d_tr, d_tmp = train_test_split(\n",
        "    idx_all, y_all, d_all, test_size=0.30, stratify=d_all, random_state=SEED, shuffle=True\n",
        ")\n",
        "idx_va, idx_te, y_va, y_te, d_va, d_te = train_test_split(\n",
        "    idx_tmp, y_tmp, d_tmp, test_size=0.50, stratify=d_tmp, random_state=SEED, shuffle=True\n",
        ")\n",
        "\n",
        "df_train = df_all.iloc[idx_tr].copy()\n",
        "df_valid = df_all.iloc[idx_va].copy()\n",
        "df_test  = df_all.iloc[idx_te].copy()\n",
        "\n",
        "# Esponi anche i decili (la cella 09 li raccoglie nel manifest “best-effort”)\n",
        "d_train, d_valid, d_test = d_tr, d_va, d_te\n",
        "\n",
        "def _counts(arr):\n",
        "    uniq, cnt = np.unique(arr, return_counts=True)\n",
        "    return {int(k): int(v) for k, v in zip(uniq, cnt)}\n",
        "\n",
        "print(\"Decile distribution:\")\n",
        "print(\" train:\", _counts(d_train))\n",
        "print(\" valid:\", _counts(d_valid))\n",
        "print(\" test :\", _counts(d_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "dd171f89-0d87-4a14-b26a-3e39db97ebec",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== VERIFICA FEATURES =====\n",
            "Categoriche: ['city', 'zone', 'urban_type', 'region', 'energy_class', 'condition', 'heating', 'view', 'public_transport_nearby']\n",
            "Numeriche  : ['size_m2', 'rooms', 'bathrooms', 'year_built', 'floor', 'building_floors', 'is_top_floor', 'has_elevator', 'has_garden', 'has_balcony', 'log_size_m2', 'sqm_per_room', 'baths_per_100sqm', 'elev_x_floor', 'no_elev_high_floor', 'rooms_per_100sqm', 'city_zone_prior', 'region_index_prior']\n",
            "Deriver attivo?: True\n",
            "Model     A: RandomForest | n_features: 27\n",
            "Model     B: RandomForest | n_features: 27\n"
          ]
        }
      ],
      "source": [
        "# === FEATURE PREP + ANALISI + AUTO-UPDATE (UNIFICATA) ========================\n",
        "from __future__ import annotations\n",
        "import os, re, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# ── util / costanti minime\n",
        "try:\n",
        "    from notebooks.shared.n03_train_model.preprocessing import ML_LEAKY_FEATURES  # type: ignore\n",
        "except Exception:\n",
        "    ML_LEAKY_FEATURES = {\n",
        "        \"price_per_sqm\",\"price_per_sqm_vs_region_avg\",\"price_per_sqm_capped\",\n",
        "        \"valuation_k_log\",\"_viz_price_per_sqm\",\n",
        "        \"valuation_k_decile\",\"valuation_rank\",\"is_top_valuation\"\n",
        "    }\n",
        "\n",
        "try:\n",
        "    NumpyJSONEncoder\n",
        "except NameError:\n",
        "    class NumpyJSONEncoder(json.JSONEncoder):\n",
        "        def default(self, obj):\n",
        "            import numpy as _np\n",
        "            if isinstance(obj, (_np.integer,)):  return int(obj)\n",
        "            if isinstance(obj, (_np.floating,)): return float(obj)\n",
        "            if isinstance(obj, (_np.ndarray,)):  return obj.tolist()\n",
        "            return super().default(obj)\n",
        "\n",
        "\n",
        "def _ensure_columns(df_part: pd.DataFrame, required: list[str]) -> pd.DataFrame:\n",
        "    missing = [c for c in required if c not in df_part.columns]\n",
        "    if missing:\n",
        "        for c in missing:\n",
        "            df_part[c] = np.nan\n",
        "    return df_part[required]\n",
        "\n",
        "from notebooks.shared.common.constants import VALUATION_K, ASSET_ID\n",
        "from notebooks.shared.common.config import ASSET_CONFIG\n",
        "\n",
        "# ── helpers\n",
        "def _uniq(xs: list[str]) -> list[str]: return list(dict.fromkeys(xs))\n",
        "def _matches_any(col: str, pats: list[str]) -> bool: return any(re.search(p, col, re.I) for p in pats)\n",
        "def _is_numeric(s: pd.Series) -> bool: return pd.api.types.is_numeric_dtype(s)\n",
        "\n",
        "# ==== 0) Policy allowlist + esclusioni (guided by config) ====================\n",
        "_cfg = ASSET_CONFIG[\"property\"]\n",
        "CFG_CAT = list(_cfg.get(\"categorical\", []))\n",
        "CFG_NUM = list(_cfg.get(\"numeric\", []))\n",
        "CFG_EXC = set(_cfg.get(\"exclude\", []))\n",
        "\n",
        "EXTRA_KEEP = [\"rooms\",\"has_elevator\",\"has_garage\",\"has_garden\",\"has_balcony\",\n",
        "              \"is_top_floor\",\"listing_month\",\"city\",\"zone\",\"urban_type\"]\n",
        "FEATURE_ALLOWLIST = _uniq([*(CFG_CAT + CFG_NUM), *EXTRA_KEEP])\n",
        "\n",
        "ALWAYS_EXCLUDE = set(CFG_EXC) | {\n",
        "    VALUATION_K,\n",
        "    ASSET_ID, \"record_id\",\"listing_id\",\"asset_type_id\",\n",
        "    \"source\",\"source_name\",\"source_url\",\"dataset_version\",\n",
        "    \"prediction_ts\",\"last_verified_ts\",\"ingestion_ts\",\n",
        "    \"created_at\",\"updated_at\",\"listing_ts\",\"lag_hours\",\n",
        "    \"sample_weight\",\"weight\",\"severity_score\",\n",
        "    \"outlier_count\",\"n_outlier_sources\",\"outlier_source\",\n",
        "    \"confidence_score\",\n",
        "    \"y_pred\",\"predicted_valuation_k\",\"valuation_k_hat\",\n",
        "    \"valuation_k_log\",\"valuation_k_decile\",\"valuation_rank\",\"is_top_valuation\",\n",
        "    \"strongly_incoherent\",\"price_per_sqm_capped_violated\",\n",
        "}\n",
        "\n",
        "EXCLUDE_PATTERNS = [\n",
        "    r\"price_per_sqm\",\n",
        "    r\"^valuation_k_.+\",\n",
        "    r\"(?:^|_)id$\",\n",
        "    r\"(?:^|_)(created|updated|ingestion|prediction|last_verified|listing)_ts$\",\n",
        "    r\"_url$|_hash$\",\n",
        "    r\"^y_pred$|_pred(?:iction)?_\",\n",
        "    r\"(?:^|_)(avg|mean|median|benchmark|zscore|rank|decile)(?:_|$).*?(price|valuation)\",\n",
        "    r\"(price|valuation).*?(avg|mean|median|benchmark|zscore|rank|decile)\",\n",
        "    r\"(?:^|_)drift(?:_|$)|(?:^|_)caps?(?:_|$)|(?:^|_)vs_(?:_|$)\",\n",
        "]\n",
        "\n",
        "# leakage hard-stop\n",
        "leaky_check = [c for c in ML_LEAKY_FEATURES if c in df_train.columns]\n",
        "if leaky_check:\n",
        "    raise ValueError(f\"Leakage detected in training set: {leaky_check}\")\n",
        "\n",
        "DYNAMIC_EXCLUDE = {c for c in df_train.columns if _matches_any(c, EXCLUDE_PATTERNS)}\n",
        "EXCLUDE_ALL = set(ALWAYS_EXCLUDE) | set(DYNAMIC_EXCLUDE)\n",
        "\n",
        "# ==== 1) Split iniziale cat / num (allowlist-aware) ==========================\n",
        "# Nota: per evitare overweight della macroarea, NON obblighiamo 'region' come categorica.\n",
        "CATEGORICAL_FEATURES = _uniq([\"city\",\"zone\",\"urban_type\"] + [c for c in CFG_CAT if c not in {\"location\"}])\n",
        "cat_cols = [c for c in CATEGORICAL_FEATURES if c in df_train.columns and c not in EXCLUDE_ALL]\n",
        "num_cols = [c for c in df_train.columns if (c not in EXCLUDE_ALL) and _is_numeric(df_train[c])]\n",
        "\n",
        "allow = set([c for c in FEATURE_ALLOWLIST if c in df_train.columns])\n",
        "cat_cols = [c for c in _uniq(cat_cols) if c in allow]\n",
        "num_cols = [c for c in _uniq(num_cols) if (c not in set(cat_cols)) and (c in allow)]\n",
        "\n",
        "MIN_FEATS = int(os.getenv(\"MIN_FEATS_ALLOWLIST\", \"12\"))\n",
        "if len(cat_cols) + len(num_cols) < MIN_FEATS:\n",
        "    more_cat = [c for c in CATEGORICAL_FEATURES if c in df_train.columns and c not in EXCLUDE_ALL and c not in cat_cols]\n",
        "    more_num = []\n",
        "    for c in df_train.columns:\n",
        "        if c in EXCLUDE_ALL or c in cat_cols or c in num_cols: \n",
        "            continue\n",
        "        s = pd.to_numeric(df_train[c], errors=\"coerce\")\n",
        "        if _is_numeric(s) and s.nunique(dropna=True) >= 10 and np.nanvar(s.values) > 0:\n",
        "            more_num.append(c)\n",
        "    cat_cols = _uniq(cat_cols + more_cat)[:15]\n",
        "    num_cols = _uniq(num_cols + more_num)[:25]\n",
        "\n",
        "constant_cols = [c for c in num_cols if df_train[c].nunique(dropna=True) <= 1]\n",
        "if constant_cols:\n",
        "    num_cols = [c for c in num_cols if c not in constant_cols]\n",
        "\n",
        "# ==== 2) (opzionale) step di derivazione — SOLO via transformer importabile ===\n",
        "# Niente logiche/priors qui: se il transformer c'è lo usiamo, altrimenti si procede senza.\n",
        "_include_derive = False\n",
        "DERIVED_FEATURES = [\n",
        "    \"log_size_m2\",\"sqm_per_room\",\"baths_per_100sqm\",\n",
        "    \"elev_x_floor\",\"no_elev_high_floor\",\"rooms_per_100sqm\",\n",
        "    \"city_zone_prior\",\"region_index_prior\",\n",
        "]\n",
        "feature_deriver = \"passthrough\"\n",
        "try:\n",
        "    # prova più namespace\n",
        "    try:\n",
        "        from notebooks.shared.common.transformers import PropertyDerivedFeatures  # type: ignore\n",
        "    except Exception:\n",
        "        from notebooks.shared.common.transformers import PropertyDerivedFeatures  # type: ignore  # noqa\n",
        "    feature_deriver = PropertyDerivedFeatures()  # usa i default interni\n",
        "    _include_derive = True\n",
        "except Exception:\n",
        "    _include_derive = False\n",
        "\n",
        "# se il deriver è attivo, dichiara le derivate tra le numeriche (verranno create nello step precedente al prep)\n",
        "if _include_derive:\n",
        "    num_cols = _uniq(num_cols + DERIVED_FEATURES)\n",
        "\n",
        "# ==== 3) Preprocessori (OHE compat) =========================================\n",
        "def _build_ohe(min_freq=None, as_sparse=True):\n",
        "    kw = dict(handle_unknown=\"ignore\")\n",
        "    if isinstance(min_freq, (int, float)):\n",
        "        try: kw[\"min_frequency\"] = min_freq\n",
        "        except TypeError: pass\n",
        "    try:\n",
        "        return OneHotEncoder(sparse_output=as_sparse, **kw)  # sklearn >=1.2\n",
        "    except TypeError:\n",
        "        return OneHotEncoder(sparse=as_sparse, **kw)         # sklearn <1.2\n",
        "\n",
        "min_freq = TRAIN_CFG.get(\"ohe_min_frequency\", None)\n",
        "\n",
        "cat_pipe = Pipeline([\n",
        "    (\"impute\", SimpleImputer(strategy=\"constant\", fill_value=\"__MISSING__\")),\n",
        "    (\"encode\", _build_ohe(min_freq)),\n",
        "])\n",
        "num_pipe = Pipeline([(\"impute\", SimpleImputer(strategy=\"median\"))])\n",
        "\n",
        "# ==== 4) Colonne & pipeline A/B =============================================\n",
        "num_cols_B = [c for c in num_cols if c != \"confidence_score\"]\n",
        "\n",
        "preproc_A = ColumnTransformer([(\"cat\", cat_pipe, cat_cols), (\"num\", num_pipe, num_cols)], remainder=\"drop\")\n",
        "preproc_B = ColumnTransformer([(\"cat\", cat_pipe, cat_cols), (\"num\", num_pipe, num_cols_B)], remainder=\"drop\")\n",
        "\n",
        "features_A = _uniq(cat_cols + num_cols)\n",
        "features_B = _uniq(cat_cols + num_cols_B)\n",
        "\n",
        "df_train_A = _ensure_columns(df_train.copy(), features_A)\n",
        "df_valid_A = _ensure_columns(df_valid.copy(), features_A)\n",
        "df_test_A  = _ensure_columns(df_test.copy(),  features_A)\n",
        "\n",
        "df_train_B = _ensure_columns(df_train.copy(), features_B)\n",
        "df_valid_B = _ensure_columns(df_valid.copy(), features_B)\n",
        "df_test_B  = _ensure_columns(df_test.copy(),  features_B)\n",
        "\n",
        "X_train = df_train_A[features_A].copy()\n",
        "X_valid = df_valid_A[features_A].copy()\n",
        "X_test  = df_test_A[features_A].copy()\n",
        "\n",
        "Xtr_B = df_train_B[features_B].copy()\n",
        "Xva_B = df_valid_B[features_B].copy()\n",
        "Xte_B = df_test_B[features_B].copy()\n",
        "\n",
        "MODEL_KIND = str(TRAIN_CFG.get(\"model\", os.getenv(\"MODEL_KIND\",\"rf\"))).lower()\n",
        "ModelA = RandomForestRegressor; ModelB = RandomForestRegressor\n",
        "MODEL_FAMILY_A = MODEL_FAMILY_B = \"RandomForest\"\n",
        "\n",
        "if MODEL_KIND in {\"xgb\",\"xgboost\"}:\n",
        "    try:\n",
        "        from xgboost import XGBRegressor  # type: ignore\n",
        "        ModelA = ModelB = XGBRegressor\n",
        "        MODEL_FAMILY_A = MODEL_FAMILY_B = \"XGBRegressor\"\n",
        "    except Exception:\n",
        "        MODEL_FAMILY_A = MODEL_FAMILY_B = \"RandomForest\"\n",
        "\n",
        "if MODEL_FAMILY_A == \"RandomForest\":\n",
        "    model_A = ModelA(n_estimators=400, random_state=SEED, n_jobs=-1, max_depth=None, min_samples_leaf=2)\n",
        "    model_B = ModelB(n_estimators=400, random_state=SEED, n_jobs=-1, max_depth=None, min_samples_leaf=2)\n",
        "else:\n",
        "    model_A = ModelA(n_estimators=500, max_depth=6, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8,\n",
        "                     reg_alpha=0.0, reg_lambda=1.0, random_state=SEED, tree_method=\"hist\")\n",
        "    model_B = ModelB(n_estimators=500, max_depth=6, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8,\n",
        "                     reg_alpha=0.0, reg_lambda=1.0, random_state=SEED, tree_method=\"hist\")\n",
        "\n",
        "steps_A = [(\"prep\", preproc_A), (\"model\", model_A)]\n",
        "steps_B = [(\"prep\", preproc_B), (\"model\", model_B)]\n",
        "if _include_derive:\n",
        "    steps_A = [(\"derive\", feature_deriver)] + steps_A\n",
        "    steps_B = [(\"derive\", feature_deriver)] + steps_B\n",
        "\n",
        "pipe_A = Pipeline(steps_A)\n",
        "pipe_B = Pipeline(steps_B)\n",
        "\n",
        "# ==== 5) Analisi semplice & artefatti =======================================\n",
        "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
        "card_rows = []\n",
        "for c in features_A:\n",
        "    if c not in df_train.columns: continue\n",
        "    s = df_train[c]\n",
        "    card_rows.append({\n",
        "        \"feature\": c, \"dtype\": str(s.dtype),\n",
        "        \"n_unique\": int(s.nunique(dropna=True)),\n",
        "        \"pct_missing\": float(s.isna().mean()*100.0),\n",
        "        \"is_categorical_like\": bool(s.dtype.name in (\"object\",\"category\",\"bool\")),\n",
        "        \"is_numeric\": bool(pd.api.types.is_numeric_dtype(s)),\n",
        "    })\n",
        "pd.DataFrame(card_rows).sort_values([\"is_categorical_like\",\"n_unique\"], ascending=[False,True]) \\\n",
        "  .to_csv(ART_DIR / \"mlprep_cardinality_missing.csv\", index=False)\n",
        "\n",
        "# ==== 6) Allinea target/pesi (senza manipolarli qui) =========================\n",
        "def _align_targets_and_weights():\n",
        "    global y_train, y_valid, y_test, y_val_orig, y_test_orig, w_train\n",
        "    y_train = df_train.loc[X_train.index, VALUATION_K].to_numpy()\n",
        "    y_valid = df_valid.loc[X_valid.index, VALUATION_K].to_numpy()\n",
        "    y_test  = df_test.loc[X_test.index,  VALUATION_K].to_numpy()\n",
        "    y_val_orig  = y_valid.copy(); y_test_orig = y_test.copy()\n",
        "    if \"Xtr_B\" in globals():\n",
        "        if \"sample_weight\" in df_train.columns:\n",
        "            w_train = df_train.loc[Xtr_B.index, \"sample_weight\"].astype(\"float32\").to_numpy()\n",
        "        else:\n",
        "            w_train = np.ones(len(Xtr_B), dtype=\"float32\")\n",
        "    assert len(X_train)==len(y_train) and len(X_valid)==len(y_val_orig) and len(X_test)==len(y_test_orig)\n",
        "_align_targets_and_weights()\n",
        "\n",
        "print(\"\\n===== VERIFICA FEATURES =====\")\n",
        "print(\"Categoriche:\", cat_cols)\n",
        "print(\"Numeriche  :\", num_cols)\n",
        "print(\"Deriver attivo?:\", _include_derive)\n",
        "print(\"Model     A:\", MODEL_FAMILY_A, \"| n_features:\", len(features_A))\n",
        "print(\"Model     B:\", MODEL_FAMILY_B, \"| n_features:\", len(features_B))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8ee34b56",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === PATCH EVAL: assicurati che le derivate/priors ci siano (no all-NaN) ===\n",
        "from __future__ import annotations\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "# prova a leggere i mapping da config (fallback sicuri)\n",
        "try:\n",
        "    from notebooks.shared.common.config import ASSET_CONFIG  # type: ignore\n",
        "    _PROP = ASSET_CONFIG[\"property\"]\n",
        "    _CITY_BASE = {c.lower(): {z.lower(): v for z, v in d.items()}\n",
        "                  for c, d in (_PROP.get(\"city_base_prices\") or {}).items()}\n",
        "    _REGION_INDEX = {k.lower(): float(v) for k, v in (_PROP.get(\"region_index\") or {\n",
        "        \"north\": 1.05, \"center\": 1.00, \"south\": 0.92\n",
        "    }).items()}\n",
        "except Exception:\n",
        "    _CITY_BASE = {}\n",
        "    _REGION_INDEX = {\"north\": 1.05, \"center\": 1.00, \"south\": 0.92}\n",
        "\n",
        "# mediane di fallback per zona e globale (se serve)\n",
        "_ZONE_KEYS = set(z for d in _CITY_BASE.values() for z in d.keys())\n",
        "_ZONE_MED = {z: float(np.nanmedian([d.get(z, np.nan) for d in _CITY_BASE.values()]))\n",
        "             for z in _ZONE_KEYS} if _CITY_BASE else {}\n",
        "_GLOBAL_CITYZONE_MED = (float(np.nanmedian([v for d in _CITY_BASE.values() for v in d.values()]))\n",
        "                        if _CITY_BASE else 0.0)\n",
        "\n",
        "_DERIVED_ALL = [\n",
        "    \"log_size_m2\",\"sqm_per_room\",\"baths_per_100sqm\",\"elev_x_floor\",\n",
        "    \"no_elev_high_floor\",\"rooms_per_100sqm\",\"city_zone_prior\",\"region_index_prior\",\n",
        "]\n",
        "\n",
        "def _needs(col: str, df: pd.DataFrame) -> bool:\n",
        "    \"\"\"Serve calcolarla? Solo se attesa nelle features e assente o tutta NaN.\"\"\"\n",
        "    if 'features_A' not in globals():\n",
        "        return False\n",
        "    if col not in features_A:\n",
        "        return False\n",
        "    if col not in df.columns:\n",
        "        return True\n",
        "    s = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "    return not s.notna().any()\n",
        "\n",
        "def _ensure_eval_derivatives(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "\n",
        "    # basi comode\n",
        "    size = pd.to_numeric(out.get(\"size_m2\"), errors=\"coerce\")\n",
        "    rooms = pd.to_numeric(out.get(\"rooms\"), errors=\"coerce\")\n",
        "    baths = pd.to_numeric(out.get(\"bathrooms\"), errors=\"coerce\")\n",
        "    floor = pd.to_numeric(out.get(\"floor\"), errors=\"coerce\")\n",
        "    elev  = pd.to_numeric(out.get(\"has_elevator\"), errors=\"coerce\").fillna(0)\n",
        "\n",
        "    # 1) derivate geometriche/funzionali\n",
        "    if _needs(\"log_size_m2\", out) and \"size_m2\" in out.columns:\n",
        "        out[\"log_size_m2\"] = np.log1p(size)\n",
        "\n",
        "    if _needs(\"sqm_per_room\", out) and {\"size_m2\",\"rooms\"}.issubset(out.columns):\n",
        "        out[\"sqm_per_room\"] = size / rooms.replace(0, np.nan)\n",
        "\n",
        "    if _needs(\"baths_per_100sqm\", out) and {\"bathrooms\",\"size_m2\"}.issubset(out.columns):\n",
        "        out[\"baths_per_100sqm\"] = 100.0 * baths / size.replace(0, np.nan)\n",
        "\n",
        "    if _needs(\"elev_x_floor\", out) and {\"has_elevator\",\"floor\"}.issubset(out.columns):\n",
        "        out[\"elev_x_floor\"] = (elev * np.maximum(floor - 1, 0)).astype(\"float64\")\n",
        "\n",
        "    if _needs(\"no_elev_high_floor\", out) and {\"has_elevator\",\"floor\"}.issubset(out.columns):\n",
        "        out[\"no_elev_high_floor\"] = ((1 - elev) * np.maximum(floor - 1, 0)).astype(\"float64\")\n",
        "\n",
        "    if _needs(\"rooms_per_100sqm\", out) and {\"rooms\",\"size_m2\"}.issubset(out.columns):\n",
        "        out[\"rooms_per_100sqm\"] = (100.0 * rooms / size.replace(0, np.nan)).astype(\"float64\")\n",
        "\n",
        "    # 2) priors city×zone e macroarea\n",
        "    if _needs(\"city_zone_prior\", out):\n",
        "        if \"city\" not in out.columns and \"location\" in out.columns:\n",
        "            out[\"city\"] = out[\"location\"]\n",
        "        if \"zone\" not in out.columns:\n",
        "            out[\"zone\"] = \"semi_center\"\n",
        "        ci = out.get(\"city\").astype(str).str.strip().str.lower() if \"city\" in out.columns else pd.Series(\"\", index=out.index)\n",
        "        zo = out.get(\"zone\").astype(str).str.strip().str.lower() if \"zone\" in out.columns else pd.Series(\"semi_center\", index=out.index)\n",
        "        vals = []\n",
        "        for c, z in zip(ci, zo):\n",
        "            v = _CITY_BASE.get(c, {}).get(z, np.nan)\n",
        "            if pd.isna(v):\n",
        "                v = _ZONE_MED.get(z, _GLOBAL_CITYZONE_MED)\n",
        "            vals.append(v)\n",
        "        out[\"city_zone_prior\"] = np.asarray(vals, dtype=\"float64\")\n",
        "\n",
        "    if _needs(\"region_index_prior\", out):\n",
        "        if \"region\" not in out.columns:\n",
        "            out[\"region\"] = \"center\"\n",
        "        out[\"region_index_prior\"] = out[\"region\"].astype(str).str.strip().str.lower().map(_REGION_INDEX).astype(\"float64\")\n",
        "\n",
        "    return out\n",
        "\n",
        "# applica solo se davvero servono (evita side-effect inutili)\n",
        "for _name in (\"df_train\",\"df_valid\",\"df_test\"):\n",
        "    if _name in globals() and isinstance(globals()[_name], pd.DataFrame):\n",
        "        globals()[_name] = _ensure_eval_derivatives(globals()[_name])\n",
        "\n",
        "# riallinea le matrici usate in queste celle di valutazione\n",
        "if \"features_A\" in globals():\n",
        "    X_train = _ensure_columns(df_train.copy(), features_A) if \"df_train\" in globals() else X_train\n",
        "    X_valid = _ensure_columns(df_valid.copy(), features_A) if \"df_valid\" in globals() else X_valid\n",
        "    X_test  = _ensure_columns(df_test.copy(),  features_A) if \"df_test\"  in globals() else X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0e4b3f3d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Leakage sentinel (VALID, shuffled y)  MAE=214.12  RMSE=293.42  R2=-0.0879\n",
            "Colonne proxy sospette in uso: —\n"
          ]
        }
      ],
      "source": [
        "# === Leakage sentinel + scan proxy (TTR, target in scala naturale) ===\n",
        "from __future__ import annotations\n",
        "import re, numpy as np\n",
        "from sklearn.base import clone\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "def _report(y_true, y_pred, tag: str):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    print(f\"{tag}  MAE={mae:.2f}  RMSE={rmse:.2f}  R2={r2:.4f}\")\n",
        "\n",
        "# y (scala naturale) per VALID — fallback se non definito\n",
        "try:\n",
        "    y_val_orig\n",
        "except NameError:\n",
        "    y_val_orig = df_valid.loc[X_valid.index, VALUATION_K].to_numpy(dtype=np.float64)\n",
        "\n",
        "# 1) Sentinel: shuffle del target in scala naturale\n",
        "y_train_shuf = shuffle(y_train, random_state=SEED).astype(np.float64)\n",
        "\n",
        "# ricostruisci estimatore coerente con la prep\n",
        "try:\n",
        "    base_model = clone(model_A)\n",
        "except Exception:\n",
        "    base_model = RandomForestRegressor(n_estimators=400, max_depth=None,\n",
        "                                       min_samples_leaf=2, random_state=SEED, n_jobs=-1)\n",
        "\n",
        "inner_pipe = Pipeline([(\"prep\", preproc_A), (\"model\", base_model)])\n",
        "\n",
        "def _log1p64(y):  return np.log1p(np.asarray(y, dtype=np.float64))\n",
        "def _expm164(y):  return np.expm1(np.asarray(y, dtype=np.float64))\n",
        "\n",
        "sentinel_reg = TransformedTargetRegressor(\n",
        "    regressor=inner_pipe,\n",
        "    func=_log1p64,\n",
        "    inverse_func=_expm164,\n",
        "    check_inverse=False,\n",
        ")\n",
        "\n",
        "fit_params = {}\n",
        "if \"w_train\" in globals() and isinstance(w_train, np.ndarray) and len(w_train) == len(X_train):\n",
        "    # ✅ chiave relativa alla Pipeline interna (step \"model\")\n",
        "    fit_params = {\"model__sample_weight\": w_train}\n",
        "\n",
        "sentinel_reg.fit(X_train, y_train_shuf, **fit_params)\n",
        "pred_val_shuf = np.clip(sentinel_reg.predict(X_valid), 0, None)\n",
        "_report(y_val_orig, pred_val_shuf, \"Leakage sentinel (VALID, shuffled y)\")\n",
        "\n",
        "# 2) Scan colonne proxy sospette nelle features in uso\n",
        "sus_patterns = [\n",
        "    r\"(?:^|_)(avg|mean|median|benchmark|zscore|rank|decile)(?:_|$).*?(price|valuation)\",\n",
        "    r\"(price|valuation).*?(avg|mean|median|benchmark|zscore|rank|decile)\",\n",
        "    r\"(?:^|_)drift(?:_|$)\", r\"(?:^|_)caps?(?:_|$)\", r\"(?:^|_)vs_(?:_|$)\"\n",
        "]\n",
        "def _is_susp(c: str) -> bool:\n",
        "    return any(re.search(p, c, re.I) for p in sus_patterns)\n",
        "\n",
        "in_use_cols = list(X_train.columns)\n",
        "print(\"Colonne proxy sospette in uso:\", [c for c in in_use_cols if _is_susp(c)] or \"—\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b5e232fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === PATCH CV: pre-trasformazione robusta + fast pipe clone-safe ==============\n",
        "from __future__ import annotations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import clone\n",
        "\n",
        "# 1) carica mapping priors (robusto)\n",
        "try:\n",
        "    from notebooks.shared.common.config import ASSET_CONFIG  # type: ignore\n",
        "    _PROP = ASSET_CONFIG[\"property\"]\n",
        "    _CITY_BASE = {c.lower(): {z.lower(): float(v) for z, v in d.items()}\n",
        "                  for c, d in (_PROP.get(\"city_base_prices\") or {}).items()}\n",
        "    _REGION_INDEX = {k.lower(): float(v) for k, v in (_PROP.get(\"region_index\") or {\n",
        "        \"north\": 1.05, \"center\": 1.00, \"south\": 0.92\n",
        "    }).items()}\n",
        "except Exception:\n",
        "    _CITY_BASE = {}\n",
        "    _REGION_INDEX = {\"north\": 1.05, \"center\": 1.00, \"south\": 0.92}\n",
        "\n",
        "# mediane di fallback\n",
        "_ZONE_KEYS = set(z for d in _CITY_BASE.values() for z in d.keys())\n",
        "_ZONE_MED = {z: float(np.nanmedian([d.get(z, np.nan) for d in _CITY_BASE.values()]))\n",
        "             for z in _ZONE_KEYS} if _CITY_BASE else {}\n",
        "_GLOBAL_CITYZONE_MED = float(np.nanmedian([v for d in _CITY_BASE.values() for v in d.values()])) if _CITY_BASE else 0.0\n",
        "\n",
        "def _required_cols_from_prep(prep: \"ColumnTransformer\") -> list[str]:\n",
        "    \"\"\"Colonne che il prep si aspetta in input (cat + num).\"\"\"\n",
        "    req = []\n",
        "    for name, _, cols in getattr(prep, \"transformers\", []):\n",
        "        if cols is None or cols == \"drop\":\n",
        "            continue\n",
        "        if isinstance(cols, (list, tuple, np.ndarray)):\n",
        "            req.extend([str(c) for c in cols])\n",
        "        else:\n",
        "            req.append(str(cols))\n",
        "    # dedup preservando ordine\n",
        "    return list(dict.fromkeys(req))\n",
        "\n",
        "def _ensure_columns(df_part: pd.DataFrame, required: list[str]) -> pd.DataFrame:\n",
        "    miss = [c for c in required if c not in df_part.columns]\n",
        "    if miss:\n",
        "        for c in miss:\n",
        "            df_part[c] = np.nan\n",
        "    return df_part[required]\n",
        "\n",
        "def _fill_priors_for_cv(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Garantisce city/zone/region canoniche e riempie city_zone_prior/region_index_prior con fallback.\"\"\"\n",
        "    out = df.copy()\n",
        "    # assicurati di avere i campi base\n",
        "    if \"city\" not in out.columns and \"location\" in out.columns:\n",
        "        out[\"city\"] = out[\"location\"]\n",
        "    if \"zone\" not in out.columns:\n",
        "        out[\"zone\"] = \"semi_center\"\n",
        "    if \"region\" not in out.columns:\n",
        "        out[\"region\"] = \"center\"\n",
        "\n",
        "    # canonizza\n",
        "    for c in (\"city\", \"zone\", \"region\"):\n",
        "        if c in out.columns:\n",
        "            out[c] = out[c].astype(str).str.strip().str.lower()\n",
        "\n",
        "    # city_zone_prior\n",
        "    if \"city_zone_prior\" not in out.columns or pd.isna(out[\"city_zone_prior\"]).all():\n",
        "        ci = out.get(\"city\", pd.Series(index=out.index, dtype=str)).astype(str)\n",
        "        zo = out.get(\"zone\", pd.Series(index=out.index, dtype=str)).astype(str)\n",
        "        vals = []\n",
        "        for c, z in zip(ci, zo):\n",
        "            v = _CITY_BASE.get(c, {}).get(z, np.nan)\n",
        "            if pd.isna(v):\n",
        "                v = _ZONE_MED.get(z, _GLOBAL_CITYZONE_MED)\n",
        "            vals.append(v)\n",
        "        out[\"city_zone_prior\"] = np.asarray(vals, dtype=\"float64\")\n",
        "    else:\n",
        "        out[\"city_zone_prior\"] = pd.to_numeric(out[\"city_zone_prior\"], errors=\"coerce\").astype(\"float64\")\n",
        "\n",
        "    # region_index_prior\n",
        "    if \"region_index_prior\" not in out.columns or pd.isna(out[\"region_index_prior\"]).all():\n",
        "        out[\"region_index_prior\"] = out[\"region\"].map(_REGION_INDEX).astype(\"float64\")\n",
        "    else:\n",
        "        out[\"region_index_prior\"] = pd.to_numeric(out[\"region_index_prior\"], errors=\"coerce\").astype(\"float64\")\n",
        "\n",
        "    # fallback finale se (rarissimo) ancora tutti NaN\n",
        "    if pd.isna(out[\"city_zone_prior\"]).all():\n",
        "        out[\"city_zone_prior\"] = float(_GLOBAL_CITYZONE_MED)\n",
        "    if pd.isna(out[\"region_index_prior\"]).all():\n",
        "        out[\"region_index_prior\"] = float(np.nanmean(list(_REGION_INDEX.values())))\n",
        "\n",
        "    return out\n",
        "\n",
        "def _build_fast_pipe_and_prefn(base_pipe: Pipeline):\n",
        "    \"\"\"\n",
        "    Ritorna (fast_pipe_senza_derive, pre_fn):\n",
        "      - fast_pipe: Pipeline(prep, rf|model) clone-safe\n",
        "      - pre_fn(X): applica derive.transform(X) se esiste, poi fill_priors e allinea alle colonne richieste dal prep\n",
        "    \"\"\"\n",
        "    steps = getattr(base_pipe, \"named_steps\", {})\n",
        "    prep = steps.get(\"prep\", None)\n",
        "    reg  = steps.get(\"rf\", steps.get(\"model\", None))\n",
        "    derive = steps.get(\"derive\", None)\n",
        "    if prep is None or reg is None:\n",
        "        raise RuntimeError(\"Pipeline base priva di 'prep' o step finale (rf/model).\")\n",
        "\n",
        "    try:\n",
        "        prep_fast = clone(prep)\n",
        "    except Exception:\n",
        "        prep_fast = prep\n",
        "    try:\n",
        "        reg_fast = clone(reg)\n",
        "    except Exception:\n",
        "        from sklearn.ensemble import RandomForestRegressor\n",
        "        reg_fast = RandomForestRegressor(n_estimators=400, random_state=SEED, n_jobs=-1, min_samples_leaf=2)\n",
        "\n",
        "    last_name = \"rf\" if \"rf\" in steps else (\"model\" if \"model\" in steps else \"est\")\n",
        "    fast_pipe = Pipeline([(\"prep\", prep_fast), (last_name, reg_fast)])\n",
        "\n",
        "    # colonne che il prep si aspetta\n",
        "    required = _required_cols_from_prep(prep)\n",
        "\n",
        "    def pre_fn(Xdf: pd.DataFrame) -> pd.DataFrame:\n",
        "        X2 = Xdf.copy()\n",
        "        # 1) derive fuori pipeline, se esiste\n",
        "        if derive is not None and hasattr(derive, \"transform\"):\n",
        "            X2 = derive.transform(X2)\n",
        "        # 2) assicurati che i prior esistano e NON siano NaN\n",
        "        X2 = _fill_priors_for_cv(X2)\n",
        "        # 3) allinea esattamente alle colonne attese dal prep\n",
        "        X2 = _ensure_columns(X2, required)\n",
        "        return X2\n",
        "\n",
        "    return fast_pipe, pre_fn\n",
        "\n",
        "# costruisci fast pipe + funzione di pre-trasformazione\n",
        "fast_pipe, _pre_fn = _build_fast_pipe_and_prefn(pipe_A)\n",
        "\n",
        "# pre-trasforma X per la CV\n",
        "X_train_cv = _pre_fn(X_train)\n",
        "# (se nella cella userai anche VALID/TEST, fai lo stesso)\n",
        "# X_valid_cv = _pre_fn(X_valid); X_test_cv = _pre_fn(X_test)\n",
        "\n",
        "# pesi (se presenti) già allineati a X_train\n",
        "w_full = None\n",
        "if \"sample_weight\" in df_train.columns:\n",
        "    w_full = df_train.loc[X_train.index, \"sample_weight\"].astype(\"float64\").to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "776b03be",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GroupShuffleSplit (5×, group=location) → MAE=72.00±31.67  RMSE=101.70±40.93  R2=0.8093±0.1128\n",
            "LOLO Top-10 (group=location) → MAE=85.44±66.00  RMSE=111.14±76.85  R2=0.7788±0.2078\n"
          ]
        }
      ],
      "source": [
        "# === Group-aware CV: GroupShuffleSplit 5× e LOLO Top-K (TTR, NO expm1 manuale) ===\n",
        "from __future__ import annotations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import GroupShuffleSplit, LeaveOneGroupOut\n",
        "from sklearn.base import clone\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "\n",
        "# 0) Wrapper numericamente stabili per TTR\n",
        "def _log1p64(y):  return np.log1p(np.asarray(y, dtype=np.float64))\n",
        "def _expm164(y):  return np.expm1(np.asarray(y, dtype=np.float64))\n",
        "\n",
        "# 1) Scegli colonna di gruppo\n",
        "GROUP_CANDIDATES = [\"location\", \"region\", \"zone\", \"urban_type\"]\n",
        "group_col = next((c for c in GROUP_CANDIDATES if c in df_train.columns), None)\n",
        "\n",
        "if group_col is None:\n",
        "    print(\"⛔ Nessuna colonna di gruppo disponibile (location/region/zone/urban_type). Salto GSS/LOLO.\")\n",
        "else:\n",
        "    # y_train DEVE essere in scala naturale (k€)\n",
        "    y_train_nat = np.asarray(y_train, dtype=np.float64)  # assicurati che y_train sia già in k€\n",
        "    groups_full = df_train.loc[X_train_cv.index, group_col].astype(str).to_numpy()\n",
        "\n",
        "    # helper: fit+metriche su uno split (usa TTR e passa pesi allo step finale della Pipeline)\n",
        "    def _fit_eval_on_split(X, y, tr_idx, va_idx, base_pipe, w: np.ndarray | None):\n",
        "        inner = clone(base_pipe)\n",
        "\n",
        "        # TTR: log1p/expm1 gestiti qui; nessuna expm1 manuale altrove\n",
        "        reg = TransformedTargetRegressor(\n",
        "            regressor=inner,\n",
        "            func=_log1p64,\n",
        "            inverse_func=_expm164,\n",
        "            check_inverse=False,\n",
        "        )\n",
        "\n",
        "        # individua nome step finale che accetta sample_weight\n",
        "        if \"rf\" in inner.named_steps:\n",
        "            last_step = \"rf\"\n",
        "        elif \"model\" in inner.named_steps:\n",
        "            last_step = \"model\"\n",
        "        else:\n",
        "            last_step = list(inner.named_steps.keys())[-1]\n",
        "\n",
        "        fit_params = {}\n",
        "        if w is not None and len(w) == len(X):\n",
        "            # ✅ chiavi relative alla Pipeline interna (niente 'regressor__')\n",
        "            fit_params = {f\"{last_step}__sample_weight\": w[tr_idx]}\n",
        "\n",
        "        reg.fit(X.iloc[tr_idx], y[tr_idx], **fit_params)\n",
        "        pred = np.clip(reg.predict(X.iloc[va_idx]), 0, None)  # già in k€\n",
        "        true = y[va_idx]\n",
        "\n",
        "        mae = mean_absolute_error(true, pred)\n",
        "        rmse = np.sqrt(mean_squared_error(true, pred))\n",
        "        r2 = r2_score(true, pred)\n",
        "        return mae, rmse, r2\n",
        "\n",
        "    if \"rf\" in fast_pipe.named_steps:\n",
        "        fast_pipe.named_steps[\"rf\"].set_params(n_estimators=200)\n",
        "\n",
        "    # pesi (se presenti) allineati a X_train\n",
        "    w_full = None\n",
        "    if \"sample_weight\" in df_train.columns:\n",
        "        w_full = df_train.loc[X_train_cv.index, \"sample_weight\"].astype(\"float64\").to_numpy()\n",
        "\n",
        "    # --- A) GroupShuffleSplit 5×\n",
        "    gss = GroupShuffleSplit(n_splits=5, test_size=0.2, random_state=SEED)\n",
        "    maeL, rmseL, r2L = [], [], []\n",
        "    for tr_idx, va_idx in gss.split(X_train_cv, y_train_nat, groups=groups_full):\n",
        "        mae, rmse, r2 = _fit_eval_on_split(X_train_cv, y_train_nat, tr_idx, va_idx, fast_pipe, w_full)\n",
        "        maeL.append(mae); rmseL.append(rmse); r2L.append(r2)\n",
        "\n",
        "    print(\n",
        "        f\"GroupShuffleSplit (5×, group={group_col}) → \"\n",
        "        f\"MAE={np.mean(maeL):.2f}±{np.std(maeL):.2f}  \"\n",
        "        f\"RMSE={np.mean(rmseL):.2f}±{np.std(rmseL):.2f}  \"\n",
        "        f\"R2={np.mean(r2L):.4f}±{np.std(r2L):.4f}\"\n",
        "    )\n",
        "\n",
        "    # --- B) LOLO Top-K (più rapido del LOLO completo)\n",
        "    K = 10\n",
        "    vc = pd.Series(groups_full).value_counts()\n",
        "    top_groups = vc.index[:min(K, len(vc))]\n",
        "    mask = pd.Series(groups_full).isin(top_groups).to_numpy()\n",
        "\n",
        "    Xk = X_train.loc[mask]\n",
        "    yk = y_train_nat[mask]\n",
        "    gk = pd.Series(groups_full)[mask].to_numpy()\n",
        "    wk = None if w_full is None else w_full[mask]\n",
        "\n",
        "    if len(np.unique(gk)) < 2 or len(Xk) < 10:\n",
        "        print(\"LOLO Top-K: gruppi insufficienti. Salto.\")\n",
        "    else:\n",
        "        logo = LeaveOneGroupOut()\n",
        "        maeL2, rmseL2, r2L2 = [], [], []\n",
        "        fast_pipe2 = clone(fast_pipe)\n",
        "\n",
        "        for tr_idx, va_idx in logo.split(Xk, yk, groups=gk):\n",
        "            mae, rmse, r2 = _fit_eval_on_split(Xk, yk, tr_idx, va_idx, fast_pipe2, wk)\n",
        "            maeL2.append(mae); rmseL2.append(rmse); r2L2.append(r2)\n",
        "\n",
        "        print(\n",
        "            f\"LOLO Top-{len(np.unique(gk))} (group={group_col}) → \"\n",
        "            f\"MAE={np.mean(maeL2):.2f}±{np.std(maeL2):.2f}  \"\n",
        "            f\"RMSE={np.mean(rmseL2):.2f}±{np.std(rmseL2):.2f}  \"\n",
        "            f\"R2={np.mean(r2L2):.4f}±{np.std(r2L2):.4f}\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "29ef33fc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Minimal VALID → MAE=65.78  RMSE=92.59  R2=0.891682  (features: 21)\n"
          ]
        }
      ],
      "source": [
        "# === Minimal features (TTR, chiavi fit_params corrette) ===\n",
        "from __future__ import annotations\n",
        "import numpy as np\n",
        "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# wrapper numericamente stabili\n",
        "def _log1p64(y):  return np.log1p(np.asarray(y, dtype=np.float64))\n",
        "def _expm164(y):  return np.expm1(np.asarray(y, dtype=np.float64))\n",
        "\n",
        "base_keep_all = [\n",
        "    \"size_m2\",\"rooms\",\"bathrooms\",\"year_built\",\"age_years\",\n",
        "    \"floor\",\"building_floors\",\"is_top_floor\",\"is_ground_floor\",\"has_elevator\",\n",
        "    \"garage\",\"parking_spot\",\"has_garden\",\"has_balcony\",\n",
        "    \"distance_to_center_km\",\"orientation\",\"view\",\"region\",\"zone\",\"urban_type\",\"location\"\n",
        "]\n",
        "base_keep = [c for c in base_keep_all if c in df_train.columns]\n",
        "\n",
        "if len(base_keep) < 2:\n",
        "    print(\"⛔ Minimal: meno di 2 feature base disponibili.\")\n",
        "else:\n",
        "    # split cat/num basati su dtype nel TRAIN\n",
        "    cat_b = [c for c in base_keep if df_train[c].dtype.name in (\"object\",\"category\",\"bool\")]\n",
        "    num_b = [c for c in base_keep if c not in set(cat_b)]\n",
        "\n",
        "    prep_b = ColumnTransformer([\n",
        "        (\"cat\", Pipeline([\n",
        "            (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "            (\"enc\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
        "        ]), cat_b),\n",
        "        (\"num\", Pipeline([\n",
        "            (\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "        ]), num_b),\n",
        "    ], remainder=\"drop\")\n",
        "\n",
        "    pipe_inner = Pipeline([\n",
        "        (\"prep\", prep_b),\n",
        "        (\"rf\", RandomForestRegressor(\n",
        "            n_estimators=400, random_state=SEED, n_jobs=-1, min_samples_leaf=2\n",
        "        )),\n",
        "    ])\n",
        "\n",
        "    # TTR: log1p/expm1 gestiti qui, niente expm1 manuale\n",
        "    pipe_min = TransformedTargetRegressor(\n",
        "        regressor=pipe_inner,\n",
        "        func=_log1p64, inverse_func=_expm164,\n",
        "        check_inverse=False,\n",
        "    )\n",
        "\n",
        "    # allinea colonne\n",
        "    def _ensure(df_part, cols):\n",
        "        miss = [c for c in cols if c not in df_part.columns]\n",
        "        if miss:\n",
        "            for c in miss: df_part[c] = np.nan\n",
        "        return df_part[cols]\n",
        "\n",
        "    Xtr_min = _ensure(df_train.copy(), base_keep).loc[X_train.index]\n",
        "    Xva_min = _ensure(df_valid.copy(), base_keep).loc[X_valid.index]\n",
        "\n",
        "    # y in scala naturale (k€)\n",
        "    y_tr_nat = df_train.loc[Xtr_min.index, VALUATION_K].astype(\"float64\").to_numpy()\n",
        "    y_va_nat = df_valid.loc[Xva_min.index, VALUATION_K].astype(\"float64\").to_numpy()\n",
        "\n",
        "    # fit_params: **niente 'regressor__'**\n",
        "    fit_params = {}\n",
        "    if \"sample_weight\" in df_train.columns:\n",
        "        w = df_train.loc[Xtr_min.index, \"sample_weight\"].astype(\"float64\").to_numpy()\n",
        "        fit_params = {\"rf__sample_weight\": w}\n",
        "\n",
        "    pipe_min.fit(Xtr_min, y_tr_nat, **fit_params)\n",
        "    pred_min = np.clip(pipe_min.predict(Xva_min), 0, None)  # già k€\n",
        "\n",
        "    mae = mean_absolute_error(y_va_nat, pred_min)\n",
        "    rmse = np.sqrt(mean_squared_error(y_va_nat, pred_min))\n",
        "    r2 = r2_score(y_va_nat, pred_min)\n",
        "\n",
        "    print(f\"Minimal VALID → MAE={mae:.2f}  RMSE={rmse:.2f}  R2={r2:.6f}  (features: {len(base_keep)})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2ebecbf5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DROP ['distance_to_center_km', 'size_m2'] → VALID  MAE=45.28  RMSE=66.13  R2=0.9447  (kept 26 cols)\n",
            "DROP ['size_m2'] → VALID  MAE=45.28  RMSE=66.13  R2=0.9447  (kept 26 cols)\n",
            "DROP ['distance_to_center_km'] → VALID  MAE=45.32  RMSE=66.19  R2=0.9446  (kept 27 cols)\n",
            "GSS 3× (drop size_m2 & distance_to_center_km) → MAE=52.44±13.47  RMSE=75.60±23.56  R2=0.8891±0.0396\n"
          ]
        }
      ],
      "source": [
        "# === Ablation sicura: rimuovi alcune colonne e valuta su VALID (e opz. GSS) ===\n",
        "from __future__ import annotations\n",
        "import numpy as np\n",
        "from sklearn.base import clone\n",
        "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "# -- wrapper numericamente stabili (se non già definiti)\n",
        "try:\n",
        "    _log1p64\n",
        "    _expm164\n",
        "except NameError:\n",
        "    def _log1p64(y):  return np.log1p(np.asarray(y, dtype=np.float64))\n",
        "    def _expm164(y):  return np.expm1(np.asarray(y, dtype=np.float64))\n",
        "\n",
        "def _build_preproc_from(base_prep: ColumnTransformer,\n",
        "                        keep_cat: list[str],\n",
        "                        keep_num: list[str]) -> ColumnTransformer:\n",
        "    \"\"\"\n",
        "    Ricostruisce un ColumnTransformer usando (se disponibili) i trasformatori\n",
        "    del prep di base, altrimenti crea pipe di default.\n",
        "    \"\"\"\n",
        "    cat_est, num_est = None, None\n",
        "    if hasattr(base_prep, \"transformers\"):\n",
        "        for name, est, cols in base_prep.transformers:\n",
        "            if name == \"cat\":\n",
        "                cat_est = clone(est)\n",
        "            elif name == \"num\":\n",
        "                num_est = clone(est)\n",
        "\n",
        "    if cat_est is None:\n",
        "        cat_est = Pipeline([\n",
        "            (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "            (\"encode\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
        "        ])\n",
        "    if num_est is None:\n",
        "        num_est = Pipeline([\n",
        "            (\"impute\", SimpleImputer(strategy=\"median\")),\n",
        "        ])\n",
        "\n",
        "    transformers = []\n",
        "    if keep_cat:\n",
        "        transformers.append((\"cat\", cat_est, keep_cat))\n",
        "    if keep_num:\n",
        "        transformers.append((\"num\", num_est, keep_num))\n",
        "    if not transformers:\n",
        "        raise ValueError(\"Nessuna feature rimanente per l'ablation.\")\n",
        "\n",
        "    return ColumnTransformer(transformers=transformers, remainder=\"drop\")\n",
        "\n",
        "def _ensure(df_part, cols):\n",
        "    \"\"\"Garantisce che tutte le colonne esistano (aggiunge NaN se mancano) e restituisce solo quelle in 'cols'.\"\"\"\n",
        "    miss = [c for c in cols if c not in df_part.columns]\n",
        "    if miss:\n",
        "        for c in miss: df_part[c] = np.nan\n",
        "    return df_part[cols]\n",
        "\n",
        "def _make_fit_params_for_ttr_regressor(pipe_base: Pipeline, mask=None):\n",
        "    \"\"\"\n",
        "    Crea il dict dei fit_params per passare i pesi allo step finale della Pipeline interna.\n",
        "    NOTA: con TTR NON usare 'regressor__' qui; le chiavi vanno direttamente allo step (rf/model).\n",
        "    \"\"\"\n",
        "    if (\"sample_weight\" in df_train.columns) and hasattr(pipe_base, \"named_steps\"):\n",
        "        if \"rf\" in pipe_base.named_steps:\n",
        "            key = \"rf__sample_weight\"\n",
        "        elif \"model\" in pipe_base.named_steps:\n",
        "            key = \"model__sample_weight\"\n",
        "        else:\n",
        "            last = list(pipe_base.named_steps.keys())[-1]\n",
        "            key = f\"{last}__sample_weight\"\n",
        "\n",
        "        w = df_train.loc[X_train.index, \"sample_weight\"].astype(\"float64\").to_numpy()\n",
        "        if mask is not None:\n",
        "            w = w[mask]\n",
        "        return {key: w}\n",
        "    return {}\n",
        "\n",
        "def eval_drop(cols_to_drop, base_pipe=pipe_A):\n",
        "    \"\"\"\n",
        "    Esegue un’ablation (drop di alcune colonne) e valuta su VALID usando TTR (log1p/expm1).\n",
        "    Ritorna (ttr_pipeline, keep_features).\n",
        "    \"\"\"\n",
        "    cols_to_drop = set(cols_to_drop)\n",
        "    keep = [c for c in features_A if c not in cols_to_drop]\n",
        "    keep_cat = [c for c in cat_cols if c in keep]\n",
        "    keep_num = [c for c in num_cols if c in keep]\n",
        "\n",
        "    # ricostruisci preproc coerente\n",
        "    preproc_new = _build_preproc_from(base_pipe.named_steps[\"prep\"], keep_cat, keep_num)\n",
        "\n",
        "    # modello base\n",
        "    rf = RandomForestRegressor(\n",
        "        n_estimators=400, random_state=SEED, n_jobs=-1, max_depth=None, min_samples_leaf=2\n",
        "    )\n",
        "    pipe_base = Pipeline([(\"prep\", preproc_new), (\"rf\", rf)])\n",
        "\n",
        "    # TTR per gestire scala target in modo consistente\n",
        "    ttr = TransformedTargetRegressor(\n",
        "        regressor=pipe_base,\n",
        "        func=_log1p64,\n",
        "        inverse_func=_expm164,\n",
        "        check_inverse=False,\n",
        "    )\n",
        "\n",
        "    # prepara matrici e target (scala naturale k€)\n",
        "    Xtr = _ensure(df_train.copy(), keep).loc[X_train.index]\n",
        "    Xva = _ensure(df_valid.copy(), keep).loc[X_valid.index]\n",
        "    y_tr_nat = df_train.loc[Xtr.index, VALUATION_K].astype(\"float64\").to_numpy()\n",
        "    y_va_nat = df_valid.loc[Xva.index, VALUATION_K].astype(\"float64\").to_numpy()\n",
        "\n",
        "    # fit con pesi (se presenti)\n",
        "    fit_params = _make_fit_params_for_ttr_regressor(pipe_base)\n",
        "    ttr.fit(Xtr, y_tr_nat, **fit_params)\n",
        "\n",
        "    # predizioni già in scala naturale\n",
        "    predV = np.clip(ttr.predict(Xva), 0, None)\n",
        "    mae = mean_absolute_error(y_va_nat, predV)\n",
        "    rmse = np.sqrt(mean_squared_error(y_va_nat, predV))\n",
        "    r2 = r2_score(y_va_nat, predV)\n",
        "\n",
        "    print(f\"DROP {sorted(cols_to_drop)} → VALID  MAE={mae:.2f}  RMSE={rmse:.2f}  R2={r2:.4f}  (kept {len(keep)} cols)\")\n",
        "    return ttr, keep\n",
        "\n",
        "# Esempi di uso (come prima)\n",
        "p_drop_both, keep_cols_both = eval_drop([\"size_m2\", \"distance_to_center_km\"])\n",
        "p_drop_size, keep_cols_size = eval_drop([\"size_m2\"])\n",
        "p_drop_dist, keep_cols_dist = eval_drop([\"distance_to_center_km\"])\n",
        "\n",
        "# --- GSS rapido con set ridotto (3 split) per il modello p_drop_both ---\n",
        "GROUP_CANDIDATES = [\"location\",\"region\",\"zone\",\"urban_type\"]\n",
        "group_col = next((c for c in GROUP_CANDIDATES if c in df_train.columns), None)\n",
        "\n",
        "if group_col:\n",
        "    # gruppi allineati a X_train\n",
        "    groups = df_train.loc[X_train.index, group_col].astype(str).to_numpy()\n",
        "\n",
        "    # ricostruisci una versione \"fast\" della pipeline interna con meno alberi\n",
        "    fast_inner = clone(p_drop_both.regressor)   # Pipeline(prep, rf)\n",
        "    if \"rf\" in fast_inner.named_steps:\n",
        "        fast_inner.named_steps[\"rf\"].set_params(n_estimators=200)\n",
        "\n",
        "    # TTR fast\n",
        "    ttr_fast = TransformedTargetRegressor(\n",
        "        regressor=fast_inner,\n",
        "        func=_log1p64,\n",
        "        inverse_func=_expm164,\n",
        "        check_inverse=False,\n",
        "    )\n",
        "\n",
        "    # prepara X e y in scala naturale per i fold\n",
        "    X_full = _ensure(df_train.copy(), keep_cols_both).loc[X_train.index]\n",
        "    y_full = df_train.loc[X_train.index, VALUATION_K].astype(\"float64\").to_numpy()\n",
        "\n",
        "    # pesi allineati se presenti\n",
        "    w_full = df_train.loc[X_train.index, \"sample_weight\"].astype(\"float64\").to_numpy() if \"sample_weight\" in df_train.columns else None\n",
        "\n",
        "    gss = GroupShuffleSplit(n_splits=3, test_size=0.2, random_state=SEED)\n",
        "    maeL, rmseL, r2L = [], [], []\n",
        "\n",
        "    for tr, va in gss.split(X_full, y_full, groups=groups):\n",
        "        # fit params per questo split (NB: chiavi relative alla Pipeline interna del TTR)\n",
        "        fit_params = {}\n",
        "        if w_full is not None:\n",
        "            # passali allo step 'rf' dentro regressor\n",
        "            fit_params = {\"rf__sample_weight\": w_full[tr]}\n",
        "\n",
        "        q = clone(ttr_fast)\n",
        "        q.fit(X_full.iloc[tr], y_full[tr], **fit_params)\n",
        "        pred = np.clip(q.predict(X_full.iloc[va]), 0, None)  # già in k€\n",
        "        true = y_full[va]\n",
        "        maeL.append(mean_absolute_error(true, pred))\n",
        "        rmseL.append(np.sqrt(mean_squared_error(true, pred)))\n",
        "        r2L.append(r2_score(true, pred))\n",
        "\n",
        "    print(\n",
        "        f\"GSS 3× (drop size_m2 & distance_to_center_km) → \"\n",
        "        f\"MAE={np.mean(maeL):.2f}±{np.std(maeL):.2f}  \"\n",
        "        f\"RMSE={np.mean(rmseL):.2f}±{np.std(rmseL):.2f}  \"\n",
        "        f\"R2={np.mean(r2L):.4f}±{np.std(r2L):.4f}\"\n",
        "    )\n",
        "else:\n",
        "    print(\"GSS: nessuna colonna di gruppo disponibile.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f10dc080",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GSS 5× (drop distance_to_center_km) → MAE=68.13±19.63  RMSE=98.76±27.06  R2=0.8259±0.0585\n"
          ]
        }
      ],
      "source": [
        "# === GSS 5× senza distance_to_center_km (robusto ai nomi step + TTR + pesi) ===\n",
        "from __future__ import annotations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.base import clone\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# wrapper numericamente stabili per TTR\n",
        "def _log1p64(y):  return np.log1p(np.asarray(y, dtype=np.float64))\n",
        "def _expm164(y):  return np.expm1(np.asarray(y, dtype=np.float64))\n",
        "\n",
        "def _extract_base_regressor(base_pipe) -> RandomForestRegressor:\n",
        "    \"\"\"Prova a clonare lo step finale del tuo pipe (rf/model), altrimenti crea un RF.\"\"\"\n",
        "    try:\n",
        "        steps = getattr(base_pipe, \"named_steps\", {})\n",
        "        if \"rf\" in steps:\n",
        "            return clone(steps[\"rf\"])\n",
        "        if \"model\" in steps:\n",
        "            return clone(steps[\"model\"])\n",
        "    except Exception:\n",
        "        pass\n",
        "    return RandomForestRegressor(\n",
        "        n_estimators=400, random_state=SEED, n_jobs=-1, min_samples_leaf=2\n",
        "    )\n",
        "\n",
        "def _fit_params_key_for(pipe_inner: Pipeline) -> str:\n",
        "    \"\"\"Ritorna la chiave corretta per passare sample_weight allo step finale.\"\"\"\n",
        "    if \"rf\" in pipe_inner.named_steps:\n",
        "        return \"rf__sample_weight\"\n",
        "    if \"model\" in pipe_inner.named_steps:\n",
        "        return \"model__sample_weight\"\n",
        "    last = list(pipe_inner.named_steps.keys())[-1]\n",
        "    return f\"{last}__sample_weight\"\n",
        "\n",
        "drop_col = \"distance_to_center_km\"\n",
        "keep_cols = [c for c in features_A if c != drop_col]\n",
        "keep_cat  = [c for c in cat_cols if c in keep_cols]\n",
        "keep_num  = [c for c in num_cols if c in keep_cols]\n",
        "\n",
        "# prep minimale per le colonne mantenute\n",
        "cat_est = Pipeline([\n",
        "    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"encode\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
        "])\n",
        "num_est = Pipeline([(\"impute\", SimpleImputer(strategy=\"median\"))])\n",
        "prep = ColumnTransformer([(\"cat\", cat_est, keep_cat), (\"num\", num_est, keep_num)], remainder=\"drop\")\n",
        "\n",
        "# regressor di base coerente col tuo pipe_A (o RF di default)\n",
        "base_reg = _extract_base_regressor(pipe_A)\n",
        "\n",
        "# pipeline interna + TTR (gestione log1p/expm1)\n",
        "inner = Pipeline([(\"prep\", prep), (\"rf\", base_reg)])\n",
        "ttr_template = TransformedTargetRegressor(\n",
        "    regressor=inner, func=_log1p64, inverse_func=_expm164, check_inverse=False\n",
        ")\n",
        "\n",
        "# allinea X, y (scala naturale, k€), pesi e gruppi agli indici di X_train\n",
        "X_keep = X_train[keep_cols]\n",
        "y_nat  = df_train.loc[X_keep.index, VALUATION_K].astype(\"float64\").to_numpy()\n",
        "w_full = None\n",
        "if \"sample_weight\" in df_train.columns:\n",
        "    w_full = df_train.loc[X_keep.index, \"sample_weight\"].astype(\"float64\").to_numpy()\n",
        "\n",
        "group_col = next((c for c in [\"location\",\"region\",\"zone\",\"urban_type\"] if c in df_train.columns), None)\n",
        "if group_col is None:\n",
        "    raise RuntimeError(\"Nessuna colonna di gruppo disponibile (location/region/zone/urban_type).\")\n",
        "groups = df_train.loc[X_keep.index, group_col].astype(str).to_numpy()\n",
        "\n",
        "# GSS 5×\n",
        "gss = GroupShuffleSplit(n_splits=5, test_size=0.2, random_state=SEED)\n",
        "maeL, rmseL, r2L = [], [], []\n",
        "\n",
        "# chiave corretta per i pesi\n",
        "fit_key = _fit_params_key_for(inner)\n",
        "\n",
        "for tr, va in gss.split(X_keep, y_nat, groups=groups):\n",
        "    ttr = clone(ttr_template)\n",
        "\n",
        "    fit_params = {}\n",
        "    if w_full is not None:\n",
        "        # ⚠️ con TTR le chiavi vanno DIRETTE allo step della pipeline interna (niente 'regressor__')\n",
        "        fit_params = {fit_key: w_full[tr]}\n",
        "\n",
        "    ttr.fit(X_keep.iloc[tr], y_nat[tr], **fit_params)\n",
        "    pred = np.clip(ttr.predict(X_keep.iloc[va]), 0, None)  # già k€\n",
        "    true = y_nat[va]\n",
        "\n",
        "    maeL.append(mean_absolute_error(true, pred))\n",
        "    rmseL.append(np.sqrt(mean_squared_error(true, pred)))\n",
        "    r2L.append(r2_score(true, pred))\n",
        "\n",
        "print(\n",
        "    f\"GSS 5× (drop {drop_col}) → \"\n",
        "    f\"MAE={np.mean(maeL):.2f}±{np.std(maeL):.2f}  \"\n",
        "    f\"RMSE={np.mean(rmseL):.2f}±{np.std(rmseL):.2f}  \"\n",
        "    f\"R2={np.mean(r2L):.4f}±{np.std(r2L):.4f}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9fdc4b9-98c5-43ac-9804-13c954b02c63",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Train & Validation (A vs B) & Champion Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "91aed31d-f657-4b2e-a37a-5bf3a588b6f3",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-08-21 08:53:59,525] INFO model_trainer: Saved RF baselines summary → c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\artifacts\\rf_baselines_summary.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A (conf as feature)  VALID: {'MAE': 45.294063643960946, 'RMSE': 66.16886996264304, 'R2': 0.9446765748630931}  TEST: {'MAE': 47.65214781991189, 'RMSE': 69.71697330889604, 'R2': 0.9409611142972081}   (fit 15.79s)\n",
            "B (conf as weight)   VALID: {'MAE': 45.31988964624947, 'RMSE': 66.18888392191661, 'R2': 0.9446431026810659}  TEST: {'MAE': 47.625011881254494, 'RMSE': 69.7239915623542, 'R2': 0.9409492270709008}   (fit 15.85s)\n",
            "Champion: A\n"
          ]
        }
      ],
      "source": [
        "# 06) Fit & validation (RF baseline A/B) con TTR, pesi corretti e metriche robuste — SAFE\n",
        "\n",
        "from __future__ import annotations\n",
        "import time, json, numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "def _metrics(y_true, y_pred):\n",
        "    mae  = float(mean_absolute_error(y_true, y_pred))\n",
        "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "    r2   = float(r2_score(y_true, y_pred))\n",
        "    return {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}\n",
        "\n",
        "# --- 0) Sanitize liste features: uniche e disgiunte\n",
        "def _uniq(xs):\n",
        "    return list(dict.fromkeys(xs))\n",
        "\n",
        "cat_cols = _uniq(cat_cols)\n",
        "_num_all = _uniq(num_cols)\n",
        "num_cols = [c for c in _num_all if c not in set(cat_cols)]\n",
        "num_cols_B = [c for c in num_cols if c != \"confidence_score\"]  # B: niente confidence come feature\n",
        "\n",
        "_inter = set(cat_cols) & set(num_cols)\n",
        "if _inter:\n",
        "    logger.warning(\"Colonne presenti sia in cat che num (rimosse da num): %s\", sorted(_inter))\n",
        "\n",
        "# --- 1) Preprocessori con le liste pulite\n",
        "cat_pipe = Pipeline([\n",
        "    (\"impute\", SimpleImputer(strategy=\"constant\", fill_value=\"__MISSING__\")),\n",
        "    (\"encode\", _build_ohe(min_freq)),\n",
        "])\n",
        "num_pipe = Pipeline([\n",
        "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
        "])\n",
        "\n",
        "preproc_A = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", cat_pipe, cat_cols),\n",
        "        (\"num\", num_pipe, num_cols),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        ")\n",
        "preproc_B = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", cat_pipe, cat_cols),\n",
        "        (\"num\", num_pipe, num_cols_B),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        ")\n",
        "\n",
        "# --- 2) Reallinea le colonne tra split (se mancano, NaN → imputazione)\n",
        "def _ensure_columns(df_part, required):\n",
        "    missing = [c for c in required if c not in df_part.columns]\n",
        "    if missing:\n",
        "        for c in missing:\n",
        "            df_part[c] = np.nan\n",
        "        logger.info(\"Aggiunte colonne mancanti allo split: %s\", missing)\n",
        "    return df_part[required]\n",
        "\n",
        "features_A = cat_cols + num_cols\n",
        "features_B = cat_cols + num_cols_B\n",
        "\n",
        "df_train_A = _ensure_columns(df_train.copy(), features_A)\n",
        "df_valid_A = _ensure_columns(df_valid.copy(), features_A)\n",
        "df_test_A  = _ensure_columns(df_test.copy(),  features_A)\n",
        "\n",
        "df_train_B = _ensure_columns(df_train.copy(), features_B)\n",
        "df_valid_B = _ensure_columns(df_valid.copy(), features_B)\n",
        "df_test_B  = _ensure_columns(df_test.copy(),  features_B)\n",
        "\n",
        "X_train = df_train_A[features_A].copy()\n",
        "X_valid = df_valid_A[features_A].copy()\n",
        "X_test  = df_test_A[features_A].copy()\n",
        "\n",
        "Xtr_B = df_train_B[features_B].copy()\n",
        "Xva_B = df_valid_B[features_B].copy()\n",
        "Xte_B = df_test_B[features_B].copy()\n",
        "\n",
        "# --- 3) Target in SCALA NATURALE (k€) — TTR farà log1p/expm1\n",
        "y_train_nat = df_train[VALUATION_K].astype(\"float64\").to_numpy()\n",
        "y_val_nat   = df_valid[VALUATION_K].astype(\"float64\").to_numpy()\n",
        "y_tst_nat   = df_test[VALUATION_K].astype(\"float64\").to_numpy()\n",
        "\n",
        "# --- 4) Modelli di base\n",
        "model_A = RandomForestRegressor(\n",
        "    n_estimators=400, random_state=SEED, n_jobs=-1, max_depth=None, min_samples_leaf=2\n",
        ")\n",
        "model_B = RandomForestRegressor(\n",
        "    n_estimators=400, random_state=SEED, n_jobs=-1, max_depth=None, min_samples_leaf=2\n",
        ")\n",
        "\n",
        "# Pipeline interne con step finale chiamato \"model\"\n",
        "pipe_A_inner = Pipeline([(\"prep\", preproc_A), (\"model\", model_A)])\n",
        "pipe_B_inner = Pipeline([(\"prep\", preproc_B), (\"model\", model_B)])\n",
        "\n",
        "# TTR per applicare log1p/expm1 in modo consistente\n",
        "ttr_A = TransformedTargetRegressor(regressor=pipe_A_inner, func=np.log1p, inverse_func=np.expm1)\n",
        "ttr_B = TransformedTargetRegressor(regressor=pipe_B_inner, func=np.log1p, inverse_func=np.expm1)\n",
        "\n",
        "# --- 5) Fit A (confidence come feature, nessun peso)\n",
        "t0 = time.perf_counter()\n",
        "ttr_A.fit(X_train, y_train_nat)\n",
        "tA = time.perf_counter() - t0\n",
        "\n",
        "pred_val_A = np.clip(ttr_A.predict(X_valid), 0, None)\n",
        "pred_tst_A = np.clip(ttr_A.predict(X_test),  0, None)\n",
        "\n",
        "# --- 6) Fit B (confidence esclusa come feature, usata come PESO se presente)\n",
        "fit_params_B = {}\n",
        "if \"sample_weight\" in df_train.columns:\n",
        "    w_train = df_train.loc[Xtr_B.index, \"sample_weight\"].astype(\"float64\").to_numpy()\n",
        "    # ✅ con TTR, i parametri vanno all'interno della pipeline: regressor__model__sample_weight\n",
        "    fit_params_B = {\"model__sample_weight\": w_train}  # <-- chiave relativa alla Pipeline interna\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "ttr_B.fit(Xtr_B, y_train_nat, **fit_params_B)\n",
        "tB = time.perf_counter() - t0\n",
        "\n",
        "pred_val_B = np.clip(ttr_B.predict(Xva_B), 0, None)\n",
        "pred_tst_B = np.clip(ttr_B.predict(Xte_B), 0, None)\n",
        "\n",
        "# --- 7) Metriche su scala naturale\n",
        "mA_val = _metrics(y_val_nat, pred_val_A)\n",
        "mA_tst = _metrics(y_tst_nat, pred_tst_A)\n",
        "mB_val = _metrics(y_val_nat, pred_val_B)\n",
        "mB_tst = _metrics(y_tst_nat, pred_tst_B)\n",
        "\n",
        "print(f\"A (conf as feature)  VALID: {mA_val}  TEST: {mA_tst}   (fit {tA:.2f}s)\")\n",
        "print(f\"B (conf as weight)   VALID: {mB_val}  TEST: {mB_tst}   (fit {tB:.2f}s)\")\n",
        "\n",
        "# --- 8) Selezione champion su VALID (MAE, tie-break RMSE)\n",
        "def _champ(mA, mB):\n",
        "    if mA[\"MAE\"] < mB[\"MAE\"]:\n",
        "        return \"A\"\n",
        "    if mA[\"MAE\"] > mB[\"MAE\"]:\n",
        "        return \"B\"\n",
        "    return \"A\" if mA[\"RMSE\"] <= mB[\"RMSE\"] else \"B\"\n",
        "\n",
        "champion = _champ(mA_val, mB_val)\n",
        "print(\"Champion:\", champion)\n",
        "\n",
        "# --- 9) Salva mini-report\n",
        "summary = {\n",
        "    \"timing_sec\": {\"A\": round(tA, 3), \"B\": round(tB, 3)},\n",
        "    \"A\": {\"VALID\": mA_val, \"TEST\": mA_tst, \"n_features\": len(features_A)},\n",
        "    \"B\": {\"VALID\": mB_val, \"TEST\": mB_tst, \"n_features\": len(features_B)},\n",
        "    \"champion\": champion,\n",
        "}\n",
        "(ART_DIR / \"rf_baselines_summary.json\").write_text(\n",
        "    canonical_json_dumps(summary), encoding=\"utf-8\"\n",
        ")\n",
        "logger.info(\"Saved RF baselines summary → %s\", ART_DIR / \"rf_baselines_summary.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ad3bab1",
      "metadata": {},
      "source": [
        "### (RF A/B): significatività & blending"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "53daff85",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ΔMAE (A−B) bootstrap: mean=0.0263, 95% CI=(-0.0349,0.0862), p≈0.422\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_4584\\1057882324.py:76: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda g: pd.Series({\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\artifacts\\ab_compare_by_decile.csv , c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\artifacts\\ab_compare_by_decile.parquet\n",
            "Saved: c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\artifacts\\blend_search_AB.csv , c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\artifacts\\blend_search_AB.parquet\n",
            "Blend best α on VALID = 0.00  |  MAE_VALID=45.294  |  MAE_TEST=47.652  |  R2_TEST=0.9410\n"
          ]
        }
      ],
      "source": [
        "# 06.1) RF A vs B: bootstrap ΔMAE + breakdown per decile + blending VALID→TEST (robusto)\n",
        "from __future__ import annotations\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# === Guard-rails\n",
        "for name in [\"pred_val_A\", \"pred_val_B\", \"pred_tst_A\", \"pred_tst_B\"]:\n",
        "    if name not in globals():\n",
        "        raise RuntimeError(f\"Variabile mancante: {name}\")\n",
        "\n",
        "if len(df_test) == 0 or len(df_valid) == 0:\n",
        "    raise RuntimeError(\"Split vuoti: df_valid/df_test non possono essere vuoti.\")\n",
        "\n",
        "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _as_1d_float(x):\n",
        "    a = np.asarray(x, dtype=\"float64\").reshape(-1)\n",
        "    # sostituisci inf con nan e poi imputiamo con mediana\n",
        "    a[~np.isfinite(a)] = np.nan\n",
        "    if np.isnan(a).any():\n",
        "        med = np.nanmedian(a)\n",
        "        a = np.where(np.isnan(a), med, a)\n",
        "    return a\n",
        "\n",
        "# === Array puliti (TEST)\n",
        "y_true_t = _as_1d_float(df_test[VALUATION_K].to_numpy())\n",
        "yhatA_t  = _as_1d_float(pred_tst_A)\n",
        "yhatB_t  = _as_1d_float(pred_tst_B)\n",
        "\n",
        "# riallineo lunghezze se necessario (difetti estremi)\n",
        "n = min(len(y_true_t), len(yhatA_t), len(yhatB_t))\n",
        "y_true_t, yhatA_t, yhatB_t = y_true_t[:n], yhatA_t[:n], yhatB_t[:n]\n",
        "\n",
        "# === Paired bootstrap ΔMAE su TEST (A−B)\n",
        "B = 1000\n",
        "rng_boot = np.random.default_rng(SEED + 101)\n",
        "idx_mat = rng_boot.integers(0, n, size=(B, n))\n",
        "\n",
        "def _mae(a, b):  # veloce e robusto\n",
        "    return np.mean(np.abs(a - b))\n",
        "\n",
        "deltas = np.fromiter(\n",
        "    (_mae(y_true_t[idx], yhatA_t[idx]) - _mae(y_true_t[idx], yhatB_t[idx]) for idx in idx_mat),\n",
        "    dtype=\"float64\", count=B\n",
        ")\n",
        "\n",
        "ci_lo, ci_hi = np.percentile(deltas, [2.5, 97.5])\n",
        "# p-value 2 code rispetto a 0\n",
        "p_two_sided = 2.0 * min((deltas <= 0).mean(), (deltas >= 0).mean())\n",
        "\n",
        "print(f\"ΔMAE (A−B) bootstrap: mean={deltas.mean():.4f}, 95% CI=({ci_lo:.4f},{ci_hi:.4f}), p≈{p_two_sided:.3f}\")\n",
        "\n",
        "ab_summary = {\n",
        "    \"delta_mae_mean\": float(deltas.mean()),\n",
        "    \"ci_95\": [float(ci_lo), float(ci_hi)],\n",
        "    \"p_two_sided\": float(p_two_sided),\n",
        "    \"B\": int(B),\n",
        "    \"n_test\": int(n),\n",
        "    \"mae_A_test\": float(_mae(y_true_t, yhatA_t)),\n",
        "    \"mae_B_test\": float(_mae(y_true_t, yhatB_t)),\n",
        "}\n",
        "(ART_DIR / \"ab_bootstrap_summary.json\").write_text(canonical_json_dumps(ab_summary), encoding=\"utf-8\")\n",
        "\n",
        "# === Breakdown per decile (TEST)\n",
        "try:\n",
        "    dec = pd.qcut(y_true_t, q=10, labels=False, duplicates=\"drop\")\n",
        "except Exception:\n",
        "    dec = pd.Series(np.zeros_like(y_true_t, dtype=int))\n",
        "\n",
        "rep = (\n",
        "    pd.DataFrame({\"y\": y_true_t, \"yA\": yhatA_t, \"yB\": yhatB_t, \"dec\": dec})\n",
        "    .groupby(\"dec\", observed=True, sort=True)\n",
        "    .apply(lambda g: pd.Series({\n",
        "        \"n\": int(len(g)),\n",
        "        \"MAE_A\": float(mean_absolute_error(g[\"y\"], g[\"yA\"])),\n",
        "        \"MAE_B\": float(mean_absolute_error(g[\"y\"], g[\"yB\"])),\n",
        "        \"ΔMAE_AminusB\": float(mean_absolute_error(g[\"y\"], g[\"yA\"]) - mean_absolute_error(g[\"y\"], g[\"yB\"])),\n",
        "        \"R2_A\": float(r2_score(g[\"y\"], g[\"yA\"])) if g[\"y\"].nunique() > 1 else np.nan,\n",
        "        \"R2_B\": float(r2_score(g[\"y\"], g[\"yB\"])) if g[\"y\"].nunique() > 1 else np.nan,\n",
        "    }))\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "rep_csv  = ART_DIR / \"ab_compare_by_decile.csv\"\n",
        "rep_parq = ART_DIR / \"ab_compare_by_decile.parquet\"\n",
        "rep.to_csv(rep_csv, index=False)\n",
        "rep.to_parquet(rep_parq, index=False)\n",
        "print(\"Saved:\", rep_csv, \",\", rep_parq)\n",
        "\n",
        "# === Blending semplice α∈[0,1] su VALID → reporting anche su TEST\n",
        "y_true_v = _as_1d_float(df_valid[VALUATION_K].to_numpy())\n",
        "yhatA_v  = _as_1d_float(pred_val_A)\n",
        "yhatB_v  = _as_1d_float(pred_val_B)\n",
        "\n",
        "m = min(len(y_true_v), len(yhatA_v), len(yhatB_v))\n",
        "y_true_v, yhatA_v, yhatB_v = y_true_v[:m], yhatA_v[:m], yhatB_v[:m]\n",
        "\n",
        "alphas = np.linspace(0.0, 1.0, 21)\n",
        "rows = []\n",
        "best_idx = None\n",
        "for i, a in enumerate(alphas):\n",
        "    y_blend_v = a * yhatB_v + (1.0 - a) * yhatA_v\n",
        "    y_blend_t = a * yhatB_t + (1.0 - a) * yhatA_t\n",
        "    mae_v = mean_absolute_error(y_true_v, y_blend_v)\n",
        "    mae_t = mean_absolute_error(y_true_t, y_blend_t)\n",
        "    rmse_t = np.sqrt(mean_squared_error(y_true_t, y_blend_t))\n",
        "    r2_t = r2_score(y_true_t, y_blend_t)\n",
        "    rows.append({\n",
        "        \"alpha\": float(a),\n",
        "        \"MAE_VALID\": float(mae_v),\n",
        "        \"MAE_TEST\": float(mae_t),\n",
        "        \"RMSE_TEST\": float(rmse_t),\n",
        "        \"R2_TEST\": float(r2_t),\n",
        "    })\n",
        "    if best_idx is None or mae_v < rows[best_idx][\"MAE_VALID\"]:\n",
        "        best_idx = i\n",
        "\n",
        "blend_df = pd.DataFrame(rows).sort_values(\"MAE_VALID\", ascending=True)\n",
        "blend_csv  = ART_DIR / \"blend_search_AB.csv\"\n",
        "blend_parq = ART_DIR / \"blend_search_AB.parquet\"\n",
        "blend_df.to_csv(blend_csv, index=False)\n",
        "blend_df.to_parquet(blend_parq, index=False)\n",
        "\n",
        "best_row = blend_df.iloc[0]\n",
        "print(\"Saved:\", blend_csv, \",\", blend_parq)\n",
        "print(\n",
        "    f\"Blend best α on VALID = {best_row['alpha']:.2f}  \"\n",
        "    f\"|  MAE_VALID={best_row['MAE_VALID']:.3f}  \"\n",
        "    f\"|  MAE_TEST={best_row['MAE_TEST']:.3f}  \"\n",
        "    f\"|  R2_TEST={best_row['R2_TEST']:.4f}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4ad2735",
      "metadata": {},
      "source": [
        "### XGBoost + Optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "9e53e7e1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected target unit: kEUR  (median=409.6)  MAX_LOG=9.900\n"
          ]
        }
      ],
      "source": [
        "# ---- TARGET SCALE HANDLER (EUR vs kEUR) ----\n",
        "import numpy as np\n",
        "\n",
        "TARGET_RAW_TRAIN = df_train[VALUATION_K].astype(\"float64\").to_numpy()\n",
        "p50 = float(np.nanmedian(TARGET_RAW_TRAIN))\n",
        "\n",
        "# eur_if_big: se la mediana > 2_000 assumo che la colonna sia in EURO; altrimenti è già in k€\n",
        "UNIT_SCALE = 1000.0 if p50 > 2000.0 else 1.0       # divide-by for training\n",
        "UNIT_LABEL = \"EUR\" if UNIT_SCALE == 1000.0 else \"kEUR\"\n",
        "\n",
        "def to_log(y_nat):\n",
        "    \"\"\"porta il target su scala log1p, uniformando all'unità di training.\"\"\"\n",
        "    return np.log1p(np.asarray(y_nat, float) / UNIT_SCALE)\n",
        "\n",
        "def from_log(y_log):\n",
        "    \"\"\"torna su scala naturale nell'UNITÀ ORIGINALE DEL DATASET (stessa di df[VALUATION_K]).\"\"\"\n",
        "    return np.expm1(np.asarray(y_log, float)) * UNIT_SCALE\n",
        "\n",
        "# Tetto numericamente stabile calcolato nella stessa unità del training\n",
        "p999 = float(np.nanpercentile(TARGET_RAW_TRAIN / UNIT_SCALE, 99.9))\n",
        "MAX_LOG = float(np.log1p(p999 * 10.0))  # 10× il 99.9p nella stessa unità del training\n",
        "\n",
        "def safe_expm1_scaled(y_log):\n",
        "    z = np.asarray(y_log, float)\n",
        "    z = np.clip(z, -20.0, MAX_LOG)      # evita overflow\n",
        "    return np.expm1(z) * UNIT_SCALE     # torna alla stessa unità del dataset\n",
        "\n",
        "# y in log per il training, coerente con UNIT_SCALE\n",
        "y_train = to_log(df_train[VALUATION_K].values)\n",
        "y_valid = to_log(df_valid[VALUATION_K].values)\n",
        "y_test  = to_log(df_test [VALUATION_K].values)\n",
        "\n",
        "# per metriche, i \"true\" restano nella loro unità originale (df_... è già in quella unità)\n",
        "y_val_true = df_valid[VALUATION_K].astype(\"float64\").to_numpy()\n",
        "y_tst_true = df_test [VALUATION_K].astype(\"float64\").to_numpy()\n",
        "\n",
        "print(f\"Detected target unit: {UNIT_LABEL}  (median={p50:,.1f})  MAX_LOG={MAX_LOG:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "ef6e7363",
      "metadata": {},
      "outputs": [],
      "source": [
        "# XGBoost + Optuna — import & setup (coerente con notebooks/outputs/modeling/property)\n",
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    import optuna\n",
        "    from optuna.pruners import MedianPruner\n",
        "    from optuna.samplers import TPESampler\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"Servono xgboost e optuna (pip install xgboost optuna)\") from e\n",
        "\n",
        "# Dir coerenti (siamo dentro notebooks/)\n",
        "BASE_OUT = Path(\"outputs\")\n",
        "PROP_DIR = BASE_OUT / \"modeling\" / \"property\"\n",
        "XGB_DIR  = PROP_DIR / \"xgb\"\n",
        "for d in (PROP_DIR, XGB_DIR):\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Config da TRAIN_CFG (già caricato in alto nel nb)\n",
        "N_TRIALS   = int(TRAIN_CFG.get(\"xgb_optuna_trials\", 25))\n",
        "EARLY_STOP = int(TRAIN_CFG.get(\"xgb_early_stopping_rounds\", 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ab0bc72f",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-21 08:54:02,789] A new study created in memory with name: no-name-212781c8-9a9d-41e1-a46d-72282ee5978b\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e42d5d52475f42628aeadf1f440be9c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[I 2025-08-21 08:54:04,854] Trial 0 finished with value: 41.93751842320595 and parameters: {'n_estimators': 924, 'max_depth': 12, 'learning_rate': 0.08960785365368121, 'subsample': 0.8394633936788146, 'colsample_bytree': 0.6624074561769746, 'min_child_weight': 2.403950683025824, 'reg_alpha': 2.5502648504032812e-08, 'reg_lambda': 2.9154431891537547, 'gamma': 3.005575058716044}. Best is trial 0 with value: 41.93751842320595.\n",
            "[I 2025-08-21 08:54:06,421] Trial 1 finished with value: 41.8434114166541 and parameters: {'n_estimators': 1392, 'max_depth': 4, 'learning_rate': 0.18276027831785724, 'subsample': 0.9329770563201687, 'colsample_bytree': 0.6849356442713105, 'min_child_weight': 2.636424704863906, 'reg_alpha': 1.9223460470643606e-07, 'reg_lambda': 0.016480446427978974, 'gamma': 2.6237821581611893}. Best is trial 1 with value: 41.8434114166541.\n",
            "[I 2025-08-21 08:54:07,721] Trial 2 finished with value: 34.5904336679743 and parameters: {'n_estimators': 1005, 'max_depth': 6, 'learning_rate': 0.06252287916406217, 'subsample': 0.6557975442608167, 'colsample_bytree': 0.7168578594140873, 'min_child_weight': 4.297256589643226, 'reg_alpha': 1.5577217702692994e-05, 'reg_lambda': 1.382623217936987, 'gamma': 0.9983689107917987}. Best is trial 2 with value: 34.5904336679743.\n",
            "[I 2025-08-21 08:54:09,703] Trial 3 finished with value: 47.52345860572431 and parameters: {'n_estimators': 1120, 'max_depth': 9, 'learning_rate': 0.011492999300221412, 'subsample': 0.8430179407605753, 'colsample_bytree': 0.6682096494749166, 'min_child_weight': 1.5854643368675156, 'reg_alpha': 0.04387314432435398, 'reg_lambda': 7.2866537374910445, 'gamma': 4.041986740582305}. Best is trial 2 with value: 34.5904336679743.\n",
            "[I 2025-08-21 08:54:10,804] Trial 4 finished with value: 35.23788827289005 and parameters: {'n_estimators': 826, 'max_depth': 4, 'learning_rate': 0.07766184280392888, 'subsample': 0.7760609974958406, 'colsample_bytree': 0.6488152939379115, 'min_child_weight': 5.456592191001432, 'reg_alpha': 1.7406828393128058e-08, 'reg_lambda': 4.337920697490942, 'gamma': 1.2938999080000846}. Best is trial 2 with value: 34.5904336679743.\n",
            "[I 2025-08-21 08:54:12,461] Trial 5 finished with value: 48.48173281039224 and parameters: {'n_estimators': 1328, 'max_depth': 6, 'learning_rate': 0.04749239763680407, 'subsample': 0.8186841117373118, 'colsample_bytree': 0.6739417822102108, 'min_child_weight': 9.726261649881026, 'reg_alpha': 0.0026664274004676823, 'reg_lambda': 5.727904470799623, 'gamma': 4.474136752138244}. Best is trial 2 with value: 34.5904336679743.\n",
            "[I 2025-08-21 08:54:14,448] Trial 6 finished with value: 49.647964613736605 and parameters: {'n_estimators': 1237, 'max_depth': 12, 'learning_rate': 0.01303561122512888, 'subsample': 0.6783931449676581, 'colsample_bytree': 0.6180909155642152, 'min_child_weight': 3.927972976869379, 'reg_alpha': 5.257036929213663e-06, 'reg_lambda': 0.01217295809836997, 'gamma': 4.143687545759647}. Best is trial 2 with value: 34.5904336679743.\n",
            "[I 2025-08-21 08:54:15,749] Trial 7 finished with value: 34.675932054438206 and parameters: {'n_estimators': 899, 'max_depth': 6, 'learning_rate': 0.05082341959721458, 'subsample': 0.6563696899899051, 'colsample_bytree': 0.9208787923016158, 'min_child_weight': 1.6709557931179373, 'reg_alpha': 0.08094845352286129, 'reg_lambda': 1.2273800987852967, 'gamma': 0.993578407670862}. Best is trial 2 with value: 34.5904336679743.\n",
            "[I 2025-08-21 08:54:16,506] Trial 8 finished with value: 46.900706437036135 and parameters: {'n_estimators': 407, 'max_depth': 11, 'learning_rate': 0.08310795711416077, 'subsample': 0.8916028672163949, 'colsample_bytree': 0.9085081386743783, 'min_child_weight': 1.6664018656068134, 'reg_alpha': 3.230428252240957e-06, 'reg_lambda': 0.0029072088906598446, 'gamma': 4.315517129377968}. Best is trial 2 with value: 34.5904336679743.\n",
            "[I 2025-08-21 08:54:18,511] Trial 9 finished with value: 41.80361483754817 and parameters: {'n_estimators': 1273, 'max_depth': 6, 'learning_rate': 0.012097379927033842, 'subsample': 0.7243929286862649, 'colsample_bytree': 0.7300733288106989, 'min_child_weight': 7.566455605042576, 'reg_alpha': 0.00029033694281285587, 'reg_lambda': 3.53875886477924, 'gamma': 2.3610746258097466}. Best is trial 2 with value: 34.5904336679743.\n",
            "[I 2025-08-21 08:54:21,172] Trial 10 finished with value: 28.561170113724753 and parameters: {'n_estimators': 1724, 'max_depth': 8, 'learning_rate': 0.025137344588454083, 'subsample': 0.6071847502459278, 'colsample_bytree': 0.8010124870699185, 'min_child_weight': 5.691097795015452, 'reg_alpha': 7.33446025237546e-05, 'reg_lambda': 0.31356971313563664, 'gamma': 0.1995376785974241}. Best is trial 10 with value: 28.561170113724753.\n",
            "[I 2025-08-21 08:54:24,563] Trial 11 finished with value: 25.664285631885164 and parameters: {'n_estimators': 1795, 'max_depth': 8, 'learning_rate': 0.023374412206397622, 'subsample': 0.6097132653988919, 'colsample_bytree': 0.814249705614196, 'min_child_weight': 5.337066966969968, 'reg_alpha': 9.001065831943221e-05, 'reg_lambda': 0.29163991155219965, 'gamma': 0.02337640770715005}. Best is trial 11 with value: 25.664285631885164.\n",
            "[I 2025-08-21 08:54:28,555] Trial 12 finished with value: 26.95673886877292 and parameters: {'n_estimators': 1754, 'max_depth': 9, 'learning_rate': 0.023662641128950907, 'subsample': 0.6050785532559382, 'colsample_bytree': 0.8374709757162001, 'min_child_weight': 6.87413080202483, 'reg_alpha': 0.00024745749757604425, 'reg_lambda': 0.15055494946272593, 'gamma': 0.015536082439507444}. Best is trial 11 with value: 25.664285631885164.\n",
            "[I 2025-08-21 08:54:31,710] Trial 13 finished with value: 27.923251574326798 and parameters: {'n_estimators': 1795, 'max_depth': 9, 'learning_rate': 0.02455537902996222, 'subsample': 0.600751750127807, 'colsample_bytree': 0.8287108271997778, 'min_child_weight': 7.526458776176712, 'reg_alpha': 0.0010397559958273764, 'reg_lambda': 0.12864725565457788, 'gamma': 0.09146016839237003}. Best is trial 11 with value: 25.664285631885164.\n",
            "[I 2025-08-21 08:54:33,972] Trial 14 finished with value: 38.10238362248397 and parameters: {'n_estimators': 1549, 'max_depth': 8, 'learning_rate': 0.02838684831520224, 'subsample': 0.9984067957444853, 'colsample_bytree': 0.9985595357146483, 'min_child_weight': 7.22362064609635, 'reg_alpha': 0.006701752722566276, 'reg_lambda': 0.2816393864538028, 'gamma': 1.7077765831491598}. Best is trial 11 with value: 25.664285631885164.\n",
            "[I 2025-08-21 08:54:36,573] Trial 15 finished with value: 31.762605788979485 and parameters: {'n_estimators': 1587, 'max_depth': 10, 'learning_rate': 0.01833312815369144, 'subsample': 0.7495910822966338, 'colsample_bytree': 0.8586963239553378, 'min_child_weight': 8.897322665986316, 'reg_alpha': 0.00010702705766492697, 'reg_lambda': 0.024484224687182917, 'gamma': 0.5221658930359901}. Best is trial 11 with value: 25.664285631885164.\n",
            "[I 2025-08-21 08:54:38,757] Trial 16 finished with value: 38.89792937838714 and parameters: {'n_estimators': 1559, 'max_depth': 7, 'learning_rate': 0.033924351602348135, 'subsample': 0.7114652779980589, 'colsample_bytree': 0.761444224941457, 'min_child_weight': 6.192542565007101, 'reg_alpha': 1.4719920281930697e-06, 'reg_lambda': 0.05940843160320319, 'gamma': 1.79195142395271}. Best is trial 11 with value: 25.664285631885164.\n",
            "[I 2025-08-21 08:54:40,341] Trial 17 finished with value: 32.65916425691621 and parameters: {'n_estimators': 698, 'max_depth': 10, 'learning_rate': 0.017950540407481577, 'subsample': 0.6435360403446044, 'colsample_bytree': 0.875869770084707, 'min_child_weight': 4.421644537474997, 'reg_alpha': 0.000537623796344171, 'reg_lambda': 0.6016583870353316, 'gamma': 0.6122133392523892}. Best is trial 11 with value: 25.664285631885164.\n",
            "[I 2025-08-21 08:54:43,760] Trial 18 finished with value: 27.256628328329775 and parameters: {'n_estimators': 1686, 'max_depth': 9, 'learning_rate': 0.018444825002317942, 'subsample': 0.6001560963638785, 'colsample_bytree': 0.7770145802474593, 'min_child_weight': 6.323659038961531, 'reg_alpha': 2.9311515463115088e-05, 'reg_lambda': 0.07449029925617398, 'gamma': 0.06632733810148511}. Best is trial 11 with value: 25.664285631885164.\n",
            "[I 2025-08-21 08:54:45,839] Trial 19 finished with value: 44.817912912282935 and parameters: {'n_estimators': 1440, 'max_depth': 7, 'learning_rate': 0.036011390902251424, 'subsample': 0.7006088259987461, 'colsample_bytree': 0.9749250435374457, 'min_child_weight': 8.624588983474878, 'reg_alpha': 0.013021147216287167, 'reg_lambda': 0.004767888795794927, 'gamma': 3.1299545467295014}. Best is trial 11 with value: 25.664285631885164.\n",
            "[I 2025-08-21 08:54:47,842] Trial 20 finished with value: 47.263706609965546 and parameters: {'n_estimators': 1648, 'max_depth': 11, 'learning_rate': 0.1312102588277893, 'subsample': 0.7516818631642986, 'colsample_bytree': 0.827760909783021, 'min_child_weight': 4.864553431264149, 'reg_alpha': 6.604190789604671e-07, 'reg_lambda': 0.0011129709953152675, 'gamma': 4.969213925248544}. Best is trial 11 with value: 25.664285631885164.\n",
            "[I 2025-08-21 08:54:51,711] Trial 21 finished with value: 27.075697310106918 and parameters: {'n_estimators': 1784, 'max_depth': 9, 'learning_rate': 0.017563420206771783, 'subsample': 0.6240908330192126, 'colsample_bytree': 0.7719898235665421, 'min_child_weight': 6.500382993492119, 'reg_alpha': 2.3588149038792523e-05, 'reg_lambda': 0.07095848277706024, 'gamma': 0.043961607483330466}. Best is trial 11 with value: 25.664285631885164.\n",
            "[I 2025-08-21 08:54:54,787] Trial 22 finished with value: 32.29058307767441 and parameters: {'n_estimators': 1781, 'max_depth': 10, 'learning_rate': 0.015680012005767754, 'subsample': 0.6374112014152274, 'colsample_bytree': 0.7507119068691461, 'min_child_weight': 6.838526202987577, 'reg_alpha': 0.00016121834529477652, 'reg_lambda': 0.1622394471952778, 'gamma': 0.6199672567575163}. Best is trial 11 with value: 25.664285631885164.\n",
            "[I 2025-08-21 08:54:56,986] Trial 23 finished with value: 36.67538603235432 and parameters: {'n_estimators': 1518, 'max_depth': 8, 'learning_rate': 0.022694767645147197, 'subsample': 0.6814728659609116, 'colsample_bytree': 0.7971383950843234, 'min_child_weight': 5.183453129742371, 'reg_alpha': 1.3596322239876697e-05, 'reg_lambda': 0.06993700387973732, 'gamma': 1.334470999305845}. Best is trial 11 with value: 25.664285631885164.\n",
            "[I 2025-08-21 08:54:59,450] Trial 24 finished with value: 33.46209894379583 and parameters: {'n_estimators': 1695, 'max_depth': 9, 'learning_rate': 0.034538861242517545, 'subsample': 0.6251823621478444, 'colsample_bytree': 0.8476283769900074, 'min_child_weight': 3.648095702292739, 'reg_alpha': 0.001686062119912951, 'reg_lambda': 0.03624271398866999, 'gamma': 0.7076690629855376}. Best is trial 11 with value: 25.664285631885164.\n",
            "Best trial: 11 MAE (valid, natural): 25.664285631885164\n"
          ]
        }
      ],
      "source": [
        "# 07ter) Optuna: tuning XGBoost (setup B: confidence come peso) — safe & robust\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.base import clone\n",
        "import inspect\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# tetto dinamico: 10× il 99.9° percentile del target (in k€), poi log1p\n",
        "MAX_LOG = float(np.log1p(np.nanpercentile(df_train[VALUATION_K].values, 99.9) * 10.0))\n",
        "\n",
        "def safe_expm1(z, max_log=MAX_LOG):\n",
        "    z = np.asarray(z, dtype=np.float64)\n",
        "    z = np.clip(z, -20.0, max_log)  # -20 ~ ~0 in scala naturale; max_log evita overflow\n",
        "    return np.expm1(z)\n",
        "\n",
        "\n",
        "# Preprocess identico a setup B (senza confidence tra le feature)\n",
        "_transformers = []\n",
        "if \"cat_cols\" in globals() and len(cat_cols) > 0:\n",
        "    _transformers.append((\"cat\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1), cat_cols))\n",
        "if \"num_cols_B\" in globals() and len(num_cols_B) > 0:\n",
        "    _transformers.append((\"num\", \"passthrough\", num_cols_B))\n",
        "preproc_B = ColumnTransformer(transformers=_transformers, remainder=\"drop\")\n",
        "\n",
        "# Feature list (dedup, ordine preservato)\n",
        "features_B = list(dict.fromkeys([*(cat_cols if \"cat_cols\" in globals() else []),\n",
        "                                 *(num_cols_B if \"num_cols_B\" in globals() else [])]))\n",
        "\n",
        "# Pesi (confidence/sample_weight) se presenti → allineati all'indice di X_train\n",
        "if \"sample_weight\" in df_train.columns:\n",
        "    weights_B = df_train.loc[X_train.index, \"sample_weight\"].to_numpy(dtype=float)\n",
        "else:\n",
        "    weights_B = np.ones(len(X_train), dtype=float)\n",
        "\n",
        "def _suggest_params(trial: optuna.Trial) -> dict:\n",
        "    return {\n",
        "        \"n_estimators\":     trial.suggest_int(\"n_estimators\", 400, 1800),\n",
        "        \"max_depth\":        trial.suggest_int(\"max_depth\", 4, 12),\n",
        "        \"learning_rate\":    trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
        "        \"subsample\":        trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
        "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1.0, 10.0),\n",
        "        \"reg_alpha\":        trial.suggest_float(\"reg_alpha\", 1e-8, 1e-1, log=True),\n",
        "        \"reg_lambda\":       trial.suggest_float(\"reg_lambda\", 1e-3, 10.0, log=True),\n",
        "        \"gamma\":            trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
        "        \"random_state\":     SEED,\n",
        "        \"tree_method\":      \"hist\",\n",
        "        \"n_jobs\":           -1,\n",
        "    }\n",
        "\n",
        "def _fit_xgb_with_preproc(params: dict,\n",
        "                          Xtr_df: pd.DataFrame, ytr_log: np.ndarray,\n",
        "                          Xva_df: pd.DataFrame, yva_log: np.ndarray,\n",
        "                          w: np.ndarray, early_rounds: int = EARLY_STOP):\n",
        "    \"\"\"\n",
        "    Allena XGB su target log1p (usiamo y_train/y_valid già in log1p),\n",
        "    early-stopping su valid (MAE calcolata in log-space da XGB).\n",
        "    Le metriche 'finali' le faremo poi in scala naturale con expm1.\n",
        "    \"\"\"\n",
        "    prep = clone(preproc_B)\n",
        "    Xt = prep.fit_transform(Xtr_df)\n",
        "    Xv = prep.transform(Xva_df)\n",
        "\n",
        "    model = xgb.XGBRegressor(**params)\n",
        "    sig = inspect.signature(model.fit)\n",
        "    fit_kwargs = {}\n",
        "    if \"eval_set\" in sig.parameters:        fit_kwargs[\"eval_set\"] = [(Xv, yva_log)]\n",
        "    if \"eval_metric\" in sig.parameters:     fit_kwargs[\"eval_metric\"] = \"mae\"\n",
        "    if \"sample_weight\" in sig.parameters:   fit_kwargs[\"sample_weight\"] = w\n",
        "    if \"early_stopping_rounds\" in sig.parameters:\n",
        "        fit_kwargs[\"early_stopping_rounds\"] = early_rounds\n",
        "    elif \"callbacks\" in sig.parameters:\n",
        "        from xgboost.callback import EarlyStopping\n",
        "        fit_kwargs[\"callbacks\"] = [EarlyStopping(rounds=early_rounds, save_best=True)]\n",
        "    if \"verbose\" in sig.parameters:         fit_kwargs[\"verbose\"] = False\n",
        "\n",
        "    model.fit(Xt, ytr_log, **fit_kwargs)\n",
        "    pipe = Pipeline([(\"prep\", prep), (\"xgb\", model)])\n",
        "    return pipe, model, Xt, Xv\n",
        "\n",
        "def objective(trial: optuna.Trial) -> float:\n",
        "    params = _suggest_params(trial)\n",
        "    Xtr_df, Xva_df = X_train[features_B], X_valid[features_B]\n",
        "    # y_train/y_valid sono in log1p dalle celle RF — li riusiamo\n",
        "    pipe, model, Xt, Xv = _fit_xgb_with_preproc(params, Xtr_df, y_train, Xva_df, y_valid, weights_B)\n",
        "    # misura l'obiettivo in scala naturale\n",
        "    y_log_pred = model.predict(Xv)              # pred in log\n",
        "    y_log_pred = model.predict(Xv)\n",
        "    pred_nat = safe_expm1_scaled(y_log_pred)\n",
        "    if not np.all(np.isfinite(pred_nat)):  # trial instabile → penalizza\n",
        "        return 1e9\n",
        "    return mean_absolute_error(y_val_true, pred_nat)\n",
        "\n",
        "study = optuna.create_study(\n",
        "    direction=\"minimize\",\n",
        "    pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=5),\n",
        "    sampler=TPESampler(seed=SEED),\n",
        ")\n",
        "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)\n",
        "\n",
        "print(\"Best trial:\", study.best_trial.number, \"MAE (valid, natural):\", study.best_value)\n",
        "best_params = study.best_trial.params\n",
        "(XGB_DIR / \"optuna_best_params_setupB.json\").write_text(canonical_json_dumps(best_params), encoding=\"utf-8\")\n",
        "study.trials_dataframe().to_parquet(XGB_DIR / \"optuna_trials_setupB.parquet\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "550bceee",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGB_B VALID: {'MAE': 25.732720898487496, 'RMSE': 37.13057347296789, 'R2': 0.9825793266271864} | unit: kEUR\n",
            "XGB_B TEST : {'MAE': 25.970034718526936, 'RMSE': 38.50072518147715, 'R2': 0.9819947596644194} | unit: kEUR\n"
          ]
        }
      ],
      "source": [
        "# 07quater-bis) Build XGB finale (setup B) + metriche VALID/TEST\n",
        "\n",
        "import inspect, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "def _rmse(y_true, y_pred):\n",
        "    try:\n",
        "        return mean_squared_error(y_true, y_pred, squared=False)\n",
        "    except TypeError:\n",
        "        return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "# Best params da Optuna o fallback sensato\n",
        "bp_path = XGB_DIR / \"optuna_best_params_setupB.json\"\n",
        "if bp_path.exists():\n",
        "    best_params = json.loads(bp_path.read_text(encoding=\"utf-8\"))\n",
        "else:\n",
        "    best_params = {\n",
        "        \"n_estimators\": 900,\n",
        "        \"max_depth\": 8,\n",
        "        \"learning_rate\": 0.06,\n",
        "        \"subsample\": 0.9,\n",
        "        \"colsample_bytree\": 0.9,\n",
        "        \"min_child_weight\": 3.0,\n",
        "        \"reg_alpha\": 1e-6,\n",
        "        \"reg_lambda\": 1.0,\n",
        "        \"gamma\": 0.0,\n",
        "        \"random_state\": SEED,\n",
        "        \"tree_method\": \"hist\",\n",
        "        \"n_jobs\": -1,\n",
        "    }\n",
        "\n",
        "# Fit finale con early stopping su VALID\n",
        "pipe_xgb_B, model_xgb_B, Xt, Xv = _fit_xgb_with_preproc(\n",
        "    best_params,\n",
        "    X_train[features_B], y_train,\n",
        "    X_valid[features_B], y_valid,\n",
        "    weights_B,\n",
        "    early_rounds=EARLY_STOP,\n",
        ")\n",
        "\n",
        "# Predizioni in scala naturale (k€)\n",
        "ylog_val = pipe_xgb_B.named_steps[\"xgb\"].predict(\n",
        "    pipe_xgb_B.named_steps[\"prep\"].transform(X_valid[features_B])\n",
        ")\n",
        "ylog_tst = pipe_xgb_B.named_steps[\"xgb\"].predict(\n",
        "    pipe_xgb_B.named_steps[\"prep\"].transform(X_test[features_B])\n",
        ")\n",
        "pred_val_XGB = safe_expm1_scaled(ylog_val)\n",
        "pred_tst_XGB = safe_expm1_scaled(ylog_tst)\n",
        "\n",
        "m_val = {\n",
        "    \"MAE\": float(mean_absolute_error(y_val_true, pred_val_XGB)),\n",
        "    \"RMSE\": float(np.sqrt(mean_squared_error(y_val_true, pred_val_XGB))),\n",
        "    \"R2\":  float(r2_score(y_val_true, pred_val_XGB)),\n",
        "}\n",
        "m_tst = {\n",
        "    \"MAE\": float(mean_absolute_error(y_tst_true, pred_tst_XGB)),\n",
        "    \"RMSE\": float(np.sqrt(mean_squared_error(y_tst_true, pred_tst_XGB))),\n",
        "    \"R2\":  float(r2_score(y_tst_true, pred_tst_XGB)),\n",
        "}\n",
        "print(\"XGB_B VALID:\", m_val, \"| unit:\", UNIT_LABEL)\n",
        "print(\"XGB_B TEST :\", m_tst,  \"| unit:\", UNIT_LABEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "eddb94ef",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved XGB model to: outputs\\modeling\\property\\xgb\\xgb_setupB_champion.joblib\n",
            "Saved: outputs\\modeling\\property\\xgb\\xgb_gain_importance.json\n",
            "Updated manifest: outputs\\modeling\\property\\training_manifest.json\n"
          ]
        }
      ],
      "source": [
        "# Persistenza XGB + gain importance + update manifest (coerente con PROP_DIR)\n",
        "\n",
        "from pathlib import Path\n",
        "import joblib, json, hashlib\n",
        "\n",
        "xgb_path = XGB_DIR / \"xgb_setupB_champion.joblib\"\n",
        "\n",
        "# 1) Salva pipeline completa (prep + xgb)\n",
        "joblib.dump(pipe_xgb_B, xgb_path)\n",
        "print(\"Saved XGB model to:\", xgb_path)\n",
        "\n",
        "# 2) Gain importance (se disponibile)\n",
        "try:\n",
        "    booster = pipe_xgb_B.named_steps[\"xgb\"].get_booster()\n",
        "    f_gain = booster.get_score(importance_type=\"gain\")\n",
        "    (XGB_DIR / \"xgb_gain_importance.json\").write_text(canonical_json_dumps(f_gain), encoding=\"utf-8\")\n",
        "    print(\"Saved:\", XGB_DIR / \"xgb_gain_importance.json\")\n",
        "except Exception as e:\n",
        "    print(\"Gain importance not available:\", e)\n",
        "\n",
        "# 3) Hash per tracciabilità\n",
        "h = hashlib.sha256()\n",
        "with open(xgb_path, \"rb\") as _f:\n",
        "    for chunk in iter(lambda: _f.read(1024 * 1024), b\"\"):\n",
        "        h.update(chunk)\n",
        "xgb_sha = h.hexdigest()\n",
        "\n",
        "# 4) Aggiorna training manifest nella cartella PROPERTY\n",
        "manifest_path = PROP_DIR / \"training_manifest.json\"\n",
        "manifest = json.loads(manifest_path.read_text(encoding=\"utf-8\")) if manifest_path.exists() else {}\n",
        "\n",
        "# paths\n",
        "paths = manifest.setdefault(\"paths\", {})\n",
        "paths.update({\n",
        "    \"xgb_model\": str(xgb_path),\n",
        "    \"xgb_model_sha256\": xgb_sha,\n",
        "    \"xgb_gain_importance\": str(XGB_DIR / \"xgb_gain_importance.json\"),\n",
        "    \"xgb_optuna_best_params\": str(XGB_DIR / \"optuna_best_params_setupB.json\") if (XGB_DIR / \"optuna_best_params_setupB.json\").exists() else paths.get(\"xgb_optuna_best_params\"),\n",
        "    \"xgb_optuna_trials\": str(XGB_DIR / \"optuna_trials_setupB.parquet\") if (XGB_DIR / \"optuna_trials_setupB.parquet\").exists() else paths.get(\"xgb_optuna_trials\"),\n",
        "    # opzionale: punta il \"pipeline_path\" di default allo XGB champion\n",
        "    \"pipeline_path\": str(xgb_path),\n",
        "})\n",
        "\n",
        "# metrics\n",
        "metrics = manifest.setdefault(\"metrics\", {})\n",
        "metrics.update({\n",
        "    \"xgb_valid\": m_val,\n",
        "    \"xgb_test\":  m_tst,\n",
        "})\n",
        "\n",
        "# meta (utile per la UI/serving)\n",
        "manifest[\"model_meta\"] = {\n",
        "    \"value_model_name\": \"XGBRegressor\",\n",
        "    \"value_model_version\": \"v2\",          # etichetta “v2” per differenziarlo dall’RF v1\n",
        "    \"setup\": \"B_conf_as_weight\",\n",
        "    \"early_stopping_rounds\": int(EARLY_STOP),\n",
        "    \"optuna_trials\": int(N_TRIALS),\n",
        "    \"seed\": int(SEED),\n",
        "}\n",
        "\n",
        "manifest_path.write_text(canonical_json_dumps(manifest), encoding=\"utf-8\")\n",
        "print(\"Updated manifest:\", manifest_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "279cc11f-e303-48de-92c6-d38c9ec38bf1",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Feature Importance (on champion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "de9c6c54-9cfc-4c43-8cb6-721f5066387c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\figures\\rf_feature_importance_builtin.png\n",
            "Saved: c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\figures\\rf_feature_importance_permutation.png\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>importance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>f46</td>\n",
              "      <td>0.265033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>f56</td>\n",
              "      <td>0.259269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>f15</td>\n",
              "      <td>0.148742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>f6</td>\n",
              "      <td>0.079935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>f49</td>\n",
              "      <td>0.040026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>f16</td>\n",
              "      <td>0.037073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>f17</td>\n",
              "      <td>0.027307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>f63</td>\n",
              "      <td>0.021181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>f19</td>\n",
              "      <td>0.020104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>f21</td>\n",
              "      <td>0.018856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>f30</td>\n",
              "      <td>0.014050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>f22</td>\n",
              "      <td>0.009348</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   feature  importance\n",
              "0      f46    0.265033\n",
              "1      f56    0.259269\n",
              "2      f15    0.148742\n",
              "3       f6    0.079935\n",
              "4      f49    0.040026\n",
              "5      f16    0.037073\n",
              "6      f17    0.027307\n",
              "7      f63    0.021181\n",
              "8      f19    0.020104\n",
              "9      f21    0.018856\n",
              "10     f30    0.014050\n",
              "11     f22    0.009348"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>importance</th>\n",
              "      <th>std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>zone</td>\n",
              "      <td>0.523299</td>\n",
              "      <td>0.014430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>city</td>\n",
              "      <td>0.295325</td>\n",
              "      <td>0.016518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>size_m2</td>\n",
              "      <td>0.247257</td>\n",
              "      <td>0.004722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>log_size_m2</td>\n",
              "      <td>0.242719</td>\n",
              "      <td>0.004676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>year_built</td>\n",
              "      <td>0.071665</td>\n",
              "      <td>0.004844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>region</td>\n",
              "      <td>0.061480</td>\n",
              "      <td>0.003029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>energy_class</td>\n",
              "      <td>0.046980</td>\n",
              "      <td>0.001685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>condition</td>\n",
              "      <td>0.037097</td>\n",
              "      <td>0.002235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>region_index_prior</td>\n",
              "      <td>0.015209</td>\n",
              "      <td>0.000838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>view</td>\n",
              "      <td>0.001784</td>\n",
              "      <td>0.000346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>baths_per_100sqm</td>\n",
              "      <td>0.001614</td>\n",
              "      <td>0.000273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>floor</td>\n",
              "      <td>0.001538</td>\n",
              "      <td>0.000251</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               feature  importance       std\n",
              "0                 zone    0.523299  0.014430\n",
              "1                 city    0.295325  0.016518\n",
              "2              size_m2    0.247257  0.004722\n",
              "3          log_size_m2    0.242719  0.004676\n",
              "4           year_built    0.071665  0.004844\n",
              "5               region    0.061480  0.003029\n",
              "6         energy_class    0.046980  0.001685\n",
              "7            condition    0.037097  0.002235\n",
              "8   region_index_prior    0.015209  0.000838\n",
              "9                 view    0.001784  0.000346\n",
              "10    baths_per_100sqm    0.001614  0.000273\n",
              "11               floor    0.001538  0.000251"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 07) Feature importances (RF champion) + salvataggio figure — ROBUST\n",
        "from __future__ import annotations\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "TOPN = 20  # barre nei grafici\n",
        "\n",
        "# -- helper: prendi per prima un'istanza esistente tra più nomi\n",
        "def _pick_first(*names):\n",
        "    for n in names:\n",
        "        if n in globals() and globals()[n] is not None:\n",
        "            return globals()[n]\n",
        "    return None\n",
        "\n",
        "# -- helper: estrai inner estimator, preproc e step finale da Pipeline o TTR\n",
        "def _extract_parts(model_like):\n",
        "    \"\"\"\n",
        "    Ritorna (inner_pipe, preproc, final_est, final_step_name, is_ttr)\n",
        "    - inner_pipe: Pipeline(\"prep\", \"...\"), l'oggetto che ha .named_steps\n",
        "    - preproc   : ColumnTransformer dello step \"prep\", se presente\n",
        "    - final_est : estimatore finale (es. RandomForestRegressor)\n",
        "    - final_step_name: nome dello step finale nella pipeline (es. \"rf\" o \"model\")\n",
        "    - is_ttr    : bool, True se model_like è un TransformedTargetRegressor\n",
        "    \"\"\"\n",
        "    is_ttr = isinstance(model_like, TransformedTargetRegressor)\n",
        "    inner = model_like.regressor_ if is_ttr and hasattr(model_like, \"regressor_\") else \\\n",
        "            (model_like.regressor if is_ttr and hasattr(model_like, \"regressor\") else model_like)\n",
        "\n",
        "    if not isinstance(inner, Pipeline):\n",
        "        raise RuntimeError(\"Pipeline interna non trovata: atteso Pipeline con step 'prep' e modello finale.\")\n",
        "\n",
        "    # preproc\n",
        "    preproc = inner.named_steps.get(\"prep\", None)\n",
        "\n",
        "    # step finale & estimator\n",
        "    # preferisci chiavi comuni, altrimenti prendi l'ultimo step\n",
        "    final_step_name = None\n",
        "    for cand in (\"rf\", \"model\", \"xgb\"):\n",
        "        if cand in inner.named_steps:\n",
        "            final_step_name = cand\n",
        "            break\n",
        "    if final_step_name is None:\n",
        "        final_step_name = list(inner.named_steps.keys())[-1]\n",
        "    final_est = inner.named_steps[final_step_name]\n",
        "\n",
        "    return inner, preproc, final_est, final_step_name, is_ttr\n",
        "\n",
        "# -- helper: ricava lista feature nella GIUSTA sequenza del CT (cat poi num)\n",
        "def _feature_names_from_ct(preproc, fallback_cat, fallback_num):\n",
        "    \"\"\"\n",
        "    Con OrdinalEncoder per le categoriche ⇒ 1 col/feature.\n",
        "    \"\"\"\n",
        "    if preproc is None or not hasattr(preproc, \"transformers\"):\n",
        "        # fallback: usa liste note\n",
        "        return [*fallback_cat, *[c for c in fallback_num if c not in set(fallback_cat)]]\n",
        "\n",
        "    cat_cols_ct, num_cols_ct = [], []\n",
        "    for name, est, cols in preproc.transformers:\n",
        "        if name == \"cat\":\n",
        "            cat_cols_ct = list(cols) if isinstance(cols, (list, tuple, np.ndarray, pd.Index)) else list(fallback_cat)\n",
        "        elif name == \"num\":\n",
        "            num_cols_ct = list(cols) if isinstance(cols, (list, tuple, np.ndarray, pd.Index)) else list(fallback_num)\n",
        "\n",
        "    # sicurezza ordine/duplicati\n",
        "    seen = set()\n",
        "    ordered = []\n",
        "    for c in list(cat_cols_ct) + list(num_cols_ct):\n",
        "        if c not in seen:\n",
        "            seen.add(c); ordered.append(c)\n",
        "    return ordered\n",
        "\n",
        "# -- 1) Scegli il CHAMPION e la relativa pipeline\n",
        "champ = (champion if \"champion\" in globals() else \"A\")  # default\n",
        "if champ == \"A\":\n",
        "    chosen = _pick_first(\"ttr_A\", \"pipe_A\")\n",
        "    # liste colonne previste per A\n",
        "    chosen_cat = [c for c in (cat_cols if \"cat_cols\" in globals() else [])]\n",
        "    chosen_num = [c for c in (num_cols  if \"num_cols\"  in globals() else [])]\n",
        "else:\n",
        "    chosen = _pick_first(\"ttr_B\", \"pipe_B\")\n",
        "    # liste colonne previste per B (no confidence come feature)\n",
        "    chosen_cat = [c for c in (cat_cols    if \"cat_cols\"    in globals() else [])]\n",
        "    chosen_num = [c for c in (num_cols_B  if \"num_cols_B\"  in globals() else [])]\n",
        "\n",
        "if chosen is None:\n",
        "    raise RuntimeError(\"Nessuna pipeline/TransformedTargetRegressor disponibile per il champion selezionato.\")\n",
        "\n",
        "# -- 2) Estrai parti interne e ricava feature usate davvero\n",
        "inner_pipe, preproc, final_est, final_step, is_ttr = _extract_parts(chosen)\n",
        "feat_in_use = _feature_names_from_ct(preproc, chosen_cat, chosen_num)\n",
        "\n",
        "# -- 3) Prepara X_test allineato (crea colonne mancanti → imputazione)\n",
        "def _ensure_cols(df_part: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
        "    dfp = df_part.copy()\n",
        "    missing = [c for c in cols if c not in dfp.columns]\n",
        "    for c in missing:\n",
        "        dfp[c] = np.nan\n",
        "    return dfp[cols]\n",
        "\n",
        "if \"df_test\" not in globals():\n",
        "    raise RuntimeError(\"df_test mancante.\")\n",
        "X_tst_use = _ensure_cols(df_test, feat_in_use)\n",
        "\n",
        "# -- 4) BUILT-IN importance (se disponibile) con nomi feature consistenti\n",
        "builtin_imp = None\n",
        "fi_raw = getattr(final_est, \"feature_importances_\", None)\n",
        "if fi_raw is not None:\n",
        "    try:\n",
        "        imp = np.asarray(fi_raw)\n",
        "        if imp.ndim == 1 and imp.size > 0:\n",
        "            # allinea lunghezze; se non combaciano, usa f0..f{n-1}\n",
        "            if len(feat_in_use) != imp.shape[0]:\n",
        "                feat_names = [f\"f{i}\" for i in range(int(imp.shape[0]))]\n",
        "            else:\n",
        "                feat_names = list(feat_in_use)\n",
        "            builtin_imp = (\n",
        "                pd.DataFrame({\"feature\": feat_names, \"importance\": imp.astype(float, copy=False)})\n",
        "                .sort_values(\"importance\", ascending=False)\n",
        "                .reset_index(drop=True)\n",
        "            )\n",
        "            builtin_imp.to_csv(MODEL_DIR / \"feature_importance_builtin.csv\", index=False)\n",
        "            builtin_imp.to_parquet(MODEL_DIR / \"feature_importance_builtin.parquet\", index=False)\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            _top = min(TOPN, len(builtin_imp))\n",
        "            ax = builtin_imp.head(_top).plot(kind=\"bar\", x=\"feature\", y=\"importance\", legend=False, rot=45)\n",
        "            ax.set_title(f\"RF Built-in Feature Importance (top {_top})\")\n",
        "            ax.set_ylabel(\"Importance\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(FIG_DIR / \"rf_feature_importance_builtin.png\", dpi=150)\n",
        "            plt.close()\n",
        "            print(\"Saved:\", FIG_DIR / \"rf_feature_importance_builtin.png\")\n",
        "        else:\n",
        "            print(\"⚠️ feature_importances_ presente ma vuoto/0-D → salto built-in.\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Impossibile calcolare built-in importance: {e}\")\n",
        "else:\n",
        "    print(\"ℹ️ feature_importances_ non disponibile sul modello finale → salto built-in.\")\n",
        "\n",
        "# -- 5) PERMUTATION importance\n",
        "#    Scala y coerente:\n",
        "#      - se TTR: predict è in scala NATURALE → passiamo y naturale\n",
        "#      - se Pipeline pura (train su log1p): passiamo y in log\n",
        "y_perm = None\n",
        "if is_ttr:\n",
        "    y_perm = df_test[VALUATION_K].to_numpy(dtype=\"float64\", copy=False)\n",
        "else:\n",
        "    y_perm = np.log1p(df_test[VALUATION_K].to_numpy(dtype=\"float64\", copy=False))\n",
        "\n",
        "perm = permutation_importance(\n",
        "    estimator=chosen,              # TTR o Pipeline\n",
        "    X=X_tst_use,\n",
        "    y=y_perm,\n",
        "    n_repeats=8,\n",
        "    random_state=SEED if \"SEED\" in globals() else 42,\n",
        "    n_jobs=-1,\n",
        "    scoring=\"r2\",\n",
        ")\n",
        "# nomi: usa feat_in_use (o fallback f0..)\n",
        "feat_names_pi = list(feat_in_use) if len(feat_in_use) == perm.importances_mean.shape[0] \\\n",
        "                else [f\"f{i}\" for i in range(perm.importances_mean.shape[0])]\n",
        "\n",
        "perm_imp = (\n",
        "    pd.DataFrame({\n",
        "        \"feature\": feat_names_pi,\n",
        "        \"importance\": perm.importances_mean.astype(float, copy=False),\n",
        "        \"std\": perm.importances_std.astype(float, copy=False),\n",
        "    })\n",
        "    .sort_values(\"importance\", ascending=False)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "perm_imp.to_csv(MODEL_DIR / \"feature_importance_permutation.csv\", index=False)\n",
        "perm_imp.to_parquet(MODEL_DIR / \"feature_importance_permutation.parquet\", index=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "_top = min(TOPN, len(perm_imp))\n",
        "ax = perm_imp.head(_top).plot(kind=\"bar\", x=\"feature\", y=\"importance\", yerr=\"std\", legend=False, rot=45)\n",
        "ax.set_title(f\"Permutation Importance (top {_top})\")\n",
        "ax.set_ylabel(\"Importance (mean ΔR²)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"rf_feature_importance_permutation.png\", dpi=150)\n",
        "plt.close()\n",
        "print(\"Saved:\", FIG_DIR / \"rf_feature_importance_permutation.png\")\n",
        "\n",
        "# -- 6) Anteprima (se disponibile)\n",
        "if builtin_imp is not None:\n",
        "    display(builtin_imp.head(12))\n",
        "display(perm_imp.head(12))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca8ec440",
      "metadata": {},
      "source": [
        "### Save figure importances & residuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "a72e0ffa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\figures\\rf_feature_importance_builtin_top20.png\n",
            "Saved: c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\figures\\rf_permutation_importance_top20.png\n",
            "Saved: c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\figures\\rf_residuals_test_hist.png\n",
            "Saved: c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\figures\\rf_residuals_vs_pred_test.png\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 900x600 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 900x600 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 07bis (v2) — Importances + Residuali TEST con ricalcolo robusto delle predizioni\n",
        "\n",
        "from __future__ import annotations\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "\n",
        "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------------- helpers ----------------\n",
        "def _expm1_safe(z, cap: float = 12.0):\n",
        "    z = np.asarray(z, dtype=np.float64)\n",
        "    z = np.clip(z, -20.0, cap)  # cap log-pred per evitare overflow\n",
        "    out = np.expm1(z)\n",
        "    out[out < 0] = 0.0\n",
        "    return out\n",
        "\n",
        "def _ensure_cols(df_part: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
        "    dfp = df_part.copy()\n",
        "    miss = [c for c in cols if c not in dfp.columns]\n",
        "    if miss:\n",
        "        for c in miss: dfp[c] = np.nan\n",
        "        print(f\"ℹ️ Aggiunte {len(miss)} colonne mancanti allo split TEST (imputate): {miss[:10]}\")\n",
        "    return dfp[cols]\n",
        "\n",
        "def _pick_first(*names):\n",
        "    for n in names:\n",
        "        if n in globals() and globals()[n] is not None:\n",
        "            return globals()[n]\n",
        "    return None\n",
        "\n",
        "def _predict_nat_safely(est, X_df: pd.DataFrame, cols: list[str], log_cap: float = 12.0):\n",
        "    X_use = _ensure_cols(X_df, cols)\n",
        "    if isinstance(est, TransformedTargetRegressor):\n",
        "        # prendi i LOG-pred dal regressor interno, poi expm1 safe\n",
        "        log_pred = est.regressor_.predict(X_use)\n",
        "        # diagnostica\n",
        "        q = np.nanpercentile(log_pred, [50, 90, 99, 99.9])\n",
        "        print(f\"log-pred percentiles (TTR inner): p50={q[0]:.3f}, p90={q[1]:.3f}, p99={q[2]:.3f}, p99.9={q[3]:.3f}\")\n",
        "        return _expm1_safe(log_pred, cap=log_cap)\n",
        "    else:\n",
        "        # pipeline RF/XGB allenata su log1p → predict = log-pred\n",
        "        log_pred = est.predict(X_use)\n",
        "        q = np.nanpercentile(log_pred, [50, 90, 99, 99.9])\n",
        "        print(f\"log-pred percentiles (pipe): p50={q[0]:.3f}, p90={q[1]:.3f}, p99={q[2]:.3f}, p99.9={q[3]:.3f}\")\n",
        "        return _expm1_safe(log_pred, cap=log_cap)\n",
        "\n",
        "def _plot_topN(df_imp: pd.DataFrame, val_col: str, title: str, out_path, xlabel: str):\n",
        "    if df_imp is None or not isinstance(df_imp, pd.DataFrame) or df_imp.empty:\n",
        "        print(f\"ℹ️ skip plot: {title} — dataframe vuoto\")\n",
        "        return\n",
        "    top = df_imp.head(20).iloc[::-1]\n",
        "    plt.figure(figsize=(9,6))\n",
        "    ax = top.plot(x=\"feature\", y=val_col, kind=\"barh\", legend=False)\n",
        "    ax.set_title(title); ax.set_xlabel(xlabel)\n",
        "    plt.tight_layout(); plt.savefig(out_path, dpi=150, bbox_inches=\"tight\"); plt.close()\n",
        "    print(\"Saved:\", out_path)\n",
        "\n",
        "# --------------- 1) Feature importances ---------------\n",
        "_plot_topN(\n",
        "    builtin_imp if \"builtin_imp\" in globals() else None,\n",
        "    \"importance\",\n",
        "    \"RF Feature Importance (built-in, top 20)\",\n",
        "    FIG_DIR / \"rf_feature_importance_builtin_top20.png\",\n",
        "    \"Importance\",\n",
        ")\n",
        "_plot_topN(\n",
        "    perm_imp if \"perm_imp\" in globals() else None,\n",
        "    \"importance\",\n",
        "    \"Permutation Importance (ΔR², top 20)\",\n",
        "    FIG_DIR / \"rf_permutation_importance_top20.png\",\n",
        "    \"Importance (mean ΔR²)\",\n",
        ")\n",
        "\n",
        "# --------------- 2) Residuali TEST ---------------\n",
        "if \"df_test\" not in globals():\n",
        "    raise RuntimeError(\"df_test non è definito — impossibile calcolare i residuali.\")\n",
        "\n",
        "y_true_t = df_test[VALUATION_K].to_numpy(dtype=\"float64\", copy=False)\n",
        "\n",
        "# Se esistono pred calcolate prima e coerenti le uso, altrimenti ricalcolo\n",
        "USE_CACHED = False\n",
        "if \"champion\" in globals():\n",
        "    if champion == \"A\" and \"pred_tst_A\" in globals() and isinstance(pred_tst_A, (np.ndarray, list)):\n",
        "        y_pred_t = np.asarray(pred_tst_A, dtype=np.float64); USE_CACHED = True\n",
        "    elif champion == \"B\" and \"pred_tst_B\" in globals() and isinstance(pred_tst_B, (np.ndarray, list)):\n",
        "        y_pred_t = np.asarray(pred_tst_B, dtype=np.float64); USE_CACHED = True\n",
        "\n",
        "if not USE_CACHED:\n",
        "    champ = champion if \"champion\" in globals() else \"A\"\n",
        "    # scegli estimator e lista colonne\n",
        "    if champ == \"A\":\n",
        "        est  = _pick_first(\"ttr_A\", \"pipe_A\", \"chosen_pipe\")\n",
        "        cols = _pick_first(\"features_A\") or ([*cat_cols, *num_cols] if \"cat_cols\" in globals() and \"num_cols\" in globals() else list(df_test.columns))\n",
        "    else:\n",
        "        est  = _pick_first(\"ttr_B\", \"pipe_B\", \"chosen_pipe\")\n",
        "        cols = _pick_first(\"features_B\") or ([*cat_cols, *num_cols_B] if \"cat_cols\" in globals() and \"num_cols_B\" in globals() else list(df_test.columns))\n",
        "    if est is None:\n",
        "        raise RuntimeError(\"Nessuna pipeline (ttr_*/pipe_*) disponibile per ricalcolare le predizioni TEST.\")\n",
        "\n",
        "    print(f\"Estimator scelto: {type(est).__name__} | n_cols={len(cols)} | esempi colonne: {cols[:10]}\")\n",
        "    missing = [c for c in cols if c not in df_test.columns]\n",
        "    if missing:\n",
        "        print(f\"⚠️ Mancano {len(missing)} colonne nel TEST (verranno imputate): {missing[:10]}\")\n",
        "\n",
        "    y_pred_t = _predict_nat_safely(est, df_test, cols, log_cap=float(TRAIN_CFG.get(\"log_cap_clip\", 12.0)))\n",
        "\n",
        "# Garantisci che sia 1D della lunghezza giusta\n",
        "y_pred_t = np.asarray(y_pred_t)\n",
        "if y_pred_t.ndim == 0:\n",
        "    # se è uno scalare (es. NaN), replico per evitare IndexError e fallisco con messaggio chiaro dopo\n",
        "    y_pred_t = np.full_like(y_true_t, fill_value=np.nan, dtype=np.float64)\n",
        "\n",
        "if y_pred_t.shape[0] != y_true_t.shape[0]:\n",
        "    raise RuntimeError(f\"Dimension mismatch: y_pred_t={y_pred_t.shape}, y_true_t={y_true_t.shape}. \"\n",
        "                       \"Controlla la lista colonne usata per il predict.\")\n",
        "\n",
        "non_finite_mask = ~(np.isfinite(y_true_t) & np.isfinite(y_pred_t))\n",
        "n_bad = int(non_finite_mask.sum())\n",
        "if n_bad > 0:\n",
        "    bad_idx = np.where(non_finite_mask)[0][:10].tolist()\n",
        "    print(f\"⚠️ Righe non finite (y_true/y_pred): {n_bad} / {len(y_true_t)}. Esempi idx: {bad_idx}\")\n",
        "\n",
        "mask = np.isfinite(y_true_t) & np.isfinite(y_pred_t)\n",
        "y_true_t = y_true_t[mask]\n",
        "y_pred_t = y_pred_t[mask]\n",
        "\n",
        "if len(y_true_t) < max(10, int(0.3 * len(df_test))):\n",
        "    raise RuntimeError(\n",
        "        \"Predizioni/target TEST contengono troppi NaN/Inf anche dopo clipping.\\n\"\n",
        "        \"- Verifica che la pipeline scelta sia *quella allenata* (ttr_A/ttr_B o pipe_A/pipe_B).\\n\"\n",
        "        \"- Stai passando le *stesse colonne* usate in training (features_A/B)?\\n\"\n",
        "        \"- Guarda i percentili dei log-pred stampati sopra: se sono enormi, aumenta TRAIN_CFG.log_cap_clip o rivedi il modello.\"\n",
        "    )\n",
        "\n",
        "# Residuali e plot\n",
        "residuals = y_true_t - y_pred_t\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.hist(residuals, bins=60, density=True)\n",
        "plt.title(f\"Residuals (TEST, champion {champion if 'champion' in globals() else 'A'})\")\n",
        "plt.xlabel(\"y − ŷ (k€)\"); plt.ylabel(\"Density\")\n",
        "plt.tight_layout()\n",
        "out_res = FIG_DIR / \"rf_residuals_test_hist.png\"\n",
        "plt.savefig(out_res, dpi=150, bbox_inches=\"tight\"); plt.close()\n",
        "print(\"Saved:\", out_res)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.scatter(y_pred_t, residuals, s=10, alpha=0.6)\n",
        "plt.axhline(0.0, linestyle=\"--\")\n",
        "plt.title(f\"Residuals vs Pred (TEST, champion {champion if 'champion' in globals() else 'A'})\")\n",
        "plt.xlabel(\"ŷ (k€)\"); plt.ylabel(\"y − ŷ (k€)\")\n",
        "plt.tight_layout()\n",
        "out_sc = FIG_DIR / \"rf_residuals_vs_pred_test.png\"\n",
        "plt.savefig(out_sc, dpi=150, bbox_inches=\"tight\"); plt.close()\n",
        "print(\"Saved:\", out_sc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02f8b0c9-97a8-474b-aac3-e31c9cdb58c2",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Valuations for Segments (decils e location) & Predictions Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "fe014bd9-acd1-4be4-989a-16455c836842",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\metrics_by_decile.csv\n",
            "Saved: c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\metrics_by_decile.parquet\n",
            "Saved: c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\metrics_by_location.csv\n",
            "Saved: c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\metrics_by_location.parquet\n",
            "Saved: c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\predictions_test.parquet\n",
            "Saved: c:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\notebooks\\outputs\\modeling\\predictions_test.csv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>asset_id</th>\n",
              "      <th>location</th>\n",
              "      <th>valuation_k</th>\n",
              "      <th>y_pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>asset_000000</td>\n",
              "      <td>Venice</td>\n",
              "      <td>251.649994</td>\n",
              "      <td>287.803001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>asset_000008</td>\n",
              "      <td>Milan</td>\n",
              "      <td>633.919983</td>\n",
              "      <td>659.403420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>asset_000019</td>\n",
              "      <td>Florence</td>\n",
              "      <td>394.690002</td>\n",
              "      <td>377.674415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>asset_000020</td>\n",
              "      <td>Venice</td>\n",
              "      <td>763.880005</td>\n",
              "      <td>636.731167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>asset_000022</td>\n",
              "      <td>Cagliari</td>\n",
              "      <td>170.759995</td>\n",
              "      <td>196.105951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>asset_000029</td>\n",
              "      <td>Genoa</td>\n",
              "      <td>188.699997</td>\n",
              "      <td>174.740513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>asset_000038</td>\n",
              "      <td>Rome</td>\n",
              "      <td>417.899994</td>\n",
              "      <td>418.842916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>asset_000040</td>\n",
              "      <td>Turin</td>\n",
              "      <td>468.970001</td>\n",
              "      <td>415.999758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>asset_000075</td>\n",
              "      <td>Rome</td>\n",
              "      <td>216.440002</td>\n",
              "      <td>223.960463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>asset_000088</td>\n",
              "      <td>Milan</td>\n",
              "      <td>420.239990</td>\n",
              "      <td>510.640201</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        asset_id  location  valuation_k      y_pred\n",
              "0   asset_000000    Venice   251.649994  287.803001\n",
              "8   asset_000008     Milan   633.919983  659.403420\n",
              "19  asset_000019  Florence   394.690002  377.674415\n",
              "20  asset_000020    Venice   763.880005  636.731167\n",
              "22  asset_000022  Cagliari   170.759995  196.105951\n",
              "29  asset_000029     Genoa   188.699997  174.740513\n",
              "38  asset_000038      Rome   417.899994  418.842916\n",
              "40  asset_000040     Turin   468.970001  415.999758\n",
              "75  asset_000075      Rome   216.440002  223.960463\n",
              "88  asset_000088     Milan   420.239990  510.640201"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 08 (refactor) — Breakdown per decili/location + export predizioni TEST (robusto)\n",
        "\n",
        "from __future__ import annotations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "\n",
        "from notebooks.shared.common.constants import ASSET_ID, LOCATION, VALUATION_K\n",
        "\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- helper numerico ---\n",
        "LOG_CAP = float(TRAIN_CFG.get(\"log_cap_clip\", 12.0))  # limita i log-pred per evitare overflow\n",
        "\n",
        "def _expm1_safe(z, cap: float = LOG_CAP):\n",
        "    z = np.asarray(z, dtype=np.float64)\n",
        "    z = np.clip(z, -20.0, cap)\n",
        "    out = np.expm1(z)\n",
        "    out[out < 0] = 0.0\n",
        "    return out\n",
        "\n",
        "def _ensure_cols(df_part: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
        "    dfp = df_part.copy()\n",
        "    miss = [c for c in cols if c not in dfp.columns]\n",
        "    if miss:\n",
        "        for c in miss:\n",
        "            dfp[c] = np.nan\n",
        "        print(f\"ℹ️ Aggiunte {len(miss)} colonne mancanti allo split TEST (imputate): {miss[:10]}\")\n",
        "    return dfp[cols]\n",
        "\n",
        "def _pick_first(*names):\n",
        "    for n in names:\n",
        "        if n in globals() and globals()[n] is not None:\n",
        "            return globals()[n]\n",
        "    return None\n",
        "\n",
        "def _predict_nat_test_from(est, X_df: pd.DataFrame, cols: list[str]) -> np.ndarray:\n",
        "    \"\"\"Predizioni in scala naturale (k€) con auto-gestione TTR vs pipeline log1p.\"\"\"\n",
        "    X_use = _ensure_cols(X_df, cols)\n",
        "    if isinstance(est, TransformedTargetRegressor):\n",
        "        pred_nat = est.predict(X_use)                # già in k€\n",
        "    else:\n",
        "        log_pred = est.predict(X_use)                # log1p(y)\n",
        "        pred_nat = _expm1_safe(log_pred)\n",
        "    return np.asarray(pred_nat, dtype=np.float64)\n",
        "\n",
        "# --- helper: groupby.apply compatibile con pandas nuovi/vecchi\n",
        "def _group_apply_safe(df: pd.DataFrame, key: str, func):\n",
        "    gb = df.groupby(key, observed=True)\n",
        "    try:\n",
        "        # pandas ≥ 2.2: esclude automaticamente la colonna di raggruppamento\n",
        "        out = gb.apply(func, include_groups=False)\n",
        "    except TypeError:\n",
        "        # pandas < 2.2: selezioniamo esplicitamente solo le colonne utili\n",
        "        out = gb[[\"y_true\", \"y_pred\"]].apply(func)\n",
        "    return out.reset_index()\n",
        "\n",
        "# --- prendi/ricostruisci y_pred_t in scala naturale (k€) ---\n",
        "def _get_champion_pred_test() -> np.ndarray:\n",
        "    # usa cache se valida\n",
        "    if \"y_pred_t\" in globals() and isinstance(y_pred_t, (np.ndarray, list)) and len(y_pred_t) == len(df_test):\n",
        "        return np.asarray(y_pred_t, dtype=np.float64)\n",
        "\n",
        "    if \"champion\" not in globals():\n",
        "        raise RuntimeError(\"Variabile 'champion' mancante.\")\n",
        "\n",
        "    # pred già calcolate in k€?\n",
        "    if champion == \"A\" and \"pred_tst_A\" in globals():\n",
        "        return np.asarray(pred_tst_A, dtype=np.float64)\n",
        "    if champion == \"B\" and \"pred_tst_B\" in globals():\n",
        "        return np.asarray(pred_tst_B, dtype=np.float64)\n",
        "\n",
        "    # altrimenti ricalcola\n",
        "    if champion == \"A\":\n",
        "        est  = _pick_first(\"ttr_A\", \"pipe_A\", \"chosen_pipe\")\n",
        "        cols = _pick_first(\"features_A\") or ([*cat_cols, *num_cols] if \"cat_cols\" in globals() and \"num_cols\" in globals() else list(df_test.columns))\n",
        "    else:\n",
        "        est  = _pick_first(\"ttr_B\", \"pipe_B\", \"chosen_pipe\")\n",
        "        cols = _pick_first(\"features_B\") or ([*cat_cols, *num_cols_B] if \"cat_cols\" in globals() and \"num_cols_B\" in globals() else list(df_test.columns))\n",
        "\n",
        "    if est is None:\n",
        "        raise RuntimeError(\"Nessuna pipeline disponibile per ricalcolare le predizioni TEST.\")\n",
        "\n",
        "    return _predict_nat_test_from(est, df_test, cols)\n",
        "\n",
        "# --- costruisci dataset metrico pulito ---\n",
        "y_true_t = df_test[VALUATION_K].to_numpy(dtype=np.float64, copy=False)\n",
        "y_pred_t = _get_champion_pred_test()\n",
        "\n",
        "# sanifica NaN/Inf\n",
        "mask = np.isfinite(y_true_t) & np.isfinite(y_pred_t)\n",
        "valid_n = int(mask.sum())\n",
        "if valid_n < max(30, int(0.3 * len(y_true_t))):\n",
        "    bad_idx = np.where(~mask)[0][:10].tolist()\n",
        "    raise RuntimeError(\n",
        "        f\"Predizioni TEST non finite: validi {valid_n}/{len(mask)}. \"\n",
        "        f\"Esempi idx problematici: {bad_idx}. Verifica allineamento feature e scala predizioni.\"\n",
        "    )\n",
        "\n",
        "yt = y_true_t[mask]\n",
        "yh = y_pred_t[mask]\n",
        "idx_valid = df_test.index[mask]\n",
        "\n",
        "cols_keep = [ASSET_ID] if ASSET_ID in df_test.columns else []\n",
        "if LOCATION in df_test.columns:\n",
        "    cols_keep.append(LOCATION)\n",
        "\n",
        "dfm = df_test.loc[idx_valid, cols_keep].copy()\n",
        "dfm[\"y_true\"] = yt\n",
        "dfm[\"y_pred\"] = yh\n",
        "\n",
        "# --- decili sul target naturale ---\n",
        "try:\n",
        "    dfm[\"decile\"] = pd.qcut(dfm[\"y_true\"], q=10, labels=False, duplicates=\"drop\")\n",
        "except Exception:\n",
        "    dfm[\"decile\"] = 0\n",
        "\n",
        "# --- funzioni metriche ---\n",
        "def _rmse(y_true, y_pred):\n",
        "    try:\n",
        "        return mean_squared_error(y_true, y_pred, squared=False)\n",
        "    except TypeError:\n",
        "        return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "def _agg_metrics(g: pd.DataFrame) -> pd.Series:\n",
        "    return pd.Series({\n",
        "        \"n\": int(len(g)),\n",
        "        \"MAE\": float(mean_absolute_error(g[\"y_true\"], g[\"y_pred\"])),\n",
        "        \"RMSE\": float(_rmse(g[\"y_true\"], g[\"y_pred\"])),\n",
        "        \"R2\": float(r2_score(g[\"y_true\"], g[\"y_pred\"])) if len(g) > 1 else np.nan,\n",
        "    })\n",
        "\n",
        "# --- Breakdown per decile ---\n",
        "dec_rep = _group_apply_safe(dfm, \"decile\", _agg_metrics)\n",
        "dec_rep.to_csv(MODEL_DIR / \"metrics_by_decile.csv\", index=False)\n",
        "dec_rep.to_parquet(MODEL_DIR / \"metrics_by_decile.parquet\", index=False)\n",
        "print(\"Saved:\", MODEL_DIR / \"metrics_by_decile.csv\")\n",
        "print(\"Saved:\", MODEL_DIR / \"metrics_by_decile.parquet\")\n",
        "\n",
        "# --- Breakdown per location ---\n",
        "if LOCATION in dfm.columns:\n",
        "    loc_rep = _group_apply_safe(dfm, LOCATION, _agg_metrics)\n",
        "else:\n",
        "    loc_rep = pd.DataFrame([{\n",
        "        LOCATION: \"NA\",\n",
        "        \"n\": int(len(dfm)),\n",
        "        \"MAE\": float(mean_absolute_error(dfm[\"y_true\"], dfm[\"y_pred\"])) if len(dfm) else np.nan,\n",
        "        \"RMSE\": float(_rmse(dfm[\"y_true\"], dfm[\"y_pred\"])) if len(dfm) else np.nan,\n",
        "        \"R2\": float(r2_score(dfm[\"y_true\"], dfm[\"y_pred\"])) if len(dfm) > 1 else np.nan,\n",
        "    }])\n",
        "\n",
        "loc_rep.to_csv(MODEL_DIR / \"metrics_by_location.csv\", index=False)\n",
        "loc_rep.to_parquet(MODEL_DIR / \"metrics_by_location.parquet\", index=False)\n",
        "print(\"Saved:\", MODEL_DIR / \"metrics_by_location.csv\")\n",
        "print(\"Saved:\", MODEL_DIR / \"metrics_by_location.parquet\")\n",
        "\n",
        "# --- export predizioni TEST ---\n",
        "pred_cols = []\n",
        "if ASSET_ID in dfm.columns: pred_cols.append(ASSET_ID)\n",
        "if LOCATION in dfm.columns: pred_cols.append(LOCATION)\n",
        "pred_df = dfm[pred_cols + [\"y_true\", \"y_pred\"]].rename(columns={\"y_true\": VALUATION_K})\n",
        "\n",
        "pred_df.to_parquet(MODEL_DIR / \"predictions_test.parquet\", index=False)\n",
        "pred_df.to_csv(MODEL_DIR / \"predictions_test.csv\", index=False, encoding=\"utf-8\")\n",
        "print(\"Saved:\", MODEL_DIR / \"predictions_test.parquet\")\n",
        "print(\"Saved:\", MODEL_DIR / \"predictions_test.csv\")\n",
        "\n",
        "display(pred_df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bf1f075-89a4-4e49-a30c-2031f17ab419",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Model Persistence & Manifest Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "701beb69",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Champion: A | n_cols=27\n"
          ]
        }
      ],
      "source": [
        "# 08.9) Deriva chosen_pipe / chosen_cols dal champion (robusto)\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "def _cols_from_ct(prep: ColumnTransformer) -> list[str]:\n",
        "    \"\"\"Estrae i nomi colonna dichiarati nel ColumnTransformer (solo selector per nome).\"\"\"\n",
        "    cols = []\n",
        "    if hasattr(prep, \"transformers\"):\n",
        "        for name, trans, sel in prep.transformers:\n",
        "            if name == \"remainder\":\n",
        "                continue\n",
        "            if isinstance(sel, (list, tuple)):\n",
        "                cols.extend([c for c in sel if isinstance(c, str)])\n",
        "    # dedup preservando ordine\n",
        "    return list(dict.fromkeys(cols))\n",
        "\n",
        "def _pipe_cols(pipe: Pipeline) -> list[str]:\n",
        "    if \"prep\" in pipe.named_steps and isinstance(pipe.named_steps[\"prep\"], ColumnTransformer):\n",
        "        return _cols_from_ct(pipe.named_steps[\"prep\"])\n",
        "    # fallback: prova a trovare il primo ColumnTransformer nella pipeline\n",
        "    for _, step in pipe.steps:\n",
        "        if isinstance(step, ColumnTransformer):\n",
        "            return _cols_from_ct(step)\n",
        "    return []\n",
        "\n",
        "# 1) Champion: se non definito, scegli in base al VALID (MAE)\n",
        "if \"champion\" not in globals():\n",
        "    if \"mA_val\" in globals() and \"mB_val\" in globals():\n",
        "        champion = \"A\" if mA_val[\"MAE\"] <= mB_val[\"MAE\"] else \"B\"\n",
        "    else:\n",
        "        champion = \"A\"  # fallback conservativo\n",
        "\n",
        "# 2) Scegli la pipeline e le colonne\n",
        "chosen_pipe = None\n",
        "chosen_cols = None\n",
        "\n",
        "if champion == \"A\" and \"pipe_A\" in globals():\n",
        "    chosen_pipe = pipe_A\n",
        "    if \"features_A\" in globals():\n",
        "        chosen_cols = list(dict.fromkeys(features_A))\n",
        "    elif \"cat_cols\" in globals() and \"num_cols\" in globals():\n",
        "        chosen_cols = list(dict.fromkeys([*cat_cols, *num_cols]))\n",
        "elif champion == \"B\" and \"pipe_B\" in globals():\n",
        "    chosen_pipe = pipe_B\n",
        "    if \"features_B\" in globals():\n",
        "        chosen_cols = list(dict.fromkeys(features_B))\n",
        "    elif \"cat_cols\" in globals() and \"num_cols_B\" in globals():\n",
        "        chosen_cols = list(dict.fromkeys([*cat_cols, *num_cols_B]))\n",
        "\n",
        "# 3) Fallback: infila dal ColumnTransformer se le liste sopra non esistono\n",
        "if (chosen_cols is None or len(chosen_cols) == 0) and \"chosen_pipe\" in globals() and chosen_pipe is not None:\n",
        "    chosen_cols = _pipe_cols(chosen_pipe)\n",
        "\n",
        "# 4) Ultime cinture: tieni solo colonne realmente presenti nel train (se disponibile)\n",
        "if \"df_train\" in globals() and isinstance(df_train, pd.DataFrame):\n",
        "    chosen_cols = [c for c in chosen_cols if c in df_train.columns]\n",
        "\n",
        "# 5) Sanity\n",
        "if chosen_pipe is None or not isinstance(chosen_pipe, Pipeline):\n",
        "    raise RuntimeError(\"Impossibile determinare chosen_pipe (pipe_A/pipe_B mancanti).\")\n",
        "if not chosen_cols:\n",
        "    raise RuntimeError(\"Impossibile derivare chosen_cols: nessuna lista features trovata nel contesto/pipeline.\")\n",
        "\n",
        "print(f\"Champion: {champion} | n_cols={len(chosen_cols)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a90328c1-1e44-4551-a9e9-bb18e4a8817c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "city_zone_prior NaN (sample 128): 0 / 128\n",
            "✅ Saved feature_order: outputs\\modeling\\property\\feature_order.json\n",
            "✅ Saved serving v2 pipeline: outputs\\modeling\\property\\value_regressor_v2.joblib\n",
            "✅ Saved meta: outputs\\modeling\\property\\value_regressor_v2_meta.json\n",
            "✅ Saved manifest: outputs\\modeling\\property\\training_manifest.json\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "tuple indices must be integers or slices, not str",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 355\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshared\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msanity_checks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m leakage_gate\n\u001b[32m    354\u001b[39m leakage_report = leakage_gate(X_train.assign(target=y_train))\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m leakage_report[\u001b[33m\"\u001b[39m\u001b[33mok\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLeakage gate failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mleakage_report\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
            "\u001b[31mTypeError\u001b[39m: tuple indices must be integers or slices, not str"
          ]
        }
      ],
      "source": [
        "# 09) Persistenza modello (RF champion) + Serving v2 — outputs/modeling/property\n",
        "from __future__ import annotations\n",
        "\n",
        "import json, os, hashlib, inspect\n",
        "from datetime import datetime, timezone\n",
        "from pathlib import Path\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
        "from sklearn.base import clone\n",
        "\n",
        "# === trasformatori SERVING importabili (niente FunctionTransformer locali) ===\n",
        "from notebooks.shared.common.serving_transformers import GeoCanonizer, PriorsGuard\n",
        "# Deriver: prova quello “ufficiale”, altrimenti un fallback classe-based dal modulo serving\n",
        "try:\n",
        "    from notebooks.shared.common.transformers import PropertyDerivedFeatures as _Deriver  # type: ignore\n",
        "except Exception:\n",
        "    try:\n",
        "        from notebooks.shared.common.serving_transformers import BasicDeriver as _Deriver  # <- fallback safe\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\n",
        "            \"Nessun deriver importabile trovato. Installa/aggiungi \"\n",
        "            \"`PropertyDerivedFeatures` o `BasicDeriver` nel modulo serving_transformers.\"\n",
        "        ) from e\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Cartelle\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "BASE_OUT   = Path(\"outputs\")\n",
        "MODEL_DIR  = BASE_OUT / \"modeling\"\n",
        "FIG_DIR    = MODEL_DIR / \"figures\"\n",
        "ART_DIR    = MODEL_DIR / \"artifacts\"\n",
        "PROP_DIR   = MODEL_DIR / \"property\"\n",
        "for d in (MODEL_DIR, FIG_DIR, ART_DIR, PROP_DIR):\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Sanity sul champion\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "from sklearn.compose import ColumnTransformer as _CT  # solo per isinstance check tipizzato\n",
        "\n",
        "if \"chosen_pipe\" not in globals() or not isinstance(chosen_pipe, Pipeline):\n",
        "    raise RuntimeError(\"chosen_pipe non definito o non è una sklearn Pipeline.\")\n",
        "\n",
        "# Ricava chosen_cols dal champion se mancano\n",
        "if \"chosen_cols\" not in globals() or not chosen_cols:\n",
        "    if \"champion\" in globals() and champion in (\"A\", \"B\"):\n",
        "        if champion == \"A\":\n",
        "            chosen_cols = list(dict.fromkeys([*(cat_cols or []), *(num_cols or [])]))\n",
        "        else:\n",
        "            chosen_cols = list(dict.fromkeys([*(cat_cols or []), *(num_cols_B or [])]))\n",
        "    else:\n",
        "        raise RuntimeError(\"chosen_cols mancante e champion non determinabile.\")\n",
        "\n",
        "if \"cat_cols\" not in globals():\n",
        "    cat_cols = []\n",
        "\n",
        "VALUATION_K = \"valuation_k\"\n",
        "SEED = int(globals().get(\"SEED\", 42))\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Helper\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "from notebooks.shared.common.utils import canonical_json_dumps  # usato più sotto\n",
        "\n",
        "def _sha256_file(p: Path, chunk: int = 1 << 20) -> str:\n",
        "    h = hashlib.sha256()\n",
        "    with p.open(\"rb\") as f:\n",
        "        for ch in iter(lambda: f.read(chunk), b\"\"):\n",
        "            h.update(ch)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def _find_preproc(pipe: Pipeline) -> ColumnTransformer:\n",
        "    if \"prep\" in pipe.named_steps and isinstance(pipe.named_steps[\"prep\"], _CT):\n",
        "        return pipe.named_steps[\"prep\"]\n",
        "    for _, step in pipe.steps:\n",
        "        if isinstance(step, _CT):\n",
        "            return step\n",
        "    raise RuntimeError(\"Nessuno step ColumnTransformer trovato nella pipeline (prep).\")\n",
        "\n",
        "def _find_regressor(pipe: Pipeline):\n",
        "    return pipe.steps[-1][1]\n",
        "\n",
        "def _rel_to_prop(p: Path | None) -> str | None:\n",
        "    if not p: return None\n",
        "    try: return os.path.relpath(p, PROP_DIR).replace(\"\\\\\", \"/\")\n",
        "    except Exception: return p.as_posix()\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Priors & derivate (coerenti con il training)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "DERIVED_FEATURES = [\n",
        "    \"log_size_m2\",\"sqm_per_room\",\"baths_per_100sqm\",\n",
        "    \"elev_x_floor\",\"no_elev_high_floor\",\"rooms_per_100sqm\",\n",
        "    \"city_zone_prior\",\"region_index_prior\",\n",
        "]\n",
        "DERIVED_SET = set(DERIVED_FEATURES)\n",
        "\n",
        "# Config per priors\n",
        "from notebooks.shared.common.config import ASSET_CONFIG\n",
        "_PROP = ASSET_CONFIG[\"property\"]\n",
        "CITY_BASE = {str(c).lower(): {str(z).lower(): float(v) for z, v in zv.items()}\n",
        "             for c, zv in (_PROP.get(\"city_base_prices\") or {}).items()}\n",
        "REGION_INDEX = {str(k).lower(): float(v) for k, v in (_PROP.get(\"region_index\") or {\n",
        "    \"north\": 1.05, \"center\": 1.00, \"south\": 0.92\n",
        "}).items()}\n",
        "\n",
        "# Fallback robusti per city_zone_prior\n",
        "_ZONE_KEYS = set(z for d in CITY_BASE.values() for z in d.keys())\n",
        "_ZONE_MED = {z: float(np.nanmedian([d.get(z, np.nan) for d in CITY_BASE.values()]))\n",
        "             for z in _ZONE_KEYS} if CITY_BASE else {}\n",
        "_GLOBAL_CITYZONE_MED = float(np.nanmedian([v for d in CITY_BASE.values() for v in d.values()])) if CITY_BASE else 0.0\n",
        "\n",
        "# Deriver istanza (classe importabile → pickle-safe) con adattamento alla firma\n",
        "def _build_deriver(Cls):\n",
        "    \"\"\"\n",
        "    Instanzia il deriver passando solo gli argomenti supportati dalla sua __init__.\n",
        "    Compatibile con:\n",
        "      - PropertyDerivedFeatures(city_base=..., region_index=..., [flag...])\n",
        "      - BasicDeriver() o BasicDeriver(city_base=..., region_index=...)\n",
        "    \"\"\"\n",
        "    candidate_kwargs = dict(\n",
        "        make_log_size=True,\n",
        "        make_sqm_per_room=True,\n",
        "        make_baths_per_100sqm=True,\n",
        "        make_elev_x_floor=True,\n",
        "        make_no_elev_penalty=True,\n",
        "        make_city_zone_prior=True,\n",
        "        make_region_macro_prior=True,\n",
        "        city_base=CITY_BASE,\n",
        "        region_index=REGION_INDEX,\n",
        "        zone_medians=_ZONE_MED,\n",
        "        global_cityzone_median=_GLOBAL_CITYZONE_MED,\n",
        "    )\n",
        "    try:\n",
        "        sig = inspect.signature(Cls.__init__)\n",
        "        allowed = {k for k in sig.parameters.keys() if k != \"self\"}\n",
        "        kwargs = {k: v for k, v in candidate_kwargs.items() if k in allowed}\n",
        "        return Cls(**kwargs)\n",
        "    except TypeError:\n",
        "        base_kwargs = {}\n",
        "        for k in (\"city_base\", \"region_index\"):\n",
        "            if k in allowed:\n",
        "                base_kwargs[k] = candidate_kwargs[k]\n",
        "        return Cls(**base_kwargs) if base_kwargs else Cls()\n",
        "\n",
        "feature_deriver = _build_deriver(_Deriver)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Loader X/y: usa solo RAW + basi minime per derivate\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "def _load_training_Xy(cols_raw: list[str]) -> tuple[pd.DataFrame, np.ndarray, np.ndarray | None]:\n",
        "    \"\"\"\n",
        "    Ritorna X (DF), y (k€), w (o None). Aggiunge sempre le basi necessarie\n",
        "    alle derivate così gli step di serving possono lavorare correttamente.\n",
        "    \"\"\"\n",
        "    base_needed = [\"size_m2\",\"rooms\",\"bathrooms\",\"floor\",\"city\",\"zone\",\"region\",\"location\"]\n",
        "    cols_plus_base = list(dict.fromkeys([*cols_raw, *base_needed]))\n",
        "\n",
        "    def _prepare(df: pd.DataFrame):\n",
        "        X = df.reindex(columns=cols_plus_base)\n",
        "        # riempi basi ragionevoli per evitare colonne tutte NaN\n",
        "        if \"city\" in X.columns and \"location\" in df.columns:\n",
        "            X[\"city\"] = X[\"city\"].fillna(df[\"location\"])\n",
        "        if \"zone\" in X.columns:\n",
        "            X[\"zone\"] = X[\"zone\"].fillna(\"semi_center\")\n",
        "        if \"region\" in X.columns:\n",
        "            X[\"region\"] = X[\"region\"].fillna(\"center\")\n",
        "\n",
        "        y = pd.to_numeric(df[VALUATION_K], errors=\"coerce\").to_numpy(dtype=\"float64\")\n",
        "        w_out = None\n",
        "        if \"sample_weight\" in df.columns:\n",
        "            w_out = pd.to_numeric(df[\"sample_weight\"], errors=\"coerce\").to_numpy(dtype=\"float64\")\n",
        "        return X, y, w_out\n",
        "\n",
        "    if \"df_train\" in globals() and isinstance(df_train, pd.DataFrame) and VALUATION_K in df_train.columns:\n",
        "        return _prepare(df_train)\n",
        "\n",
        "    candidates = []\n",
        "    if \"data_path\" in globals():\n",
        "        try: candidates.append(Path(str(data_path)))\n",
        "        except Exception: pass\n",
        "    candidates += [BASE_OUT / \"dataset_generated.parquet\", BASE_OUT / \"dataset_generated.csv\"]\n",
        "\n",
        "    for p in candidates:\n",
        "        if p and p.exists():\n",
        "            df = pd.read_parquet(p) if p.suffix.lower() in {\".parquet\", \".pq\"} else pd.read_csv(p)\n",
        "            if VALUATION_K not in df.columns:\n",
        "                continue\n",
        "            return _prepare(df)\n",
        "\n",
        "    raise RuntimeError(\"Impossibile ricostruire X/y (e pesi) per il fit della serving pipeline.\")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Salvataggio legacy per retro-compat\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "legacy_model_path = ART_DIR / f\"rf_champion_{globals().get('champion','A')}.joblib\"\n",
        "joblib.dump(chosen_pipe, legacy_model_path)\n",
        "legacy_sha = _sha256_file(legacy_model_path)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Serving pipeline v2 (GeoCanonizer → Deriver → PriorsGuard → prep → TTR)\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "preproc  = clone(_find_preproc(chosen_pipe))\n",
        "base_reg = _find_regressor(chosen_pipe)\n",
        "RegCls   = base_reg.__class__\n",
        "reg_params = base_reg.get_params()\n",
        "\n",
        "serving_pipe_v2 = Pipeline(steps=[\n",
        "    (\"canon_geo\",   GeoCanonizer()),\n",
        "    (\"derive\",      feature_deriver),  # classe importabile\n",
        "    (\"priors_guard\",PriorsGuard(\n",
        "        city_base=CITY_BASE,\n",
        "        region_index=REGION_INDEX,\n",
        "        zone_medians=_ZONE_MED,\n",
        "        global_cityzone_median=_GLOBAL_CITYZONE_MED,\n",
        "    )),\n",
        "    (\"prep\",        preproc),\n",
        "    (\"ttr\",         TransformedTargetRegressor(\n",
        "        regressor=RegCls(**reg_params),\n",
        "        func=np.log1p, inverse_func=np.expm1, check_inverse=False\n",
        "    )),\n",
        "])\n",
        "\n",
        "# Fit: SOLO colonne raw (le derivate le crea la pipeline)\n",
        "chosen_cols_raw = [c for c in chosen_cols if c not in DERIVED_SET]\n",
        "X_fit, y_fit, w_fit = _load_training_Xy(chosen_cols_raw)\n",
        "\n",
        "fit_kwargs = {}\n",
        "if w_fit is not None and np.isfinite(w_fit).any():\n",
        "    fit_kwargs = {\"ttr__sample_weight\": w_fit}\n",
        "\n",
        "# diagnostica soft prima del fit (no fit richiesto sugli step stateless)\n",
        "X_probe = X_fit.iloc[:128].copy()\n",
        "X_probe = serving_pipe_v2.named_steps[\"canon_geo\"].transform(X_probe)\n",
        "X_probe = serving_pipe_v2.named_steps[\"derive\"].transform(X_probe)\n",
        "X_probe = serving_pipe_v2.named_steps[\"priors_guard\"].transform(X_probe)\n",
        "\n",
        "print(\"city_zone_prior NaN (sample 128):\",\n",
        "      int(pd.to_numeric(X_probe.get(\"city_zone_prior\"), errors=\"coerce\").isna().sum()),\n",
        "      \"/\", len(X_probe))\n",
        "\n",
        "# --- Leakage gate (best-effort, non-blocking) ---\n",
        "try:\n",
        "    from notebooks.shared.common.sanity_checks import leakage_gate\n",
        "    _leak_df = X_fit.copy()\n",
        "    _leak_df[\"valuation_k\"] = y_fit\n",
        "    leakage_report = leakage_gate(_leak_df, target_col=\"valuation_k\")\n",
        "    assert leakage_report.get(\"ok\", True), f\"Leakage gate failed: {leakage_report}\"\n",
        "    print(\"✅ Leakage gate: OK\")\n",
        "except AssertionError as e:\n",
        "    # In demo si può decidere di far fallire esplicitamente\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(\"⚠️ Leakage gate skipped:\", e)\n",
        "\n",
        "serving_pipe_v2.fit(X_fit, y_fit, **fit_kwargs)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Salva pipeline + meta\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "from hashlib import sha256 as _sha256\n",
        "\n",
        "feature_order = list(chosen_cols_raw)\n",
        "FEATURES_FILE = (PROP_DIR / \"feature_order.json\")\n",
        "FEATURES_FILE.write_text(canonical_json_dumps(feature_order), encoding=\"utf-8\")\n",
        "feature_order_sha256 = _sha256(FEATURES_FILE.read_bytes()).hexdigest()\n",
        "\n",
        "# 2) Salva pipeline + meta canonico\n",
        "pipe_path = PROP_DIR / \"value_regressor_v2.joblib\"\n",
        "joblib.dump(serving_pipe_v2, pipe_path)\n",
        "pipeline_sha = _sha256_file(pipe_path)\n",
        "\n",
        "meta_path = PROP_DIR / \"value_regressor_v2_meta.json\"\n",
        "model_meta = {\n",
        "    \"asset_type\": \"property\",\n",
        "    \"task\": \"value_regressor\",\n",
        "    \"model_version\": \"v2\",  # <=16 char (ok per p1.mv)\n",
        "    \"model_class\": RegCls.__name__,\n",
        "    \"trained_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),\n",
        "    \"schema_version\": \"2.0\",\n",
        "    \"model_hash\": pipeline_sha,          # usato come p1.mh\n",
        "    \"pipeline_sha256\": pipeline_sha,     # alias\n",
        "    \"feature_order_sha256\": feature_order_sha256,  # <<< aggiunto\n",
        "    \"n_features\": int(len(chosen_cols_raw)),\n",
        "    \"features_categorical\": list(cat_cols),\n",
        "    \"features_numeric\": [c for c in chosen_cols_raw if c not in set(cat_cols)],\n",
        "    \"target_name\": VALUATION_K,\n",
        "    \"unit\": \"k_eur\",\n",
        "    \"feature_order_path\": str(FEATURES_FILE.resolve().as_posix()),\n",
        "    \"pipeline_path\": str(pipe_path.resolve().as_posix()),\n",
        "}\n",
        "\n",
        "# (best-effort) dataset_sha256 se rintracciabile\n",
        "try:\n",
        "    ds_candidates = [\n",
        "        BASE_OUT / \"dataset_generated.parquet\",\n",
        "        BASE_OUT / \"dataset_generated.csv\",\n",
        "    ]\n",
        "    ds_path = next((p for p in ds_candidates if p.exists()), None)\n",
        "    if ds_path:\n",
        "        model_meta[\"dataset_sha256\"] = _sha256(ds_path.read_bytes()).hexdigest()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "meta_path.write_text(canonical_json_dumps(model_meta), encoding=\"utf-8\")\n",
        "print(\"✅ Saved feature_order:\", FEATURES_FILE)\n",
        "print(\"✅ Saved serving v2 pipeline:\", pipe_path)\n",
        "print(\"✅ Saved meta:\", meta_path)\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "# Manifest v2\n",
        "# ─────────────────────────────────────────────────────────────────────────────\n",
        "manifest_path = PROP_DIR / \"training_manifest.json\"\n",
        "existing = {}\n",
        "if manifest_path.exists():\n",
        "    try:\n",
        "        existing = json.loads(manifest_path.read_text(encoding=\"utf-8\"))\n",
        "    except Exception:\n",
        "        existing = {}\n",
        "\n",
        "paths = dict(existing.get(\"paths\", {}))\n",
        "paths.update({\n",
        "    \"pipeline_path\": str(pipe_path.resolve().as_posix()),\n",
        "    \"pipeline_sha256\": pipeline_sha,\n",
        "    \"feature_order\": str(FEATURES_FILE.resolve().as_posix()),\n",
        "    \"meta_path\": str(meta_path.resolve().as_posix()),\n",
        "    \"predictions_test_csv\": str((MODEL_DIR / \"predictions_test.csv\").resolve().as_posix()),\n",
        "    \"predictions_test_parquet\": str((MODEL_DIR / \"predictions_test.parquet\").resolve().as_posix()),\n",
        "    \"metrics_by_decile_csv\": str((MODEL_DIR / \"metrics_by_decile.csv\").resolve().as_posix()),\n",
        "    \"metrics_by_location_csv\": str((MODEL_DIR / \"metrics_by_location.csv\").resolve().as_posix()),\n",
        "})\n",
        "\n",
        "# merge metrics (se 'metrics' è definita prima nella notebook, mantienila)\n",
        "metrics = dict(existing.get(\"metrics\", {})) | dict(metrics or {})\n",
        "\n",
        "# (best-effort) aggiungi splits/decili se variabili esistono già\n",
        "splits = {}\n",
        "deciles = {}\n",
        "try:\n",
        "    if \"X_train\" in globals(): splits[\"train_rows\"] = int(len(X_train))\n",
        "    if \"X_valid\" in globals(): splits[\"valid_rows\"] = int(len(X_valid))\n",
        "    if \"X_test\"  in globals(): splits[\"test_rows\"]  = int(len(X_test))\n",
        "    if \"d_train\" in globals():\n",
        "        deciles[\"train\"] = {int(k): int((d_train==k).sum()) for k in range(10)}\n",
        "    if \"d_valid\" in globals():\n",
        "        deciles[\"valid\"] = {int(k): int((d_valid==k).sum()) for k in range(10)}\n",
        "    if \"d_test\" in globals():\n",
        "        deciles[\"test\"]  = {int(k): int((d_test==k).sum())  for k in range(10)}\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "feature_config = {\n",
        "    \"categorical\": list(cat_cols),\n",
        "    \"numeric\": [c for c in chosen_cols_raw if c not in set(cat_cols)],\n",
        "    \"excluded\": sorted(list(globals().get(\"exclude\", []))) if \"exclude\" in globals() else [],\n",
        "}\n",
        "\n",
        "manifest_new = {\n",
        "    \"generated_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),\n",
        "    \"schema_version\": \"2.0\",\n",
        "    \"asset_type\": \"property\",\n",
        "    \"task\": \"value_regressor\",\n",
        "    \"seed\": int(SEED),\n",
        "    \"paths\": paths,\n",
        "    \"model_meta\": {\n",
        "        \"model_version\": \"v2\",\n",
        "        \"model_class\": RegCls.__name__,\n",
        "        \"model_hash\": pipeline_sha,\n",
        "        \"feature_order_sha256\": feature_order_sha256,  # <<< aggiunto\n",
        "    },\n",
        "    \"metrics\": metrics,\n",
        "    \"feature_config\": feature_config,\n",
        "    \"expected_features\": {\n",
        "        \"categorical\": feature_config[\"categorical\"],\n",
        "        \"numeric\": feature_config[\"numeric\"],\n",
        "    },\n",
        "}\n",
        "if splits:  manifest_new[\"splits\"] = splits\n",
        "if deciles: manifest_new[\"decile_distribution\"] = deciles\n",
        "\n",
        "merged = dict(existing); merged.update({k: v for k, v in manifest_new.items() if v is not None})\n",
        "manifest_path.write_text(canonical_json_dumps(merged), encoding=\"utf-8\")\n",
        "print(\"✅ Saved manifest:\", manifest_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "b009b74b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Worst-k salvato: outputs\\modeling\\worst_k.json\n"
          ]
        }
      ],
      "source": [
        "# === Worst-k 10% ===\n",
        "from notebooks.shared.common.utils import canonical_json_dumps\n",
        "import numpy as np, json\n",
        "from pathlib import Path\n",
        "\n",
        "def _worst_k(y_true: np.ndarray, y_pred: np.ndarray, k: float = 0.10) -> dict:\n",
        "    err = np.abs(y_true - y_pred).astype(float)\n",
        "    n = max(1, int(len(err) * k))\n",
        "    top = np.partition(err, -n)[-n:]\n",
        "    return {\n",
        "        \"worst_k\": float(k),\n",
        "        \"worst_k_mean_abs_err\": float(top.mean()),\n",
        "        \"worst_k_max_abs_err\": float(top.max()),\n",
        "        \"worst_k_count\": int(n),\n",
        "    }\n",
        "\n",
        "# Sostituisci dfm_* con il df che usi per le metriche finali (deve avere y_true / y_pred)\n",
        "y_true_np = dfm[\"y_true\"].to_numpy()\n",
        "y_pred_np = dfm[\"y_pred\"].to_numpy()\n",
        "\n",
        "wk = _worst_k(y_true_np, y_pred_np, k=0.10)\n",
        "wk_path = Path(\"outputs/modeling/worst_k.json\")\n",
        "wk_path.write_text(canonical_json_dumps(wk), encoding=\"utf-8\")\n",
        "\n",
        "# Aggiorna manifest training\n",
        "PROP_DIR = Path(\"outputs/modeling/property\")\n",
        "mpath = PROP_DIR / \"training_manifest.json\"\n",
        "m = json.loads(mpath.read_text(encoding=\"utf-8\")) if mpath.exists() else {}\n",
        "mx = m.setdefault(\"metrics\", {})\n",
        "mx[\"worst_k_10pct\"] = wk\n",
        "mpath.write_text(canonical_json_dumps(m), encoding=\"utf-8\")\n",
        "\n",
        "print(\"✅ Worst-k salvato:\", wk_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "d2088df6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Golden set: golden_inputs.json golden_predictions_train.csv\n"
          ]
        }
      ],
      "source": [
        "# === Golden parity set (5 record deterministici dal TEST) ===\n",
        "from notebooks.shared.common.utils import canonical_json_dumps\n",
        "import numpy as np, json\n",
        "from pathlib import Path\n",
        "\n",
        "MODEL_DIR = Path(\"outputs/modeling\")\n",
        "PROP_DIR = MODEL_DIR / \"property\"\n",
        "\n",
        "# Assumiamo che df_test e feature_order esistano già\n",
        "rng = np.random.default_rng(SEED)\n",
        "take = min(5, len(df_test))\n",
        "idx = sorted(rng.choice(df_test.index, size=take, replace=False).tolist())\n",
        "\n",
        "golden_inputs = df_test.loc[idx, feature_order].copy()\n",
        "golden_inputs_path = MODEL_DIR / \"golden_inputs.json\"\n",
        "golden_inputs_path.write_text(\n",
        "    canonical_json_dumps(json.loads(golden_inputs.to_json(orient=\"records\"))),\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "# Predizioni con la pipeline servibile già fittata (serving_pipe_v2)\n",
        "y_pred_golden = serving_pipe_v2.predict(df_test.loc[idx, feature_order])\n",
        "golden_train_preds = pd.DataFrame({\n",
        "    \"index\": idx,\n",
        "    VALUATION_K: df_test.loc[idx, VALUATION_K].to_numpy(dtype=np.float64),\n",
        "    \"y_pred\": y_pred_golden.astype(float),\n",
        "})\n",
        "golden_train_preds_path = MODEL_DIR / \"golden_predictions_train.csv\"\n",
        "golden_train_preds.to_csv(golden_train_preds_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "# Aggiorna manifest training con i path golden\n",
        "manifest_path = PROP_DIR / \"training_manifest.json\"\n",
        "m = json.loads(manifest_path.read_text(encoding=\"utf-8\")) if manifest_path.exists() else {}\n",
        "paths = m.setdefault(\"paths\", {})\n",
        "paths.update({\n",
        "    \"golden_inputs\": str(golden_inputs_path.resolve().as_posix()),\n",
        "    \"golden_predictions_train\": str(golden_train_preds_path.resolve().as_posix()),\n",
        "})\n",
        "manifest_path.write_text(canonical_json_dumps(m), encoding=\"utf-8\")\n",
        "\n",
        "print(\"✅ Golden set:\", golden_inputs_path.name, golden_train_preds_path.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "6a09e003",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved drift metrics → outputs/modeling/location_drift_train_vs_test.json  (manifest aggiornato: outputs/modeling/property/training_manifest.json)\n"
          ]
        }
      ],
      "source": [
        "# 11) Post-training drift check (location) — firma: compute_location_drift(df, target_weights, tolerance)\n",
        "from __future__ import annotations\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Import robusto della funzione (firma: df, target_weights, tolerance)\n",
        "try:\n",
        "    from notebooks.shared.n03_train_model.metrics import compute_location_drift  # <- richiede (df, target_weights, tolerance)\n",
        "    _HAS_CLD = True\n",
        "except Exception:\n",
        "    _HAS_CLD = False\n",
        "\n",
        "# 2) Contesto: path & config (già definiti a inizio notebook)\n",
        "manifest_path = PROP_DIR / \"training_manifest.json\"  # es.: notebooks/outputs/modeling/property/training_manifest.json\n",
        "try:\n",
        "    manifest = json.loads(manifest_path.read_text(encoding=\"utf-8\")) if manifest_path.exists() else {}\n",
        "except Exception:\n",
        "    manifest = {}\n",
        "\n",
        "TRAIN_CFG = globals().get(\"TRAIN_CFG\", {}) or {}\n",
        "LOCATION  = globals().get(\"LOCATION\", \"location\")\n",
        "TOL       = float(TRAIN_CFG.get(\"drift_tolerance\", 0.05))  # default 5%\n",
        "\n",
        "# 3) Helpers -----------------------------------------------------------------\n",
        "def _norm_weights(d: dict) -> dict[str, float]:\n",
        "    \"\"\"Normalizza pesi (>=0) per sommare a 1.0; ignora chiavi con pesi negativi/non numerici.\"\"\"\n",
        "    clean = {str(k): float(v) for k, v in d.items() if pd.api.types.is_number(v) and float(v) >= 0.0}\n",
        "    s = float(sum(clean.values()))\n",
        "    if s <= 0:\n",
        "        return {k: 0.0 for k in clean}\n",
        "    return {k: v / s for k, v in clean.items()}\n",
        "\n",
        "def _empirical_weights(df_like: pd.DataFrame, col: str) -> dict[str, float]:\n",
        "    if not isinstance(df_like, pd.DataFrame) or col not in df_like.columns:\n",
        "        return {}\n",
        "    vc = df_like[col].dropna().astype(str).value_counts(normalize=True)\n",
        "    return {k: float(v) for k, v in vc.items()}\n",
        "\n",
        "def _fallback_drift(df_like: pd.DataFrame, target_w: dict[str, float]) -> dict:\n",
        "    \"\"\"Fallback semplice: differenze assolute + ratio + TVD/JSD.\"\"\"\n",
        "    emp = _empirical_weights(df_like, LOCATION)\n",
        "    keys = sorted(set(emp) | set(target_w))\n",
        "    p = np.array([emp.get(k, 0.0) for k in keys], dtype=float)\n",
        "    q = np.array([target_w.get(k, 0.0) for k in keys], dtype=float)\n",
        "    eps = 1e-12\n",
        "    p = np.clip(p, eps, 1.0); q = np.clip(q, eps, 1.0)\n",
        "    p /= p.sum(); q /= q.sum()\n",
        "    m = 0.5 * (p + q)\n",
        "    jsd = float(0.5 * (np.sum(p * (np.log(p) - np.log(m))) + np.sum(q * (np.log(q) - np.log(m)))))\n",
        "    tvd = float(0.5 * np.abs(p - q).sum())\n",
        "    report = {\n",
        "        \"method\": \"fallback_jsd_tvd\",\n",
        "        \"JSD\": jsd,\n",
        "        \"TVD\": tvd,\n",
        "        \"per_location\": {}\n",
        "    }\n",
        "    for k in keys:\n",
        "        emp_k = emp.get(k, 0.0); tgt_k = target_w.get(k, 0.0)\n",
        "        diff = emp_k - tgt_k\n",
        "        report[\"per_location\"][k] = {\n",
        "            \"target_weight\": tgt_k,\n",
        "            \"empirical_weight\": emp_k,\n",
        "            \"difference\": diff,\n",
        "            \"drifted\": bool(abs(diff) > TOL),\n",
        "            \"ratio\": (emp_k / tgt_k) if tgt_k > 0 else float(\"inf\")\n",
        "        }\n",
        "    return report\n",
        "\n",
        "# 4) Scegli scenario: baseline da config OPPURE train vs test ---------------\n",
        "baseline_cfg = (TRAIN_CFG.get(\"expected_profile\", {}) or {}).get(\"location_distribution\", {}) or None\n",
        "\n",
        "try:\n",
        "    if baseline_cfg:\n",
        "        # Scenario A: confronto dataset complessivo vs baseline attesa\n",
        "        if \"df\" not in globals():\n",
        "            raise RuntimeError(\"df non disponibile per il drift vs baseline.\")\n",
        "        target_w = _norm_weights(baseline_cfg)\n",
        "        if _HAS_CLD:\n",
        "            drift_result = compute_location_drift(df, target_w, TOL)\n",
        "        else:\n",
        "            drift_result = _fallback_drift(df, target_w)\n",
        "        out_path = MODEL_DIR / \"location_drift_vs_expected.json\"\n",
        "        out_key  = \"location_drift_vs_expected\"\n",
        "    else:\n",
        "        # Scenario B: train vs test — usa la distribuzione del TEST come target_weights\n",
        "        if \"df_train\" not in globals() or \"df_test\" not in globals():\n",
        "            raise RuntimeError(\"df_train/df_test non disponibili per il drift train vs test.\")\n",
        "        tgt = _empirical_weights(df_test, LOCATION)\n",
        "        target_w = _norm_weights(tgt)\n",
        "        if _HAS_CLD:\n",
        "            drift_result = compute_location_drift(df_train, target_w, TOL)\n",
        "        else:\n",
        "            drift_result = _fallback_drift(df_train, target_w)\n",
        "        out_path = MODEL_DIR / \"location_drift_train_vs_test.json\"\n",
        "        out_key  = \"location_drift_train_vs_test\"\n",
        "\n",
        "except Exception as e:\n",
        "    # Fallback totale (se qualcosa va storto nelle ramificazioni sopra)\n",
        "    try:\n",
        "        logger.info(\"compute_location_drift non disponibile/errore (%s). Uso fallback semplice.\", e)\n",
        "    except Exception:\n",
        "        print(f\"compute_location_drift non disponibile/errore ({e}). Uso fallback semplice.\")\n",
        "    if baseline_cfg and \"df\" in globals():\n",
        "        drift_result = _fallback_drift(df, _norm_weights(baseline_cfg))\n",
        "        out_path = MODEL_DIR / \"location_drift_vs_expected.json\"\n",
        "        out_key  = \"location_drift_vs_expected\"\n",
        "    elif \"df_train\" in globals() and \"df_test\" in globals():\n",
        "        drift_result = _fallback_drift(df_train, _norm_weights(_empirical_weights(df_test, LOCATION)))\n",
        "        out_path = MODEL_DIR / \"location_drift_train_vs_test.json\"\n",
        "        out_key  = \"location_drift_train_vs_test\"\n",
        "    else:\n",
        "        raise\n",
        "\n",
        "# 5) Persistenza output + aggiornamento manifest ----------------------------\n",
        "out_path.write_text(canonical_json_dumps(drift_result), encoding=\"utf-8\")\n",
        "\n",
        "m = dict(manifest.get(\"metrics\", {}))\n",
        "m[out_key] = drift_result\n",
        "manifest[\"metrics\"] = m\n",
        "manifest_path.write_text(canonical_json_dumps(manifest), encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Saved drift metrics → {out_path.as_posix()}  (manifest aggiornato: {manifest_path.as_posix()})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bbc1d70-e1f4-48ce-ac17-2592e450c444",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### ModelReportRunner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "20e93904",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random split → R²(all)=0.8156  MAE=81.57  RMSE=119.84\n",
            "Random split → R²(num)=0.3919  MAE=154.14  RMSE=217.62\n",
            "ΔR² (all - num): +0.4237\n",
            "GSS 5× (group=location) → R²=0.8408±0.0725  MAE=63.82±15.09  RMSE=90.55±16.23\n",
            "\n",
            "Top 10 feature importance (Ordinal+CT):\n",
            "        feature  importance\n",
            "        size_m2    0.515095\n",
            "           zone    0.212770\n",
            "         region    0.092810\n",
            "     year_built    0.053383\n",
            "   energy_class    0.041878\n",
            "      condition    0.020360\n",
            "building_floors    0.010782\n",
            "          floor    0.010665\n",
            "          rooms    0.009456\n",
            "           view    0.008230\n"
          ]
        }
      ],
      "source": [
        "# === Model Report Runner (coerente con il training: OrdinalEncoder + TTR, paths outputs/…) ===\n",
        "from __future__ import annotations\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "\n",
        "# --- helper RMSE retro-compatibile (sklearn vecchie non hanno 'squared=')\n",
        "def _rmse(y_true, y_pred):\n",
        "    try:\n",
        "        return mean_squared_error(y_true, y_pred, squared=False)\n",
        "    except TypeError:\n",
        "        return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "# ── 0) Carica manifest v2 per feature_config (paths coerenti: outputs/…)\n",
        "PROP_DIR = Path(\"outputs/modeling/property\")\n",
        "MF_PATH = PROP_DIR / \"training_manifest.json\"\n",
        "\n",
        "cat_cols, num_cols = [], []\n",
        "if MF_PATH.exists():\n",
        "    mf = json.loads(MF_PATH.read_text(encoding=\"utf-8\"))\n",
        "    fc = (mf.get(\"feature_config\") or {})\n",
        "    cat_cols = list(fc.get(\"categorical\") or [])\n",
        "    num_cols = list(fc.get(\"numeric\") or [])\n",
        "else:\n",
        "    # fallback se non c'è manifest\n",
        "    cat_cols = [\"location\", \"region\", \"zone\", \"energy_class\", \"urban_type\", \"orientation\", \"view\", \"condition\", \"heating\"]\n",
        "    num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c]) and c != \"valuation_k\"]\n",
        "\n",
        "# Sanity: tieni solo colonne esistenti\n",
        "cat_cols = [c for c in cat_cols if c in df.columns]\n",
        "num_cols = [c for c in num_cols if c in df.columns]\n",
        "ALL = cat_cols + num_cols\n",
        "\n",
        "assert \"valuation_k\" in df.columns, \"Manca il target valuation_k\"\n",
        "assert len(ALL) > 0, \"Nessuna feature trovata (cat+num)\"\n",
        "\n",
        "# ── 1) Pipeline coerente con il training (Ordinal + imputazioni) + TTR\n",
        "pre_all = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", Pipeline([\n",
        "            (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "            (\"enc\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
        "        ]), cat_cols) if cat_cols else (\"cat\", \"drop\", []),\n",
        "        (\"num\", Pipeline([\n",
        "            (\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "        ]), num_cols) if num_cols else (\"num\", \"drop\", []),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    verbose_feature_names_out=False,\n",
        ")\n",
        "\n",
        "rf_all = RandomForestRegressor(\n",
        "    n_estimators=300, random_state=42, n_jobs=-1, max_depth=None, min_samples_leaf=2\n",
        ")\n",
        "\n",
        "pipe_all = Pipeline([\n",
        "    (\"prep\", pre_all),\n",
        "    (\"ttr\", TransformedTargetRegressor(\n",
        "        regressor=rf_all,\n",
        "        func=np.log1p, inverse_func=np.expm1, check_inverse=False\n",
        "    )),\n",
        "])\n",
        "\n",
        "# ── 2) Valutazione split semplice (random) con tutte le feature\n",
        "X_all = df[ALL].copy()\n",
        "y_nat = df[\"valuation_k\"].astype(float).to_numpy()\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X_all, y_nat, test_size=0.2, random_state=42)\n",
        "pipe_all.fit(X_tr, y_tr)\n",
        "y_hat = pipe_all.predict(X_te)\n",
        "\n",
        "r2_all  = r2_score(y_te, y_hat)\n",
        "mae_all = mean_absolute_error(y_te, y_hat)\n",
        "rmse_all = _rmse(y_te, y_hat)\n",
        "print(f\"Random split → R²(all)={r2_all:.4f}  MAE={mae_all:.2f}  RMSE={rmse_all:.2f}\")\n",
        "\n",
        "# ── 3) Solo numeriche (stesso schema di imputazione) + TTR\n",
        "if num_cols:\n",
        "    pre_num = ColumnTransformer(\n",
        "        [(\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))]), num_cols)],\n",
        "        remainder=\"drop\",\n",
        "        verbose_feature_names_out=False,\n",
        "    )\n",
        "    rf_num = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1, min_samples_leaf=2)\n",
        "    pipe_num = Pipeline([\n",
        "        (\"prep\", pre_num),\n",
        "        (\"ttr\", TransformedTargetRegressor(\n",
        "            regressor=rf_num, func=np.log1p, inverse_func=np.expm1, check_inverse=False\n",
        "        )),\n",
        "    ])\n",
        "    Xn = df[num_cols].copy()\n",
        "    Xn_tr, Xn_te, yn_tr, yn_te = train_test_split(Xn, y_nat, test_size=0.2, random_state=42)\n",
        "    pipe_num.fit(Xn_tr, yn_tr)\n",
        "    y_num = pipe_num.predict(Xn_te)\n",
        "\n",
        "    r2_num  = r2_score(yn_te, y_num)\n",
        "    mae_num = mean_absolute_error(yn_te, y_num)\n",
        "    rmse_num = _rmse(yn_te, y_num)\n",
        "    print(f\"Random split → R²(num)={r2_num:.4f}  MAE={mae_num:.2f}  RMSE={rmse_num:.2f}\")\n",
        "    print(f\"ΔR² (all - num): {r2_all - r2_num:+.4f}\")\n",
        "\n",
        "# ── 4) Stima più robusta: GroupShuffleSplit (group=location se presente)\n",
        "if \"location\" in df.columns:\n",
        "    gss = GroupShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
        "    r2s, maes, rmses = [], [], []\n",
        "    groups = df[\"location\"].astype(str).to_numpy()\n",
        "    for tr_idx, te_idx in gss.split(df[ALL], y_nat, groups=groups):\n",
        "        pipe_all.fit(df.iloc[tr_idx][ALL], y_nat[tr_idx])\n",
        "        y_g = pipe_all.predict(df.iloc[te_idx][ALL])\n",
        "        r2s.append(r2_score(y_nat[te_idx], y_g))\n",
        "        maes.append(mean_absolute_error(y_nat[te_idx], y_g))\n",
        "        rmses.append(_rmse(y_nat[te_idx], y_g))\n",
        "    print(f\"GSS 5× (group=location) → R²={np.mean(r2s):.4f}±{np.std(r2s):.4f}  \"\n",
        "          f\"MAE={np.mean(maes):.2f}±{np.std(maes):.2f}  RMSE={np.mean(rmses):.2f}±{np.std(rmses):.2f}\")\n",
        "\n",
        "# ── 5) Feature importance (dal RF dentro TTR) + nomi coerenti CT\n",
        "try:\n",
        "    try:\n",
        "        feat_names = list(pipe_all.named_steps[\"prep\"].get_feature_names_out())\n",
        "    except Exception:\n",
        "        feat_names = [*cat_cols, *num_cols]\n",
        "\n",
        "    rf_fitted = pipe_all.named_steps[\"ttr\"].regressor_\n",
        "    importances = getattr(rf_fitted, \"feature_importances_\", None)\n",
        "    if importances is None:\n",
        "        raise RuntimeError(\"feature_importances_ non disponibile sul regressore.\")\n",
        "\n",
        "    imp = np.asarray(importances, dtype=float)\n",
        "    if len(feat_names) != len(imp):\n",
        "        feat_names = [f\"f{i}\" for i in range(len(imp))]\n",
        "\n",
        "    fi = (pd.DataFrame({\"feature\": feat_names, \"importance\": imp})\n",
        "            .sort_values(\"importance\", ascending=False)\n",
        "            .reset_index(drop=True))\n",
        "\n",
        "    print(\"\\nTop 10 feature importance (Ordinal+CT):\")\n",
        "    print(fi.head(10).to_string(index=False))\n",
        "except Exception as e:\n",
        "    print(\"Feature importance non disponibile:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "814c338c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ training_manifest firmato: a1b6d3079eaadc5d99aab01d8ad6f379c4f4213062bca885bcb251bee488b918\n"
          ]
        }
      ],
      "source": [
        "# === Firma manifest (sha256) + created_utc ===\n",
        "from notebooks.shared.common.utils import canonical_json_dumps, sha256_hex\n",
        "from datetime import datetime, timezone\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "PROP_DIR = Path(\"outputs/modeling/property\")\n",
        "manifest_path = PROP_DIR / \"training_manifest.json\"\n",
        "\n",
        "m = json.loads(manifest_path.read_text(encoding=\"utf-8\")) if manifest_path.exists() else {}\n",
        "manifest_canon = canonical_json_dumps(m)\n",
        "m[\"manifest_sha256\"] = sha256_hex(manifest_canon)\n",
        "m[\"created_utc\"] = datetime.now(timezone.utc).replace(microsecond=0).isoformat().replace(\"+00:00\",\"Z\")\n",
        "\n",
        "manifest_path.write_text(canonical_json_dumps(m), encoding=\"utf-8\")\n",
        "print(\"✅ training_manifest firmato:\", m[\"manifest_sha256\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "af220429",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OHE categories — region: ['urban']\n",
            "OHE categories — zone  : ['bari', 'bologna', 'cagliari', 'catania', 'florence', 'genoa', 'milan', 'naples', 'padua', 'palermo']\n",
            "handle_unknown: ignore\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Users\\Utente1\\miniconda3\\envs\\ai-oracle\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['city_zone_prior' 'region_index_prior']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# --- Introspezione categorie OHE (safe) ---\n",
        "# 1) prende il prep dal pipeline\n",
        "prep = pipe_A.named_steps[\"prep\"]\n",
        "\n",
        "def _ensure_prep_fitted(prep):\n",
        "    \"\"\"Se prep non è fit, fa un fit temporaneo SOLO sul preproc (senza toccare il modello).\"\"\"\n",
        "    if not hasattr(prep, \"transformers_\"):\n",
        "        # se esiste uno step di derivazione, applicalo prima del fit del prep\n",
        "        X_for_fit = X_train\n",
        "        if \"derive\" in pipe_A.named_steps:\n",
        "            X_for_fit = pipe_A.named_steps[\"derive\"].transform(X_train)\n",
        "        # fit solo il preproc (NO model)\n",
        "        prep.fit(X_for_fit, y_train)\n",
        "    return prep\n",
        "\n",
        "prep = _ensure_prep_fitted(prep)\n",
        "\n",
        "# 2) recupera il OneHotEncoder dentro al ramo 'cat'\n",
        "cat_branch = prep.named_transformers_.get(\"cat\")\n",
        "if hasattr(cat_branch, \"named_steps\"):\n",
        "    ohe = cat_branch.named_steps.get(\"encode\")\n",
        "else:\n",
        "    ohe = None\n",
        "\n",
        "if ohe is None or not hasattr(ohe, \"categories_\"):\n",
        "    raise RuntimeError(\"Il ramo categorico non contiene uno step 'encode' con OneHotEncoder già fit.\")\n",
        "\n",
        "cats_map = {col: list(cats) for col, cats in zip(cat_cols, ohe.categories_)}\n",
        "\n",
        "print(\"OHE categories — region:\", cats_map.get(\"region\", [])[:10])\n",
        "print(\"OHE categories — zone  :\", cats_map.get(\"zone\", [])[:10])\n",
        "\n",
        "# extra: verifica handle_unknown\n",
        "try:\n",
        "    print(\"handle_unknown:\", getattr(ohe, \"handle_unknown\", None))\n",
        "except Exception:\n",
        "    pass"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ai-oracle",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
