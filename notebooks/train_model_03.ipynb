{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8cc119a1-28bd-47af-8a5d-38204c005bdf",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Imports & Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba165936-c2d9-4dd5-b388-62912b63f069",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 00) Import & setup â€” esecuzione da dentro `notebooks/`\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Siamo giÃ  in notebooks/, quindi `shared/` Ã¨ un pacchetto sibling\n",
        "NB_ROOT = Path.cwd()                 # .../notebooks\n",
        "PROJ_ROOT = NB_ROOT.parent           # project root\n",
        "\n",
        "if str(NB_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(NB_ROOT))\n",
        "if str(PROJ_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJ_ROOT))\n",
        "\n",
        "# â”€â”€ shared imports (coerenti con i tuoi notebook)\n",
        "from notebooks.shared.common.utils import (\n",
        "    NumpyJSONEncoder,\n",
        "    optimize_dtypes,\n",
        "    log_basic_diagnostics,\n",
        "    set_global_seed,\n",
        ")\n",
        "from notebooks.shared.common.config import load_config, configure_logger\n",
        "from notebooks.shared.common.constants import (\n",
        "    VALUATION_K,\n",
        "    LAST_VERIFIED_TS, PREDICTION_TS, LAG_HOURS,\n",
        "    CONDITION_SCORE, RISK_SCORE,\n",
        ")\n",
        "\n",
        "# sklearn (alcuni import usati in celle successive)\n",
        "from sklearn.ensemble import RandomForestRegressor  # noqa: F401\n",
        "\n",
        "# â”€â”€ Logger\n",
        "LOG_LEVEL = os.getenv(\"NB_LOG_LEVEL\", \"INFO\")\n",
        "logger = configure_logger(name=\"model_trainer\", level=LOG_LEVEL)\n",
        "\n",
        "# â”€â”€ Config (opzionale): se non esiste, lavoriamo in fallback senza YAML\n",
        "CFG_PATH = NB_ROOT / \"dataset_config.yaml\"\n",
        "if CFG_PATH.exists():\n",
        "    CONFIG = load_config(str(CFG_PATH))\n",
        "    logger.info(\"Config YAML caricato: %s\", CFG_PATH.as_posix())\n",
        "else:\n",
        "    CONFIG = {}\n",
        "    logger.warning(\"dataset_config.yaml NON trovato: uso fallback di default.\")\n",
        "\n",
        "TRAIN_CFG = CONFIG.get(\"training\", {}) or {}\n",
        "\n",
        "# â”€â”€ Seed globale\n",
        "SEED = int(TRAIN_CFG.get(\"seed\", CONFIG.get(\"seed\", 42)))\n",
        "set_global_seed(SEED)\n",
        "\n",
        "# â”€â”€ Cartelle output (da usare SEMPRE relative a `notebooks/`)\n",
        "BASE_OUT = NB_ROOT / \"outputs\"                  # <- ðŸ‘ˆ corretto: siamo giÃ  in notebooks/\n",
        "MODEL_DIR = BASE_OUT / \"modeling\"\n",
        "FIG_DIR   = MODEL_DIR / \"figures\"\n",
        "ART_DIR   = MODEL_DIR / \"artifacts\"\n",
        "PROP_DIR  = MODEL_DIR / \"property\"              # cartella â€œservitaâ€ dal backend/registry\n",
        "\n",
        "for d in (BASE_OUT, MODEL_DIR, FIG_DIR, ART_DIR, PROP_DIR):\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# â”€â”€ Dataset path canonico (override da YAML se presente)\n",
        "DATASET_PATH = Path(TRAIN_CFG.get(\"dataset_path\", BASE_OUT / \"dataset_generated.csv\"))\n",
        "\n",
        "# QoL\n",
        "pd.set_option(\"display.max_columns\", 200)\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "logger.info(\"Setup OK | seed=%s | outputs_dir=%s\", SEED, BASE_OUT.as_posix())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c0f3f96-b7be-48ff-a9b2-2c9b9569cb63",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38892f35-5a7d-488c-83d2-a99bca1e6fee",
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# 01) Carica dataset dal manifest di nb01 (robusto) + ottimizza + valida\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# sanity (preferisci notebooks.shared, fallback a shared)\n",
        "try:\n",
        "    from notebooks.shared.common.sanity_checks import validate_dataset\n",
        "except Exception:\n",
        "    from notebooks.shared.common.sanity_checks import validate_dataset  # type: ignore\n",
        "\n",
        "# --- helper: risolvi path relativo verso posizioni note ---\n",
        "def _resolve_path(p: str | Path) -> Path | None:\n",
        "    cand = Path(p)\n",
        "    if cand.exists():\n",
        "        return cand\n",
        "    # se Ã¨ relativo, prova sotto base output e progetto\n",
        "    for base in [BASE_OUT, NB_ROOT, PROJ_ROOT]:\n",
        "        q = (base / str(p)).resolve()\n",
        "        if q.exists():\n",
        "            return q\n",
        "    return None\n",
        "\n",
        "# 1) Trova ultimo manifest di nb01\n",
        "snap_dir = BASE_OUT / \"snapshots\"            # notebooks/outputs/snapshots\n",
        "snap_dir.mkdir(parents=True, exist_ok=True)\n",
        "manifests = sorted(snap_dir.glob(\"manifest_*.json\"))\n",
        "manifest01 = None\n",
        "if manifests:\n",
        "    try:\n",
        "        manifest01 = json.loads(manifests[-1].read_text(encoding=\"utf-8\"))\n",
        "        logger.info(\"Manifest nb01 trovato: %s\", manifests[-1].as_posix())\n",
        "    except Exception as e:\n",
        "        logger.warning(\"Impossibile leggere il manifest piÃ¹ recente: %s\", e)\n",
        "\n",
        "# 2) Determina data_path dal manifest (supporta varie chiavi)\n",
        "data_path: Path | None = None\n",
        "if isinstance(manifest01, dict):\n",
        "    paths = (manifest01.get(\"paths\") or {})  # type: ignore\n",
        "    for k in (\"dataset_path\", \"dataset\", \"output_path\"):\n",
        "        p = paths.get(k)\n",
        "        if p:\n",
        "            rp = _resolve_path(p)\n",
        "            if rp:\n",
        "                data_path = rp\n",
        "                break\n",
        "\n",
        "# 3) Fallback: usa DATASET_PATH (da Cella 01) o cerca in BASE_OUT\n",
        "if data_path is None or not data_path.exists():\n",
        "    candidates = [\n",
        "        Path(DATASET_PATH) if isinstance(DATASET_PATH, (str, Path)) else None,\n",
        "        BASE_OUT / \"dataset_generated.parquet\",\n",
        "        BASE_OUT / \"dataset_generated.csv\",\n",
        "    ]\n",
        "    # estendi con eventuali file simili\n",
        "    candidates += sorted(BASE_OUT.glob(\"dataset_*.parquet\"))\n",
        "    candidates += sorted(BASE_OUT.glob(\"dataset_*.csv\"))\n",
        "    data_path = next((c for c in candidates if c and c.exists()), None)\n",
        "\n",
        "if not data_path or not data_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        \"Dataset non trovato. Verifica manifest nb01 in notebooks/outputs/snapshots \"\n",
        "        \"oppure che esista notebooks/outputs/dataset_generated.(csv|parquet).\"\n",
        "    )\n",
        "\n",
        "logger.info(\"ðŸ“„ Caricamento dataset da: %s\", data_path.as_posix())\n",
        "\n",
        "# 4) Caricamento parquet/csv\n",
        "if data_path.suffix.lower() in {\".parquet\", \".pq\"}:\n",
        "    df = pd.read_parquet(data_path)\n",
        "else:\n",
        "    df = pd.read_csv(data_path)\n",
        "\n",
        "# 5) Ottimizzazione dtypes (log risparmio)\n",
        "mem_before = df.memory_usage(deep=True).sum() / 1024**2\n",
        "df = optimize_dtypes(df)\n",
        "mem_after = df.memory_usage(deep=True).sum() / 1024**2\n",
        "logger.info(\n",
        "    \"âœ… Dtypes optimized: %.2f MB â†’ %.2f MB (âˆ’%.2f MB, %.1f%%)\",\n",
        "    mem_before, mem_after, mem_before - mem_after,\n",
        "    0.0 if mem_before == 0 else (mem_before - mem_after) / mem_before * 100.0\n",
        ")\n",
        "\n",
        "# 6) Diagnostica rapida\n",
        "log_basic_diagnostics(df, logger)\n",
        "\n",
        "# 7) Validazione schema (asset_type da config, come nb01)\n",
        "asset_type = str(CONFIG.get(\"generation\", {}).get(\"asset_type\", \"property\"))\n",
        "try:\n",
        "    val_report = validate_dataset(df, asset_type=asset_type, raise_on_failure=True)\n",
        "    logger.info(\"âœ… Schema validation passed\")\n",
        "except Exception as e:\n",
        "    logger.warning(\"Schema validation warning: %s\", e)\n",
        "    val_report = {\"overall_passed\": False, \"error\": str(e)}\n",
        "\n",
        "# 8) Persistenza report vicino ai modeling outputs\n",
        "(MODEL_DIR / \"validation_nb03.json\").write_text(\n",
        "    json.dumps(val_report, cls=NumpyJSONEncoder, indent=2, ensure_ascii=False),\n",
        "    encoding=\"utf-8\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f92a3f27",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 02) PULIZIA LEAKAGE IMMEDIATA (robusta)\n",
        "from __future__ import annotations\n",
        "\n",
        "import re\n",
        "\n",
        "# --- costanti: preferisci notebooks.shared, fallback a shared, poi fallback a stringhe ---\n",
        "try:\n",
        "    from notebooks.shared.common.constants import VALUATION_K as _VAL_K\n",
        "except Exception:\n",
        "    try:\n",
        "        from notebooks.shared.common.constants import VALUATION_K as _VAL_K  # type: ignore\n",
        "    except Exception:\n",
        "        _VAL_K = \"valuation_k\"\n",
        "\n",
        "try:\n",
        "    from notebooks.shared.common.constants import PRICE_PER_SQM as _PPS, PRICE_PER_SQM_CAPPED_VIOLATED as _PPSV\n",
        "except Exception:\n",
        "    try:\n",
        "        from notebooks.shared.common.constants import PRICE_PER_SQM as _PPS, PRICE_PER_SQM_CAPPED_VIOLATED as _PPSV  # type: ignore\n",
        "    except Exception:\n",
        "        _PPS = \"price_per_sqm\"\n",
        "        _PPSV = \"price_per_sqm_capped_violated\"\n",
        "\n",
        "VALUTION_K = _VAL_K  # compat alias usato piÃ¹ sotto\n",
        "PRICE_PER_SQM = _PPS\n",
        "PRICE_PER_SQM_CAPPED_VIOLATED = _PPSV\n",
        "\n",
        "# --- lista centrale di feature leaky (se presente) ---\n",
        "try:\n",
        "    from notebooks.shared.n03_train_model.preprocessing import ML_LEAKY_FEATURES as _ML_LEAKY\n",
        "except Exception:\n",
        "    try:\n",
        "        from notebooks.shared.n03_train_model.preprocessing import ML_LEAKY_FEATURES as _ML_LEAKY  # type: ignore\n",
        "    except Exception:\n",
        "        _ML_LEAKY = {\n",
        "            \"price_per_sqm\", \"price_per_sqm_vs_region_avg\", \"price_per_sqm_capped\",\n",
        "            \"valuation_k_log\", \"_viz_price_per_sqm\", \"valuation_k_decile\",\n",
        "            \"valuation_rank\", \"is_top_valuation\"\n",
        "        }\n",
        "\n",
        "# --- 1) Rimozione esplicita (case-insensitive) ---\n",
        "explicit_leaky = {\n",
        "    PRICE_PER_SQM,\n",
        "    \"price_per_sqm\",\n",
        "    \"price_per_sqm_vs_region_avg\",\n",
        "    \"price_per_sqm_capped\",\n",
        "    \"valuation_k_log\",\n",
        "    PRICE_PER_SQM_CAPPED_VIOLATED,\n",
        "    \"strongly_incoherent\",\n",
        "    \"valuation_k_decile\",\n",
        "    \"valuation_rank\",\n",
        "    \"is_top_valuation\",\n",
        "}\n",
        "explicit_leaky |= set(map(str, _ML_LEAKY))\n",
        "\n",
        "# mappa lowercase -> originale\n",
        "lower_map = {c.lower(): c for c in df.columns}\n",
        "present_explicit = [lower_map[n.lower()] for n in explicit_leaky if n and n.lower() in lower_map]\n",
        "\n",
        "# --- 2) Rimozione pattern-based (regex, case-insensitive) ---\n",
        "regex_patterns = [\n",
        "    r\"price_per_sqm\",       # qualunque col contenga price_per_sqm\n",
        "    r\"^valuation_k_.+$\",    # derivate del target\n",
        "]\n",
        "present_regex = []\n",
        "for col in df.columns:\n",
        "    if col == VALUTION_K:\n",
        "        continue\n",
        "    if any(re.search(pat, col, flags=re.IGNORECASE) for pat in regex_patterns):\n",
        "        present_regex.append(col)\n",
        "\n",
        "# --- 3) Applica rimozione ---\n",
        "to_drop = sorted(set(present_explicit) | set(present_regex))\n",
        "if to_drop:\n",
        "    logger.warning(\"ðŸ”´ RIMOZIONE FEATURES LEAKY: %s\", to_drop)\n",
        "    df.drop(columns=to_drop, inplace=True, errors=\"ignore\")\n",
        "    logger.info(\"âœ… Dataset pulito: %d colonne rimanenti\", df.shape[1])\n",
        "else:\n",
        "    logger.info(\"âœ… Nessuna feature leaky trovata nel dataset\")\n",
        "\n",
        "# --- 4) Verifiche finali ---\n",
        "assert not any(\"price_per_sqm\" in c.lower() for c in df.columns), \"ERRORE: colonne 'price_per_sqm*' ancora presenti!\"\n",
        "assert not any(c.lower().startswith(\"valuation_k_\") for c in df.columns if c != VALUTION_K), \"ERRORE: derivate 'valuation_k_*' ancora presenti!\"\n",
        "\n",
        "# Debug essenziale\n",
        "logger.debug(\"Colonne rimanenti: %s\", list(df.columns))\n",
        "print(f\"Shape dopo pulizia: {df.shape}\")\n",
        "print(f\"Colonne numeriche: {df.select_dtypes(include='number').columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c08c1d08-0b0e-4ed0-bbd9-94c27b0909e3",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Preparation (derivations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e773326-72f1-480d-8473-b685af965df1",
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# 04) FEATURE DERIVATE\n",
        "from __future__ import annotations\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    from notebooks.shared.common.constants import PRICE_PER_SQM  # type: ignore\n",
        "except Exception:\n",
        "    PRICE_PER_SQM = \"price_per_sqm\"\n",
        "\n",
        "# 1) LAG_HOURS se mancante (da timestamp UTC)\n",
        "if (LAG_HOURS not in df.columns) and ({LAST_VERIFIED_TS, PREDICTION_TS} <= set(df.columns)):\n",
        "    # parse tollerante (accetta naive â†’ le rende UTC)\n",
        "    df[LAST_VERIFIED_TS] = pd.to_datetime(df[LAST_VERIFIED_TS], utc=True, errors=\"coerce\")\n",
        "    df[PREDICTION_TS]   = pd.to_datetime(df[PREDICTION_TS],   utc=True, errors=\"coerce\")\n",
        "\n",
        "    lag = (df[PREDICTION_TS] - df[LAST_VERIFIED_TS]).dt.total_seconds().div(3600)\n",
        "    # valori negativi o assurdi â†’ NaN; poi cast a float32\n",
        "    lag = lag.where(lag >= 0, other=pd.NA)\n",
        "    df[LAG_HOURS] = lag.astype(\"Float32\")\n",
        "    logger.info(\"Creato %s da %s & %s\", LAG_HOURS, LAST_VERIFIED_TS, PREDICTION_TS)\n",
        "\n",
        "elif LAG_HOURS in df.columns:\n",
        "    df[LAG_HOURS] = pd.to_numeric(df[LAG_HOURS], errors=\"coerce\").astype(\"Float32\")\n",
        "else:\n",
        "    logger.warning(\"Impossibile derivare %s: mancano %s o %s\", LAG_HOURS, LAST_VERIFIED_TS, PREDICTION_TS)\n",
        "\n",
        "# 2) condition_minus_risk (utile e non-leaky)\n",
        "if (CONDITION_SCORE in df.columns) and (RISK_SCORE in df.columns):\n",
        "    df[CONDITION_SCORE] = pd.to_numeric(df[CONDITION_SCORE], errors=\"coerce\").astype(\"Float32\")\n",
        "    df[RISK_SCORE]      = pd.to_numeric(df[RISK_SCORE],      errors=\"coerce\").astype(\"Float32\")\n",
        "    df[\"condition_minus_risk\"] = (df[CONDITION_SCORE] - df[RISK_SCORE]).astype(\"Float32\")\n",
        "    logger.info(\"Creato feature derivata: condition_minus_risk\")\n",
        "else:\n",
        "    logger.debug(\"condition_minus_risk non creato: mancano %s o %s\", CONDITION_SCORE, RISK_SCORE)\n",
        "\n",
        "if (\"listing_month\" not in df.columns) and (PREDICTION_TS in df.columns):\n",
        "    if pd.api.types.is_datetime64_any_dtype(df[PREDICTION_TS]) or pd.api.types.is_object_dtype(df[PREDICTION_TS]):\n",
        "        try:\n",
        "            ts = pd.to_datetime(df[PREDICTION_TS], utc=True, errors=\"coerce\")\n",
        "            df[\"listing_month\"] = ts.dt.month.astype(\"Int16\")\n",
        "            logger.info(\"Creato listing_month da %s\", PREDICTION_TS)\n",
        "        except Exception:\n",
        "            logger.debug(\"listing_month non creato (parse fallita)\")\n",
        "\n",
        "# 4) Target: check + coercizione numerica\n",
        "if VALUATION_K not in df.columns:\n",
        "    raise ValueError(f\"{VALUATION_K} mancante: impossibile allenare.\")\n",
        "df[VALUATION_K] = pd.to_numeric(df[VALUATION_K], errors=\"coerce\").astype(\"Float32\")\n",
        "\n",
        "# 5) Verifica finale assenza leakage\n",
        "assert not any(\"price_per_sqm\" in c.lower() for c in df.columns), \"LEAKAGE: colonne 'price_per_sqm*' presenti!\"\n",
        "assert not any(c.lower().startswith(\"valuation_k_\") for c in df.columns if c != VALUATION_K), \\\n",
        "       \"LEAKAGE: derivate 'valuation_k_*' ancora presenti!\"\n",
        "\n",
        "# 6) Snapshot\n",
        "print(\"=\" * 60)\n",
        "print(\"DATASET PULITO - PRIME 3 RIGHE\")\n",
        "print(\"=\" * 60)\n",
        "display(df.head(3))\n",
        "print(f\"\\nShape: {df.shape}\")\n",
        "print(f\"Target (valuation_k) range: [{df[VALUATION_K].min():.2f}, {df[VALUATION_K].max():.2f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fca06807",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 05) ANALISI CORRELAZIONI CON IL TARGET (no-leakage, robusta)\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "SUSPICIOUS_THR = 0.95\n",
        "corr_json_path = ART_DIR / \"target_correlations.json\"\n",
        "corr_csv_path  = ART_DIR / \"target_correlations.csv\"\n",
        "\n",
        "# 0) Safety: il target deve essere numerico (coercizzato in cella 04)\n",
        "if VALUATION_K not in df.columns:\n",
        "    logger.error(\"Target %s non trovato nel dataset\", VALUATION_K)\n",
        "    corr_json_path.write_text(json.dumps({\"error\": \"target missing\"}), encoding=\"utf-8\")\n",
        "else:\n",
        "    # 1) Colonne numeriche (post-pulizia) + sanity\n",
        "    numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
        "    if VALUATION_K not in numeric_cols:\n",
        "        logger.warning(\"Il target non risulta numerico: provo a forzare il cast.\")\n",
        "        df[VALUATION_K] = pd.to_numeric(df[VALUATION_K], errors=\"coerce\")\n",
        "        if not pd.api.types.is_numeric_dtype(df[VALUATION_K]):\n",
        "            raise TypeError(f\"{VALUATION_K} non numerico; impossibile calcolare correlazioni.\")\n",
        "\n",
        "    if len(numeric_cols) < 2:\n",
        "        logger.warning(\"Poche colonne numeriche per calcolare correlazioni.\")\n",
        "        corr_json_path.write_text(json.dumps({\"error\": \"not enough numeric columns\"}), encoding=\"utf-8\")\n",
        "    else:\n",
        "        # 2) Pearson\n",
        "        corr_mat = df[numeric_cols].corr(method=\"pearson\")\n",
        "        if VALUATION_K not in corr_mat.columns:\n",
        "            raise RuntimeError(\"Correlazione Pearson non calcolabile sul target (tutti NaN?).\")\n",
        "\n",
        "        correlations = corr_mat[VALUATION_K].drop(labels=[VALUATION_K], errors=\"ignore\").sort_values(ascending=False)\n",
        "\n",
        "        print(\"=\" * 60)\n",
        "        print(\"TOP 15 CORRELAZIONI POSITIVE (Pearson) CON IL TARGET\")\n",
        "        print(\"=\" * 60)\n",
        "        print(correlations.head(15))\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"TOP 15 CORRELAZIONI NEGATIVE (Pearson) CON IL TARGET\")\n",
        "        print(\"=\" * 60)\n",
        "        print(correlations.tail(15))\n",
        "\n",
        "        suspicious = correlations[correlations.abs() > SUSPICIOUS_THR]\n",
        "        if not suspicious.empty:\n",
        "            print(\"\\nðŸ”´ ATTENZIONE: Correlazioni sospette |r| >\", SUSPICIOUS_THR)\n",
        "            for feat, corr in suspicious.items():\n",
        "                print(f\"  - {feat}: {corr:.4f}\")\n",
        "            logger.warning(\"Possibile leakage o duplicati semantici: %s\", list(suspicious.index))\n",
        "        else:\n",
        "            print(\"\\nâœ… Nessuna correlazione sospetta (>|r| >\", SUSPICIOUS_THR, \")\")\n",
        "\n",
        "        # 3) Report strutturato + CSV\n",
        "        corr_df = pd.DataFrame(\n",
        "            {\"feature\": correlations.index, \"correlation_pearson\": correlations.values}\n",
        "        )\n",
        "        payload = {\n",
        "            \"meta\": {\n",
        "                \"method\": \"pearson\",\n",
        "                \"n_numeric_features\": int(len(numeric_cols) - 1),\n",
        "                \"target\": VALUATION_K,\n",
        "                \"suspicious_threshold\": SUSPICIOUS_THR,\n",
        "            },\n",
        "            \"correlations\": corr_df.to_dict(\"records\"),\n",
        "            \"suspicious\": suspicious.to_dict() if not suspicious.empty else {},\n",
        "        }\n",
        "        corr_json_path.write_text(\n",
        "            json.dumps(payload, indent=2, ensure_ascii=False, cls=NumpyJSONEncoder),\n",
        "            encoding=\"utf-8\",\n",
        "        )\n",
        "        corr_df.to_csv(corr_csv_path, index=False)\n",
        "        logger.info(\"Correlations saved: %s (JSON) | %s (CSV)\", corr_json_path, corr_csv_path)\n",
        "\n",
        "        # 4) Spearman (best-effort, robusto a monotonia non lineare)\n",
        "        try:\n",
        "            spearman = df[numeric_cols].corr(method=\"spearman\")[VALUATION_K].drop(labels=[VALUATION_K], errors=\"ignore\")\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(\"TOP 10 CORRELAZIONI SPEARMAN (assolute) CON IL TARGET\")\n",
        "            print(\"=\" * 60)\n",
        "            print(spearman.reindex(spearman.abs().sort_values(ascending=False).index).head(10))\n",
        "        except Exception as e:\n",
        "            logger.debug(\"Spearman correlation failed: %s\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69b68095-70f4-4b5d-ad0d-7b47ba5f9b7a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 06) SPLIT TRAIN/VALID/TEST (strat. su decili) + blocco duplicati per gruppo (default: ASSET_ID)\n",
        "from __future__ import annotations\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# costanti (PRICE_PER_SQM puÃ² non essere importato in questo notebook)\n",
        "try:\n",
        "    from notebooks.shared.common.constants import VALUATION_K, ASSET_ID, PRICE_PER_SQM  # type: ignore\n",
        "except Exception:\n",
        "    from notebooks.shared.common.constants import VALUATION_K, ASSET_ID  # type: ignore\n",
        "    PRICE_PER_SQM = \"price_per_sqm\"\n",
        "\n",
        "TARGET = VALUATION_K\n",
        "\n",
        "# --- Verifica preliminare\n",
        "print(f\"Dataset shape prima dello split: {df.shape}\")\n",
        "print(f\"Colonne totali: {len(df.columns)}\")\n",
        "\n",
        "# --- 0) Parametri da config (con fallback)\n",
        "if \"TRAIN_CFG\" not in globals() or not isinstance(TRAIN_CFG, dict):\n",
        "    TRAIN_CFG = {}\n",
        "TEST_SIZE = float(TRAIN_CFG.get(\"test_size\", 0.15))\n",
        "VAL_SIZE  = float(TRAIN_CFG.get(\"val_size\",  0.15))\n",
        "N_DECILES = int(TRAIN_CFG.get(\"n_deciles\",   10))\n",
        "GROUP_COL = str(TRAIN_CFG.get(\"group_col\", ASSET_ID))  # puoi mettere 'location' in config se vuoi\n",
        "\n",
        "if not (0.01 <= TEST_SIZE <= 0.9) or not (0.01 <= VAL_SIZE <= 0.9):\n",
        "    logger.warning(\"test_size/val_size fuori range â†’ fallback 0.15/0.15\")\n",
        "    TEST_SIZE, VAL_SIZE = 0.15, 0.15\n",
        "\n",
        "# --- 1) Pulizia target per lo split\n",
        "mask_y = pd.to_numeric(df[TARGET], errors=\"coerce\").notna()\n",
        "if not mask_y.all():\n",
        "    logger.warning(\"Righe senza target rimosse dallo split: %d\", (~mask_y).sum())\n",
        "df_clean = df.loc[mask_y].copy()\n",
        "\n",
        "# --- helper: stratificazione per decili â€œrobustaâ€\n",
        "def _strat_bins(y: pd.Series, q: int = 10) -> pd.Series:\n",
        "    \"\"\"Decili robusti sul target (usa rank per duplicati). Fallback a singola classe.\"\"\"\n",
        "    y_num = pd.to_numeric(y, errors=\"coerce\")\n",
        "    ranks = y_num.rank(method=\"first\")\n",
        "    unique = int(ranks.nunique())\n",
        "    if unique < 2:\n",
        "        return pd.Series(0, index=y.index, dtype=int)\n",
        "    q_eff = max(2, min(int(q), unique))\n",
        "    try:\n",
        "        bins = pd.qcut(ranks, q=q_eff, labels=False, duplicates=\"drop\")\n",
        "    except Exception:\n",
        "        bins = pd.Series(0, index=y.index, dtype=int)\n",
        "    # riempi eventuali NaN con la moda\n",
        "    if bins.isna().any():\n",
        "        mode_bin = int(bins.dropna().mode().iat[0]) if not bins.dropna().empty else 0\n",
        "        bins = bins.fillna(mode_bin).astype(int)\n",
        "    return bins.astype(int)\n",
        "\n",
        "def _safe_stratify(labels: pd.Series | np.ndarray, min_per_class: int = 2):\n",
        "    \"\"\"Ritorna labels se idonee alla stratificazione, altrimenti None.\"\"\"\n",
        "    lab = pd.Series(labels)\n",
        "    vc = lab.value_counts()\n",
        "    if len(vc) < 2 or (vc < min_per_class).any():\n",
        "        return None\n",
        "    return lab.values\n",
        "\n",
        "# --- 2) Split con blocco duplicati per GROUP_COL (default: ASSET_ID), fallback classico se manca\n",
        "if GROUP_COL in df_clean.columns and df_clean[GROUP_COL].notna().any():\n",
        "    # mediana target per gruppo â†’ decili a livello gruppo\n",
        "    df_clean[GROUP_COL] = df_clean[GROUP_COL].astype(str)\n",
        "    gstats = (\n",
        "        df_clean[[GROUP_COL, TARGET]]\n",
        "        .groupby(GROUP_COL, as_index=False)[TARGET]\n",
        "        .median()\n",
        "        .rename(columns={TARGET: f\"{TARGET}__group_median\"})\n",
        "    )\n",
        "\n",
        "    g_all = gstats[GROUP_COL].values\n",
        "    g_bins_all = _strat_bins(gstats[f\"{TARGET}__group_median\"], q=N_DECILES).values\n",
        "    strat_all = _safe_stratify(g_bins_all)\n",
        "\n",
        "    # primo split: test groups\n",
        "    g_tmp, g_test = train_test_split(\n",
        "        g_all,\n",
        "        test_size=TEST_SIZE,\n",
        "        random_state=SEED,\n",
        "        stratify=strat_all,\n",
        "    )\n",
        "\n",
        "    # secondo split: valid dal residuo\n",
        "    val_rel = VAL_SIZE / max(1e-9, (1.0 - TEST_SIZE))\n",
        "    val_rel = float(min(max(val_rel, 0.05), 0.8))\n",
        "\n",
        "    tmp_mask = np.isin(gstats[GROUP_COL].values, g_tmp)\n",
        "    gstats_tmp = gstats.loc[tmp_mask].copy()\n",
        "    # decili solo sui gruppi rimasti\n",
        "    bins_tmp = _strat_bins(gstats_tmp[f\"{TARGET}__group_median\"], q=N_DECILES).values\n",
        "    # mappa gruppoâ†’bin per stratify\n",
        "    bin_map_tmp = dict(zip(gstats_tmp[GROUP_COL].values, bins_tmp))\n",
        "    y_tmp_bins = np.array([bin_map_tmp.get(g, 0) for g in g_tmp])\n",
        "    strat_tmp = _safe_stratify(y_tmp_bins)\n",
        "\n",
        "    g_train, g_valid = train_test_split(\n",
        "        g_tmp,\n",
        "        test_size=val_rel,\n",
        "        random_state=SEED,\n",
        "        stratify=strat_tmp,\n",
        "    )\n",
        "\n",
        "    G_TRAIN, G_VALID, G_TEST = set(g_train), set(g_valid), set(g_test)\n",
        "    df_train = df_clean[df_clean[GROUP_COL].isin(G_TRAIN)].copy()\n",
        "    df_valid = df_clean[df_clean[GROUP_COL].isin(G_VALID)].copy()\n",
        "    df_test  = df_clean[df_clean[GROUP_COL].isin(G_TEST)].copy()\n",
        "\n",
        "    # overlap check\n",
        "    def _overlap(a, b):\n",
        "        return set(a[GROUP_COL].astype(str)) & set(b[GROUP_COL].astype(str))\n",
        "    ov_tv = _overlap(df_train, df_valid)\n",
        "    ov_tt = _overlap(df_train, df_test)\n",
        "    ov_vt = _overlap(df_valid, df_test)\n",
        "    assert len(ov_tv) == 0 and len(ov_tt) == 0 and len(ov_vt) == 0, (\n",
        "        f\"Overlap {GROUP_COL} tra split! \"\n",
        "        f\"trainâˆ©valid={list(ov_tv)[:5]}, trainâˆ©test={list(ov_tt)[:5]}, validâˆ©test={list(ov_vt)[:5]}\"\n",
        "    )\n",
        "\n",
        "else:\n",
        "    logger.warning(\n",
        "        \"%s assente/non valido: uso fallback senza grouping (possibile leakage se ci sono duplicati).\",\n",
        "        GROUP_COL,\n",
        "    )\n",
        "\n",
        "    # stratify riga-level su decili target\n",
        "    bins_all = _strat_bins(df_clean[TARGET], q=N_DECILES)\n",
        "    df_tmp, df_test = train_test_split(\n",
        "        df_clean,\n",
        "        test_size=TEST_SIZE,\n",
        "        random_state=SEED,\n",
        "        stratify=_safe_stratify(bins_all),\n",
        "    )\n",
        "\n",
        "    val_rel = VAL_SIZE / max(1e-9, (1.0 - TEST_SIZE))\n",
        "    val_rel = float(min(max(val_rel, 0.05), 0.8))\n",
        "    bins_tmp = _strat_bins(df_tmp[TARGET], q=N_DECILES)\n",
        "    df_train, df_valid = train_test_split(\n",
        "        df_tmp,\n",
        "        test_size=val_rel,\n",
        "        random_state=SEED,\n",
        "        stratify=_safe_stratify(bins_tmp),\n",
        "    )\n",
        "\n",
        "# --- 3) Log e verifiche generali\n",
        "for name, part in ((\"train\", df_train), (\"valid\", df_valid), (\"test\", df_test)):\n",
        "    logger.info(\"%s: %d rows, %d cols\", name, len(part), part.shape[1])\n",
        "\n",
        "# partition disjoint per index\n",
        "assert len(set(df_train.index) & set(df_valid.index)) == 0\n",
        "assert len(set(df_train.index) & set(df_test.index)) == 0\n",
        "assert len(set(df_valid.index) & set(df_test.index)) == 0\n",
        "\n",
        "# --- 4) Airbag anti-leakage sugli split\n",
        "for split_name, split_df in [(\"train\", df_train), (\"valid\", df_valid), (\"test\", df_test)]:\n",
        "    if any(\"price_per_sqm\" in c.lower() for c in split_df.columns):\n",
        "        logger.error(\"ðŸ”´ LEAKAGE: colonne 'price_per_sqm*' in df_%s!\", split_name)\n",
        "\n",
        "print(\"\\nâœ… Split completato:\")\n",
        "print(f\"  Train: {df_train.shape}\")\n",
        "print(f\"  Valid: {df_valid.shape}\")\n",
        "print(f\"  Test:  {df_test.shape}\")\n",
        "print(f\"  Group column: {GROUP_COL}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61650dba",
      "metadata": {},
      "outputs": [],
      "source": [
        "# A) PRE-CHAIN GLOBALE: canonizza geo + crea/riempi prior/derivate minime\n",
        "from notebooks.shared.common.constants import SIZE_M2\n",
        "\n",
        "\n",
        "try:\n",
        "    from notebooks.shared.common.config import ASSET_CONFIG  # type: ignore\n",
        "    _PROP = ASSET_CONFIG[\"property\"]\n",
        "    _CITY_BASE = {c.lower(): {z.lower(): v for z, v in d.items()}\n",
        "                  for c, d in (_PROP.get(\"city_base_prices\") or {}).items()}\n",
        "    _REGION_INDEX = {k.lower(): v for k, v in (_PROP.get(\"region_index\") or {\n",
        "        \"north\": 1.05, \"center\": 1.00, \"south\": 0.92\n",
        "    }).items()}\n",
        "except Exception:\n",
        "    _CITY_BASE = {}\n",
        "    _REGION_INDEX = {\"north\": 1.05, \"center\": 1.00, \"south\": 0.92}\n",
        "\n",
        "# mediane di fallback per zona e globale\n",
        "_ZONE_KEYS = set(z for d in _CITY_BASE.values() for z in d.keys())\n",
        "_ZONE_MED = {z: float(np.nanmedian([d.get(z, np.nan) for d in _CITY_BASE.values()])) for z in _ZONE_KEYS} if _CITY_BASE else {}\n",
        "_GLOBAL_CITYZONE_MED = float(np.nanmedian([v for d in _CITY_BASE.values() for v in d.values()])) if _CITY_BASE else 0.0\n",
        "\n",
        "def _canon_geo(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    if \"city\" not in out.columns and \"location\" in out.columns:\n",
        "        out[\"city\"] = out[\"location\"]\n",
        "    if \"zone\" not in out.columns:\n",
        "        out[\"zone\"] = \"semi_center\"\n",
        "    if \"region\" not in out.columns:\n",
        "        out[\"region\"] = \"center\"\n",
        "    for col in (\"city\",\"zone\",\"region\"):\n",
        "        if col in out.columns:\n",
        "            out[col] = out[col].astype(str).str.strip().str.lower()\n",
        "    return out\n",
        "\n",
        "def _ensure_priors_and_min_derivatives(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Crea/riempi: no_elev_high_floor, rooms_per_100sqm, city_zone_prior, region_index_prior.\"\"\"\n",
        "    out = _canon_geo(df)\n",
        "\n",
        "    # no_elev_high_floor = penalitÃ  (niente ascensore & piano > 1)\n",
        "    f = pd.to_numeric(out.get(\"floor\"), errors=\"coerce\")\n",
        "    e = pd.to_numeric(out.get(\"has_elevator\"), errors=\"coerce\").fillna(0)\n",
        "    out[\"no_elev_high_floor\"] = ((1 - e) * np.maximum(f - 1, 0)).astype(\"float64\")\n",
        "\n",
        "    # rooms_per_100sqm\n",
        "    s = pd.to_numeric(out.get(SIZE_M2), errors=\"coerce\").replace(0, np.nan)\n",
        "    r = pd.to_numeric(out.get(\"rooms\"), errors=\"coerce\")\n",
        "    out[\"rooms_per_100sqm\"] = (100.0 * r / s).astype(\"float64\")\n",
        "\n",
        "    # city_zone_prior (CITY_BASE con fallback zonaâ†’globale)\n",
        "    ci = out.get(\"city\", pd.Series(index=out.index, dtype=str)).astype(str).str.lower()\n",
        "    zo = out.get(\"zone\", pd.Series(index=out.index, dtype=str)).astype(str).str.lower()\n",
        "    vals = []\n",
        "    for c, z in zip(ci, zo):\n",
        "        v = _CITY_BASE.get(c, {}).get(z, np.nan)\n",
        "        if pd.isna(v):\n",
        "            v = _ZONE_MED.get(z, _GLOBAL_CITYZONE_MED)\n",
        "        vals.append(v)\n",
        "    out[\"city_zone_prior\"] = np.asarray(vals, dtype=\"float64\")\n",
        "\n",
        "    # region_index_prior (macroarea)\n",
        "    out[\"region_index_prior\"] = out[\"region\"].astype(str).str.lower().map(_REGION_INDEX).astype(\"float64\")\n",
        "\n",
        "    return out\n",
        "\n",
        "# applica a tutti gli split (non Ã¨ leakage: sono feature ex-ante, no target)\n",
        "for _name in (\"df_train\",\"df_valid\",\"df_test\"):\n",
        "    if _name in globals() and isinstance(globals()[_name], pd.DataFrame):\n",
        "        globals()[_name] = _ensure_priors_and_min_derivatives(globals()[_name])\n",
        "\n",
        "try:\n",
        "    n_nan_cz = int(pd.to_numeric(df_train[\"city_zone_prior\"], errors=\"coerce\").isna().sum())\n",
        "    n_nan_ri = int(pd.to_numeric(df_train[\"region_index_prior\"], errors=\"coerce\").isna().sum())\n",
        "    n_nan_ne = int(pd.to_numeric(df_train[\"no_elev_high_floor\"], errors=\"coerce\").isna().sum())\n",
        "    n_nan_rr = int(pd.to_numeric(df_train[\"rooms_per_100sqm\"], errors=\"coerce\").isna().sum())\n",
        "    (ART_DIR / \"prechain_checks.txt\").write_text(\n",
        "        f\"train NaN city_zone_prior={n_nan_cz}, region_index_prior={n_nan_ri}, \"\n",
        "        f\"no_elev_high_floor={n_nan_ne}, rooms_per_100sqm={n_nan_rr}\\n\",\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "except Exception:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f60aec06-522b-4e18-ad3b-71c1f0e62d1b",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Anomaly Flags (train only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3379aac2-ab77-48cc-a77d-693a20d1104c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 04) Flags di outlier/anomalie SOLO sul TRAIN â†’ feature/pesi (no leakage)\n",
        "from __future__ import annotations\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- costanti con import robusto\n",
        "try:\n",
        "    from notebooks.shared.common.constants import ENV_SCORE, LUXURY_SCORE, SIZE_M2, VALUATION_K  # type: ignore\n",
        "except Exception:\n",
        "    ENV_SCORE, LUXURY_SCORE, SIZE_M2, VALUATION_K = \"env_score\", \"luxury_score\", \"size_m2\", \"valuation_k\"\n",
        "\n",
        "try:\n",
        "    from notebooks.shared.common.constants import LAG_HOURS  # type: ignore\n",
        "except Exception:\n",
        "    LAG_HOURS = \"lag_hours\"\n",
        "\n",
        "# feature â€œleakyâ€/derivate definite nellâ€™EDA (se presenti)\n",
        "try:\n",
        "    from notebooks.shared.n02_explore_dataset.eda_core import AnomalyDetector, LEAKY_FEATURES, TARGET_DERIVED_FEATURES  # type: ignore\n",
        "except Exception:\n",
        "    AnomalyDetector = None  # fallback sotto\n",
        "    LEAKY_FEATURES = {\"price_per_sqm\", \"price_per_sqm_capped\", \"price_per_sqm_vs_region_avg\", \"valuation_k_log\"}\n",
        "    TARGET_DERIVED_FEATURES = {\"_viz_price_per_sqm\", \"valuation_k_decile\", \"valuation_rank\", \"is_top_valuation\"}\n",
        "\n",
        "# Parametri con override da config\n",
        "if \"TRAIN_CFG\" not in globals() or not isinstance(TRAIN_CFG, dict):\n",
        "    TRAIN_CFG = {}\n",
        "contamination       = float(TRAIN_CFG.get(\"anomaly_contamination\", 0.03))\n",
        "strong_z_threshold  = float(TRAIN_CFG.get(\"anomaly_strong_z\", 2.5))\n",
        "severity_percentile = float(TRAIN_CFG.get(\"anomaly_severity_pct\", 90.0))\n",
        "n_estimators        = int(TRAIN_CFG.get(\"anomaly_n_estimators\", 200))\n",
        "\n",
        "# 4.1 Scelta feature candidate (numeric, no target/leaky/derived)\n",
        "num_cols = [c for c in df_train.columns if pd.api.types.is_numeric_dtype(df_train[c])]\n",
        "exclude  = set(LEAKY_FEATURES) | set(TARGET_DERIVED_FEATURES) | {VALUATION_K, \"price_per_sqm\"}\n",
        "\n",
        "prefer   = [\n",
        "    \"condition_minus_risk\", SIZE_M2, LUXURY_SCORE, ENV_SCORE,\n",
        "    \"building_age_years\", \"distance_to_center_km\", LAG_HOURS,\n",
        "    \"air_quality_index\", \"noise_level\", \"humidity_level\", \"temperature_avg\",\n",
        "]\n",
        "feat_cand = [c for c in prefer if c in df_train.columns and c in num_cols and c not in exclude]\n",
        "\n",
        "# fallback: scegli le prime N numeriche con var > 0 e almeno 10 valori unici\n",
        "if len(feat_cand) < 3:\n",
        "    cand_pool = []\n",
        "    for c in num_cols:\n",
        "        if c in exclude or c == \"sample_weight\":\n",
        "            continue\n",
        "        s = pd.to_numeric(df_train[c], errors=\"coerce\")\n",
        "        if s.nunique(dropna=True) >= 10 and np.nanvar(s.values) > 0:\n",
        "            cand_pool.append((c, float(np.nanvar(s.values))))\n",
        "    cand_pool.sort(key=lambda x: x[1], reverse=True)\n",
        "    feat_cand = [c for c, _ in cand_pool[:8]]  # max 8\n",
        "\n",
        "if feat_cand:\n",
        "    logger.info(\"Anomaly features (train only): %s\", feat_cand)\n",
        "\n",
        "    # 4.2 Rilevamento anomalie (classe ufficiale â†’ fallback z-score)\n",
        "    if AnomalyDetector is not None:\n",
        "        anom = AnomalyDetector(\n",
        "            contamination=contamination,\n",
        "            strong_z_threshold=strong_z_threshold,\n",
        "            severity_percentile=severity_percentile,\n",
        "            n_estimators=n_estimators,\n",
        "            random_state=SEED,\n",
        "        )\n",
        "        df_train_anom, anom_rep = anom.detect_anomalies(\n",
        "            df_train,\n",
        "            feature_candidates=feat_cand,\n",
        "            exclude_features=set(),  # giÃ  esclusi a monte\n",
        "        )\n",
        "    else:\n",
        "        # --- Fallback semplice: z-score medio + percentile su features candidate\n",
        "        X = df_train[feat_cand].copy()\n",
        "        X = X.apply(pd.to_numeric, errors=\"coerce\")\n",
        "        mu = X.mean(axis=0)\n",
        "        sd = X.std(axis=0).replace(0, np.nan)\n",
        "        z  = (X - mu) / sd\n",
        "        z_abs = z.abs()\n",
        "        z_mean = z_abs.mean(axis=1)  # severitÃ  media\n",
        "        thr = np.nanpercentile(z_mean.dropna().values, severity_percentile)\n",
        "        flags_raw = (z_abs > strong_z_threshold).any(axis=1)\n",
        "        flags_ref = (z_mean >= thr)\n",
        "\n",
        "        df_train_anom = df_train.copy()\n",
        "        df_train_anom[\"anomaly_flag\"]    = flags_raw.astype(np.int8)\n",
        "        df_train_anom[\"anomaly_refined\"] = flags_ref.astype(np.int8)\n",
        "        df_train_anom[\"severity_score\"]  = z_mean.fillna(0).astype(\"float32\")\n",
        "\n",
        "        n_raw = int(flags_raw.sum())\n",
        "        n_ref = int(flags_ref.sum())\n",
        "        anom_rep = {\n",
        "            \"method\": \"fallback_zscore\",\n",
        "            \"features\": feat_cand,\n",
        "            \"strong_z_threshold\": strong_z_threshold,\n",
        "            \"severity_percentile\": severity_percentile,\n",
        "            \"n_anomalies_raw\": n_raw,\n",
        "            \"n_anomalies_refined\": n_ref,\n",
        "        }\n",
        "\n",
        "    logger.info(\"Anomalie raw: %s | refined: %s\",\n",
        "                anom_rep.get(\"n_anomalies_raw\"), anom_rep.get(\"n_anomalies_refined\"))\n",
        "\n",
        "    # 4.3 trasferisci colonne utili SOLO su train (no leakage)\n",
        "    for col in (\"anomaly_flag\", \"anomaly_refined\", \"severity_score\"):\n",
        "        if col in df_train_anom.columns:\n",
        "            df_train.loc[df_train_anom.index, col] = df_train_anom[col]\n",
        "\n",
        "    # 4.4 salva report\n",
        "    (ART_DIR / \"anomaly_train_report.json\").write_text(\n",
        "        json.dumps(anom_rep, cls=NumpyJSONEncoder, indent=2, ensure_ascii=False),\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "else:\n",
        "    logger.info(\"Anomaly detection skipped: nessuna feature candidata valida.\")\n",
        "\n",
        "# 4.5 sample_weight (fallback = 1.0) â€” SOLO TRAIN\n",
        "if \"severity_score\" in df_train.columns and df_train[\"severity_score\"].notna().any():\n",
        "    sev = pd.to_numeric(df_train[\"severity_score\"], errors=\"coerce\").clip(lower=0).astype(\"float32\")\n",
        "    w   = 1.0 / (1.0 + sev)              # decresce con severitÃ \n",
        "    w   = w.clip(lower=0.2, upper=1.0)   # evita pesi troppo piccoli\n",
        "    w   = w * (1.0 / max(w.mean(), 1e-6))  # normalize meanâ‰ˆ1.0\n",
        "    df_train[\"sample_weight\"] = w.astype(\"float32\")\n",
        "    logger.info(\"sample_weight da severity_score (mean=%.3f, min=%.3f, max=%.3f)\",\n",
        "                float(w.mean()), float(w.min()), float(w.max()))\n",
        "elif \"confidence_score\" in df_train.columns and df_train[\"confidence_score\"].notna().any():\n",
        "    w = pd.to_numeric(df_train[\"confidence_score\"], errors=\"coerce\").clip(0.2, 1.0).astype(\"float32\")\n",
        "    w = w * (1.0 / max(w.mean(), 1e-6))\n",
        "    df_train[\"sample_weight\"] = w\n",
        "    logger.info(\"sample_weight da confidence_score (mean=%.3f, min=%.3f, max=%.3f)\",\n",
        "                float(w.mean()), float(w.min()), float(w.max()))\n",
        "else:\n",
        "    df_train[\"sample_weight\"] = np.float32(1.0)\n",
        "    logger.info(\"sample_weight uniforme (1.0) â€” nessuna metrica disponibile.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c915f13-189c-4380-9c72-a30680dfdd1b",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Feature Preparation & Pipelines A/B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd171f89-0d87-4a14-b26a-3e39db97ebec",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === FEATURE PREP + ANALISI + AUTO-UPDATE (UNIFICATA) ========================\n",
        "from __future__ import annotations\n",
        "import os, re, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# â”€â”€ util / costanti minime\n",
        "try:\n",
        "    from notebooks.shared.n03_train_model.preprocessing import ML_LEAKY_FEATURES  # type: ignore\n",
        "except Exception:\n",
        "    ML_LEAKY_FEATURES = {\n",
        "        \"price_per_sqm\",\"price_per_sqm_vs_region_avg\",\"price_per_sqm_capped\",\n",
        "        \"valuation_k_log\",\"_viz_price_per_sqm\",\n",
        "        \"valuation_k_decile\",\"valuation_rank\",\"is_top_valuation\"\n",
        "    }\n",
        "\n",
        "try:\n",
        "    NumpyJSONEncoder\n",
        "except NameError:\n",
        "    class NumpyJSONEncoder(json.JSONEncoder):\n",
        "        def default(self, obj):\n",
        "            import numpy as _np\n",
        "            if isinstance(obj, (_np.integer,)):  return int(obj)\n",
        "            if isinstance(obj, (_np.floating,)): return float(obj)\n",
        "            if isinstance(obj, (_np.ndarray,)):  return obj.tolist()\n",
        "            return super().default(obj)\n",
        "\n",
        "try:\n",
        "    _ensure_columns\n",
        "except NameError:\n",
        "    def _ensure_columns(df_part: pd.DataFrame, required: list[str]) -> pd.DataFrame:\n",
        "        missing = [c for c in required if c not in df_part.columns]\n",
        "        if missing:\n",
        "            for c in missing:\n",
        "                df_part[c] = np.nan\n",
        "        return df_part[required]\n",
        "\n",
        "from notebooks.shared.common.constants import VALUATION_K, ASSET_ID\n",
        "from notebooks.shared.common.config import ASSET_CONFIG\n",
        "\n",
        "# â”€â”€ helpers\n",
        "def _uniq(xs: list[str]) -> list[str]: return list(dict.fromkeys(xs))\n",
        "def _matches_any(col: str, pats: list[str]) -> bool: return any(re.search(p, col, re.I) for p in pats)\n",
        "def _is_numeric(s: pd.Series) -> bool: return pd.api.types.is_numeric_dtype(s)\n",
        "\n",
        "# ==== 0) Policy allowlist + esclusioni (guided by config) ====================\n",
        "_cfg = ASSET_CONFIG[\"property\"]\n",
        "CFG_CAT = list(_cfg.get(\"categorical\", []))\n",
        "CFG_NUM = list(_cfg.get(\"numeric\", []))\n",
        "CFG_EXC = set(_cfg.get(\"exclude\", []))\n",
        "\n",
        "EXTRA_KEEP = [\"rooms\",\"has_elevator\",\"has_garage\",\"has_garden\",\"has_balcony\",\n",
        "              \"is_top_floor\",\"listing_month\",\"city\",\"zone\",\"urban_type\"]\n",
        "FEATURE_ALLOWLIST = _uniq([*(CFG_CAT + CFG_NUM), *EXTRA_KEEP])\n",
        "\n",
        "ALWAYS_EXCLUDE = set(CFG_EXC) | {\n",
        "    VALUATION_K,\n",
        "    ASSET_ID, \"record_id\",\"listing_id\",\"asset_type_id\",\n",
        "    \"source\",\"source_name\",\"source_url\",\"dataset_version\",\n",
        "    \"prediction_ts\",\"last_verified_ts\",\"ingestion_ts\",\n",
        "    \"created_at\",\"updated_at\",\"listing_ts\",\"lag_hours\",\n",
        "    \"sample_weight\",\"weight\",\"severity_score\",\n",
        "    \"outlier_count\",\"n_outlier_sources\",\"outlier_source\",\n",
        "    \"confidence_score\",\n",
        "    \"y_pred\",\"predicted_valuation_k\",\"valuation_k_hat\",\n",
        "    \"valuation_k_log\",\"valuation_k_decile\",\"valuation_rank\",\"is_top_valuation\",\n",
        "    \"strongly_incoherent\",\"price_per_sqm_capped_violated\",\n",
        "}\n",
        "\n",
        "EXCLUDE_PATTERNS = [\n",
        "    r\"price_per_sqm\",\n",
        "    r\"^valuation_k_.+\",\n",
        "    r\"(?:^|_)id$\",\n",
        "    r\"(?:^|_)(created|updated|ingestion|prediction|last_verified|listing)_ts$\",\n",
        "    r\"_url$|_hash$\",\n",
        "    r\"^y_pred$|_pred(?:iction)?_\",\n",
        "    r\"(?:^|_)(avg|mean|median|benchmark|zscore|rank|decile)(?:_|$).*?(price|valuation)\",\n",
        "    r\"(price|valuation).*?(avg|mean|median|benchmark|zscore|rank|decile)\",\n",
        "    r\"(?:^|_)drift(?:_|$)|(?:^|_)caps?(?:_|$)|(?:^|_)vs_(?:_|$)\",\n",
        "]\n",
        "\n",
        "# leakage hard-stop\n",
        "leaky_check = [c for c in ML_LEAKY_FEATURES if c in df_train.columns]\n",
        "if leaky_check:\n",
        "    raise ValueError(f\"Leakage detected in training set: {leaky_check}\")\n",
        "\n",
        "DYNAMIC_EXCLUDE = {c for c in df_train.columns if _matches_any(c, EXCLUDE_PATTERNS)}\n",
        "EXCLUDE_ALL = set(ALWAYS_EXCLUDE) | set(DYNAMIC_EXCLUDE)\n",
        "\n",
        "# ==== 1) Split iniziale cat / num (allowlist-aware) ==========================\n",
        "# Nota: per evitare overweight della macroarea, NON obblighiamo 'region' come categorica.\n",
        "CATEGORICAL_FEATURES = _uniq([\"city\",\"zone\",\"urban_type\"] + [c for c in CFG_CAT if c not in {\"location\"}])\n",
        "cat_cols = [c for c in CATEGORICAL_FEATURES if c in df_train.columns and c not in EXCLUDE_ALL]\n",
        "num_cols = [c for c in df_train.columns if (c not in EXCLUDE_ALL) and _is_numeric(df_train[c])]\n",
        "\n",
        "allow = set([c for c in FEATURE_ALLOWLIST if c in df_train.columns])\n",
        "cat_cols = [c for c in _uniq(cat_cols) if c in allow]\n",
        "num_cols = [c for c in _uniq(num_cols) if (c not in set(cat_cols)) and (c in allow)]\n",
        "\n",
        "MIN_FEATS = int(os.getenv(\"MIN_FEATS_ALLOWLIST\", \"12\"))\n",
        "if len(cat_cols) + len(num_cols) < MIN_FEATS:\n",
        "    more_cat = [c for c in CATEGORICAL_FEATURES if c in df_train.columns and c not in EXCLUDE_ALL and c not in cat_cols]\n",
        "    more_num = []\n",
        "    for c in df_train.columns:\n",
        "        if c in EXCLUDE_ALL or c in cat_cols or c in num_cols: \n",
        "            continue\n",
        "        s = pd.to_numeric(df_train[c], errors=\"coerce\")\n",
        "        if _is_numeric(s) and s.nunique(dropna=True) >= 10 and np.nanvar(s.values) > 0:\n",
        "            more_num.append(c)\n",
        "    cat_cols = _uniq(cat_cols + more_cat)[:15]\n",
        "    num_cols = _uniq(num_cols + more_num)[:25]\n",
        "\n",
        "constant_cols = [c for c in num_cols if df_train[c].nunique(dropna=True) <= 1]\n",
        "if constant_cols:\n",
        "    num_cols = [c for c in num_cols if c not in constant_cols]\n",
        "\n",
        "# ==== 2) (opzionale) step di derivazione â€” SOLO via transformer importabile ===\n",
        "# Niente logiche/priors qui: se il transformer c'Ã¨ lo usiamo, altrimenti si procede senza.\n",
        "_include_derive = False\n",
        "DERIVED_FEATURES = [\n",
        "    \"log_size_m2\",\"sqm_per_room\",\"baths_per_100sqm\",\n",
        "    \"elev_x_floor\",\"no_elev_high_floor\",\"rooms_per_100sqm\",\n",
        "    \"city_zone_prior\",\"region_index_prior\",\n",
        "]\n",
        "feature_deriver = \"passthrough\"\n",
        "try:\n",
        "    # prova piÃ¹ namespace\n",
        "    try:\n",
        "        from notebooks.shared.common.transformers import PropertyDerivedFeatures  # type: ignore\n",
        "    except Exception:\n",
        "        from notebooks.shared.common.transformers import PropertyDerivedFeatures  # type: ignore  # noqa\n",
        "    feature_deriver = PropertyDerivedFeatures()  # usa i default interni\n",
        "    _include_derive = True\n",
        "except Exception:\n",
        "    _include_derive = False\n",
        "\n",
        "# se il deriver Ã¨ attivo, dichiara le derivate tra le numeriche (verranno create nello step precedente al prep)\n",
        "if _include_derive:\n",
        "    num_cols = _uniq(num_cols + DERIVED_FEATURES)\n",
        "\n",
        "# ==== 3) Preprocessori (OHE compat) =========================================\n",
        "def _build_ohe(min_freq=None, as_sparse=True):\n",
        "    kw = dict(handle_unknown=\"ignore\")\n",
        "    if isinstance(min_freq, (int, float)):\n",
        "        try: kw[\"min_frequency\"] = min_freq\n",
        "        except TypeError: pass\n",
        "    try:\n",
        "        return OneHotEncoder(sparse_output=as_sparse, **kw)  # sklearn >=1.2\n",
        "    except TypeError:\n",
        "        return OneHotEncoder(sparse=as_sparse, **kw)         # sklearn <1.2\n",
        "\n",
        "min_freq = TRAIN_CFG.get(\"ohe_min_frequency\", None)\n",
        "\n",
        "cat_pipe = Pipeline([\n",
        "    (\"impute\", SimpleImputer(strategy=\"constant\", fill_value=\"__MISSING__\")),\n",
        "    (\"encode\", _build_ohe(min_freq)),\n",
        "])\n",
        "num_pipe = Pipeline([(\"impute\", SimpleImputer(strategy=\"median\"))])\n",
        "\n",
        "# ==== 4) Colonne & pipeline A/B =============================================\n",
        "num_cols_B = [c for c in num_cols if c != \"confidence_score\"]\n",
        "\n",
        "preproc_A = ColumnTransformer([(\"cat\", cat_pipe, cat_cols), (\"num\", num_pipe, num_cols)], remainder=\"drop\")\n",
        "preproc_B = ColumnTransformer([(\"cat\", cat_pipe, cat_cols), (\"num\", num_pipe, num_cols_B)], remainder=\"drop\")\n",
        "\n",
        "features_A = _uniq(cat_cols + num_cols)\n",
        "features_B = _uniq(cat_cols + num_cols_B)\n",
        "\n",
        "df_train_A = _ensure_columns(df_train.copy(), features_A)\n",
        "df_valid_A = _ensure_columns(df_valid.copy(), features_A)\n",
        "df_test_A  = _ensure_columns(df_test.copy(),  features_A)\n",
        "\n",
        "df_train_B = _ensure_columns(df_train.copy(), features_B)\n",
        "df_valid_B = _ensure_columns(df_valid.copy(), features_B)\n",
        "df_test_B  = _ensure_columns(df_test.copy(),  features_B)\n",
        "\n",
        "X_train = df_train_A[features_A].copy()\n",
        "X_valid = df_valid_A[features_A].copy()\n",
        "X_test  = df_test_A[features_A].copy()\n",
        "\n",
        "Xtr_B = df_train_B[features_B].copy()\n",
        "Xva_B = df_valid_B[features_B].copy()\n",
        "Xte_B = df_test_B[features_B].copy()\n",
        "\n",
        "MODEL_KIND = str(TRAIN_CFG.get(\"model\", os.getenv(\"MODEL_KIND\",\"rf\"))).lower()\n",
        "ModelA = RandomForestRegressor; ModelB = RandomForestRegressor\n",
        "MODEL_FAMILY_A = MODEL_FAMILY_B = \"RandomForest\"\n",
        "\n",
        "if MODEL_KIND in {\"xgb\",\"xgboost\"}:\n",
        "    try:\n",
        "        from xgboost import XGBRegressor  # type: ignore\n",
        "        ModelA = ModelB = XGBRegressor\n",
        "        MODEL_FAMILY_A = MODEL_FAMILY_B = \"XGBRegressor\"\n",
        "    except Exception:\n",
        "        MODEL_FAMILY_A = MODEL_FAMILY_B = \"RandomForest\"\n",
        "\n",
        "if MODEL_FAMILY_A == \"RandomForest\":\n",
        "    model_A = ModelA(n_estimators=400, random_state=SEED, n_jobs=-1, max_depth=None, min_samples_leaf=2)\n",
        "    model_B = ModelB(n_estimators=400, random_state=SEED, n_jobs=-1, max_depth=None, min_samples_leaf=2)\n",
        "else:\n",
        "    model_A = ModelA(n_estimators=500, max_depth=6, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8,\n",
        "                     reg_alpha=0.0, reg_lambda=1.0, random_state=SEED, tree_method=\"hist\")\n",
        "    model_B = ModelB(n_estimators=500, max_depth=6, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8,\n",
        "                     reg_alpha=0.0, reg_lambda=1.0, random_state=SEED, tree_method=\"hist\")\n",
        "\n",
        "steps_A = [(\"prep\", preproc_A), (\"model\", model_A)]\n",
        "steps_B = [(\"prep\", preproc_B), (\"model\", model_B)]\n",
        "if _include_derive:\n",
        "    steps_A = [(\"derive\", feature_deriver)] + steps_A\n",
        "    steps_B = [(\"derive\", feature_deriver)] + steps_B\n",
        "\n",
        "pipe_A = Pipeline(steps_A)\n",
        "pipe_B = Pipeline(steps_B)\n",
        "\n",
        "# ==== 5) Analisi semplice & artefatti =======================================\n",
        "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
        "card_rows = []\n",
        "for c in features_A:\n",
        "    if c not in df_train.columns: continue\n",
        "    s = df_train[c]\n",
        "    card_rows.append({\n",
        "        \"feature\": c, \"dtype\": str(s.dtype),\n",
        "        \"n_unique\": int(s.nunique(dropna=True)),\n",
        "        \"pct_missing\": float(s.isna().mean()*100.0),\n",
        "        \"is_categorical_like\": bool(s.dtype.name in (\"object\",\"category\",\"bool\")),\n",
        "        \"is_numeric\": bool(pd.api.types.is_numeric_dtype(s)),\n",
        "    })\n",
        "pd.DataFrame(card_rows).sort_values([\"is_categorical_like\",\"n_unique\"], ascending=[False,True]) \\\n",
        "  .to_csv(ART_DIR / \"mlprep_cardinality_missing.csv\", index=False)\n",
        "\n",
        "# ==== 6) Allinea target/pesi (senza manipolarli qui) =========================\n",
        "def _align_targets_and_weights():\n",
        "    global y_train, y_valid, y_test, y_val_orig, y_test_orig, w_train\n",
        "    y_train = df_train.loc[X_train.index, VALUATION_K].to_numpy()\n",
        "    y_valid = df_valid.loc[X_valid.index, VALUATION_K].to_numpy()\n",
        "    y_test  = df_test.loc[X_test.index,  VALUATION_K].to_numpy()\n",
        "    y_val_orig  = y_valid.copy(); y_test_orig = y_test.copy()\n",
        "    if \"Xtr_B\" in globals():\n",
        "        if \"sample_weight\" in df_train.columns:\n",
        "            w_train = df_train.loc[Xtr_B.index, \"sample_weight\"].astype(\"float32\").to_numpy()\n",
        "        else:\n",
        "            w_train = np.ones(len(Xtr_B), dtype=\"float32\")\n",
        "    assert len(X_train)==len(y_train) and len(X_valid)==len(y_val_orig) and len(X_test)==len(y_test_orig)\n",
        "_align_targets_and_weights()\n",
        "\n",
        "print(\"\\n===== VERIFICA FEATURES =====\")\n",
        "print(\"Categoriche:\", cat_cols)\n",
        "print(\"Numeriche  :\", num_cols)\n",
        "print(\"Deriver attivo?:\", _include_derive)\n",
        "print(\"Model     A:\", MODEL_FAMILY_A, \"| n_features:\", len(features_A))\n",
        "print(\"Model     B:\", MODEL_FAMILY_B, \"| n_features:\", len(features_B))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ee34b56",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === PATCH EVAL: assicurati che le derivate/priors ci siano (no all-NaN) ===\n",
        "from __future__ import annotations\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "# prova a leggere i mapping da config (fallback sicuri)\n",
        "try:\n",
        "    from notebooks.shared.common.config import ASSET_CONFIG  # type: ignore\n",
        "    _PROP = ASSET_CONFIG[\"property\"]\n",
        "    _CITY_BASE = {c.lower(): {z.lower(): v for z, v in d.items()}\n",
        "                  for c, d in (_PROP.get(\"city_base_prices\") or {}).items()}\n",
        "    _REGION_INDEX = {k.lower(): float(v) for k, v in (_PROP.get(\"region_index\") or {\n",
        "        \"north\": 1.05, \"center\": 1.00, \"south\": 0.92\n",
        "    }).items()}\n",
        "except Exception:\n",
        "    _CITY_BASE = {}\n",
        "    _REGION_INDEX = {\"north\": 1.05, \"center\": 1.00, \"south\": 0.92}\n",
        "\n",
        "# mediane di fallback per zona e globale (se serve)\n",
        "_ZONE_KEYS = set(z for d in _CITY_BASE.values() for z in d.keys())\n",
        "_ZONE_MED = {z: float(np.nanmedian([d.get(z, np.nan) for d in _CITY_BASE.values()]))\n",
        "             for z in _ZONE_KEYS} if _CITY_BASE else {}\n",
        "_GLOBAL_CITYZONE_MED = (float(np.nanmedian([v for d in _CITY_BASE.values() for v in d.values()]))\n",
        "                        if _CITY_BASE else 0.0)\n",
        "\n",
        "_DERIVED_ALL = [\n",
        "    \"log_size_m2\",\"sqm_per_room\",\"baths_per_100sqm\",\"elev_x_floor\",\n",
        "    \"no_elev_high_floor\",\"rooms_per_100sqm\",\"city_zone_prior\",\"region_index_prior\",\n",
        "]\n",
        "\n",
        "def _needs(col: str, df: pd.DataFrame) -> bool:\n",
        "    \"\"\"Serve calcolarla? Solo se attesa nelle features e assente o tutta NaN.\"\"\"\n",
        "    if 'features_A' not in globals():\n",
        "        return False\n",
        "    if col not in features_A:\n",
        "        return False\n",
        "    if col not in df.columns:\n",
        "        return True\n",
        "    s = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "    return not s.notna().any()\n",
        "\n",
        "def _ensure_eval_derivatives(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "\n",
        "    # basi comode\n",
        "    size = pd.to_numeric(out.get(\"size_m2\"), errors=\"coerce\")\n",
        "    rooms = pd.to_numeric(out.get(\"rooms\"), errors=\"coerce\")\n",
        "    baths = pd.to_numeric(out.get(\"bathrooms\"), errors=\"coerce\")\n",
        "    floor = pd.to_numeric(out.get(\"floor\"), errors=\"coerce\")\n",
        "    elev  = pd.to_numeric(out.get(\"has_elevator\"), errors=\"coerce\").fillna(0)\n",
        "\n",
        "    # 1) derivate geometriche/funzionali\n",
        "    if _needs(\"log_size_m2\", out) and \"size_m2\" in out.columns:\n",
        "        out[\"log_size_m2\"] = np.log1p(size)\n",
        "\n",
        "    if _needs(\"sqm_per_room\", out) and {\"size_m2\",\"rooms\"}.issubset(out.columns):\n",
        "        out[\"sqm_per_room\"] = size / rooms.replace(0, np.nan)\n",
        "\n",
        "    if _needs(\"baths_per_100sqm\", out) and {\"bathrooms\",\"size_m2\"}.issubset(out.columns):\n",
        "        out[\"baths_per_100sqm\"] = 100.0 * baths / size.replace(0, np.nan)\n",
        "\n",
        "    if _needs(\"elev_x_floor\", out) and {\"has_elevator\",\"floor\"}.issubset(out.columns):\n",
        "        out[\"elev_x_floor\"] = (elev * np.maximum(floor - 1, 0)).astype(\"float64\")\n",
        "\n",
        "    if _needs(\"no_elev_high_floor\", out) and {\"has_elevator\",\"floor\"}.issubset(out.columns):\n",
        "        out[\"no_elev_high_floor\"] = ((1 - elev) * np.maximum(floor - 1, 0)).astype(\"float64\")\n",
        "\n",
        "    if _needs(\"rooms_per_100sqm\", out) and {\"rooms\",\"size_m2\"}.issubset(out.columns):\n",
        "        out[\"rooms_per_100sqm\"] = (100.0 * rooms / size.replace(0, np.nan)).astype(\"float64\")\n",
        "\n",
        "    # 2) priors cityÃ—zone e macroarea\n",
        "    if _needs(\"city_zone_prior\", out):\n",
        "        if \"city\" not in out.columns and \"location\" in out.columns:\n",
        "            out[\"city\"] = out[\"location\"]\n",
        "        if \"zone\" not in out.columns:\n",
        "            out[\"zone\"] = \"semi_center\"\n",
        "        ci = out.get(\"city\").astype(str).str.strip().str.lower() if \"city\" in out.columns else pd.Series(\"\", index=out.index)\n",
        "        zo = out.get(\"zone\").astype(str).str.strip().str.lower() if \"zone\" in out.columns else pd.Series(\"semi_center\", index=out.index)\n",
        "        vals = []\n",
        "        for c, z in zip(ci, zo):\n",
        "            v = _CITY_BASE.get(c, {}).get(z, np.nan)\n",
        "            if pd.isna(v):\n",
        "                v = _ZONE_MED.get(z, _GLOBAL_CITYZONE_MED)\n",
        "            vals.append(v)\n",
        "        out[\"city_zone_prior\"] = np.asarray(vals, dtype=\"float64\")\n",
        "\n",
        "    if _needs(\"region_index_prior\", out):\n",
        "        if \"region\" not in out.columns:\n",
        "            out[\"region\"] = \"center\"\n",
        "        out[\"region_index_prior\"] = out[\"region\"].astype(str).str.strip().str.lower().map(_REGION_INDEX).astype(\"float64\")\n",
        "\n",
        "    return out\n",
        "\n",
        "# applica solo se davvero servono (evita side-effect inutili)\n",
        "for _name in (\"df_train\",\"df_valid\",\"df_test\"):\n",
        "    if _name in globals() and isinstance(globals()[_name], pd.DataFrame):\n",
        "        globals()[_name] = _ensure_eval_derivatives(globals()[_name])\n",
        "\n",
        "# riallinea le matrici usate in queste celle di valutazione\n",
        "if \"features_A\" in globals():\n",
        "    X_train = _ensure_columns(df_train.copy(), features_A) if \"df_train\" in globals() else X_train\n",
        "    X_valid = _ensure_columns(df_valid.copy(), features_A) if \"df_valid\" in globals() else X_valid\n",
        "    X_test  = _ensure_columns(df_test.copy(),  features_A) if \"df_test\"  in globals() else X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e4b3f3d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Leakage sentinel + scan proxy (TTR, target in scala naturale) ===\n",
        "from __future__ import annotations\n",
        "import re, numpy as np\n",
        "from sklearn.base import clone\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "def _report(y_true, y_pred, tag: str):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    print(f\"{tag}  MAE={mae:.2f}  RMSE={rmse:.2f}  R2={r2:.4f}\")\n",
        "\n",
        "# y (scala naturale) per VALID â€” fallback se non definito\n",
        "try:\n",
        "    y_val_orig\n",
        "except NameError:\n",
        "    y_val_orig = df_valid.loc[X_valid.index, VALUATION_K].to_numpy(dtype=np.float64)\n",
        "\n",
        "# 1) Sentinel: shuffle del target in scala naturale\n",
        "y_train_shuf = shuffle(y_train, random_state=SEED).astype(np.float64)\n",
        "\n",
        "# ricostruisci estimatore coerente con la prep\n",
        "try:\n",
        "    base_model = clone(model_A)\n",
        "except Exception:\n",
        "    base_model = RandomForestRegressor(n_estimators=400, max_depth=None,\n",
        "                                       min_samples_leaf=2, random_state=SEED, n_jobs=-1)\n",
        "\n",
        "inner_pipe = Pipeline([(\"prep\", preproc_A), (\"model\", base_model)])\n",
        "\n",
        "def _log1p64(y):  return np.log1p(np.asarray(y, dtype=np.float64))\n",
        "def _expm164(y):  return np.expm1(np.asarray(y, dtype=np.float64))\n",
        "\n",
        "sentinel_reg = TransformedTargetRegressor(\n",
        "    regressor=inner_pipe,\n",
        "    func=_log1p64,\n",
        "    inverse_func=_expm164,\n",
        "    check_inverse=False,\n",
        ")\n",
        "\n",
        "fit_params = {}\n",
        "if \"w_train\" in globals() and isinstance(w_train, np.ndarray) and len(w_train) == len(X_train):\n",
        "    # âœ… chiave relativa alla Pipeline interna (step \"model\")\n",
        "    fit_params = {\"model__sample_weight\": w_train}\n",
        "\n",
        "sentinel_reg.fit(X_train, y_train_shuf, **fit_params)\n",
        "pred_val_shuf = np.clip(sentinel_reg.predict(X_valid), 0, None)\n",
        "_report(y_val_orig, pred_val_shuf, \"Leakage sentinel (VALID, shuffled y)\")\n",
        "\n",
        "# 2) Scan colonne proxy sospette nelle features in uso\n",
        "sus_patterns = [\n",
        "    r\"(?:^|_)(avg|mean|median|benchmark|zscore|rank|decile)(?:_|$).*?(price|valuation)\",\n",
        "    r\"(price|valuation).*?(avg|mean|median|benchmark|zscore|rank|decile)\",\n",
        "    r\"(?:^|_)drift(?:_|$)\", r\"(?:^|_)caps?(?:_|$)\", r\"(?:^|_)vs_(?:_|$)\"\n",
        "]\n",
        "def _is_susp(c: str) -> bool:\n",
        "    return any(re.search(p, c, re.I) for p in sus_patterns)\n",
        "\n",
        "in_use_cols = list(X_train.columns)\n",
        "print(\"Colonne proxy sospette in uso:\", [c for c in in_use_cols if _is_susp(c)] or \"â€”\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5e232fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === PATCH CV: pre-trasformazione robusta + fast pipe clone-safe ==============\n",
        "from __future__ import annotations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import clone\n",
        "\n",
        "# 1) carica mapping priors (robusto)\n",
        "try:\n",
        "    from notebooks.shared.common.config import ASSET_CONFIG  # type: ignore\n",
        "    _PROP = ASSET_CONFIG[\"property\"]\n",
        "    _CITY_BASE = {c.lower(): {z.lower(): float(v) for z, v in d.items()}\n",
        "                  for c, d in (_PROP.get(\"city_base_prices\") or {}).items()}\n",
        "    _REGION_INDEX = {k.lower(): float(v) for k, v in (_PROP.get(\"region_index\") or {\n",
        "        \"north\": 1.05, \"center\": 1.00, \"south\": 0.92\n",
        "    }).items()}\n",
        "except Exception:\n",
        "    _CITY_BASE = {}\n",
        "    _REGION_INDEX = {\"north\": 1.05, \"center\": 1.00, \"south\": 0.92}\n",
        "\n",
        "# mediane di fallback\n",
        "_ZONE_KEYS = set(z for d in _CITY_BASE.values() for z in d.keys())\n",
        "_ZONE_MED = {z: float(np.nanmedian([d.get(z, np.nan) for d in _CITY_BASE.values()]))\n",
        "             for z in _ZONE_KEYS} if _CITY_BASE else {}\n",
        "_GLOBAL_CITYZONE_MED = float(np.nanmedian([v for d in _CITY_BASE.values() for v in d.values()])) if _CITY_BASE else 0.0\n",
        "\n",
        "def _required_cols_from_prep(prep: \"ColumnTransformer\") -> list[str]:\n",
        "    \"\"\"Colonne che il prep si aspetta in input (cat + num).\"\"\"\n",
        "    req = []\n",
        "    for name, _, cols in getattr(prep, \"transformers\", []):\n",
        "        if cols is None or cols == \"drop\":\n",
        "            continue\n",
        "        if isinstance(cols, (list, tuple, np.ndarray)):\n",
        "            req.extend([str(c) for c in cols])\n",
        "        else:\n",
        "            req.append(str(cols))\n",
        "    # dedup preservando ordine\n",
        "    return list(dict.fromkeys(req))\n",
        "\n",
        "def _ensure_columns(df_part: pd.DataFrame, required: list[str]) -> pd.DataFrame:\n",
        "    miss = [c for c in required if c not in df_part.columns]\n",
        "    if miss:\n",
        "        for c in miss:\n",
        "            df_part[c] = np.nan\n",
        "    return df_part[required]\n",
        "\n",
        "def _fill_priors_for_cv(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Garantisce city/zone/region canoniche e riempie city_zone_prior/region_index_prior con fallback.\"\"\"\n",
        "    out = df.copy()\n",
        "    # assicurati di avere i campi base\n",
        "    if \"city\" not in out.columns and \"location\" in out.columns:\n",
        "        out[\"city\"] = out[\"location\"]\n",
        "    if \"zone\" not in out.columns:\n",
        "        out[\"zone\"] = \"semi_center\"\n",
        "    if \"region\" not in out.columns:\n",
        "        out[\"region\"] = \"center\"\n",
        "\n",
        "    # canonizza\n",
        "    for c in (\"city\", \"zone\", \"region\"):\n",
        "        if c in out.columns:\n",
        "            out[c] = out[c].astype(str).str.strip().str.lower()\n",
        "\n",
        "    # city_zone_prior\n",
        "    if \"city_zone_prior\" not in out.columns or pd.isna(out[\"city_zone_prior\"]).all():\n",
        "        ci = out.get(\"city\", pd.Series(index=out.index, dtype=str)).astype(str)\n",
        "        zo = out.get(\"zone\", pd.Series(index=out.index, dtype=str)).astype(str)\n",
        "        vals = []\n",
        "        for c, z in zip(ci, zo):\n",
        "            v = _CITY_BASE.get(c, {}).get(z, np.nan)\n",
        "            if pd.isna(v):\n",
        "                v = _ZONE_MED.get(z, _GLOBAL_CITYZONE_MED)\n",
        "            vals.append(v)\n",
        "        out[\"city_zone_prior\"] = np.asarray(vals, dtype=\"float64\")\n",
        "    else:\n",
        "        out[\"city_zone_prior\"] = pd.to_numeric(out[\"city_zone_prior\"], errors=\"coerce\").astype(\"float64\")\n",
        "\n",
        "    # region_index_prior\n",
        "    if \"region_index_prior\" not in out.columns or pd.isna(out[\"region_index_prior\"]).all():\n",
        "        out[\"region_index_prior\"] = out[\"region\"].map(_REGION_INDEX).astype(\"float64\")\n",
        "    else:\n",
        "        out[\"region_index_prior\"] = pd.to_numeric(out[\"region_index_prior\"], errors=\"coerce\").astype(\"float64\")\n",
        "\n",
        "    # fallback finale se (rarissimo) ancora tutti NaN\n",
        "    if pd.isna(out[\"city_zone_prior\"]).all():\n",
        "        out[\"city_zone_prior\"] = float(_GLOBAL_CITYZONE_MED)\n",
        "    if pd.isna(out[\"region_index_prior\"]).all():\n",
        "        out[\"region_index_prior\"] = float(np.nanmean(list(_REGION_INDEX.values())))\n",
        "\n",
        "    return out\n",
        "\n",
        "def _build_fast_pipe_and_prefn(base_pipe: Pipeline):\n",
        "    \"\"\"\n",
        "    Ritorna (fast_pipe_senza_derive, pre_fn):\n",
        "      - fast_pipe: Pipeline(prep, rf|model) clone-safe\n",
        "      - pre_fn(X): applica derive.transform(X) se esiste, poi fill_priors e allinea alle colonne richieste dal prep\n",
        "    \"\"\"\n",
        "    steps = getattr(base_pipe, \"named_steps\", {})\n",
        "    prep = steps.get(\"prep\", None)\n",
        "    reg  = steps.get(\"rf\", steps.get(\"model\", None))\n",
        "    derive = steps.get(\"derive\", None)\n",
        "    if prep is None or reg is None:\n",
        "        raise RuntimeError(\"Pipeline base priva di 'prep' o step finale (rf/model).\")\n",
        "\n",
        "    try:\n",
        "        prep_fast = clone(prep)\n",
        "    except Exception:\n",
        "        prep_fast = prep\n",
        "    try:\n",
        "        reg_fast = clone(reg)\n",
        "    except Exception:\n",
        "        from sklearn.ensemble import RandomForestRegressor\n",
        "        reg_fast = RandomForestRegressor(n_estimators=400, random_state=SEED, n_jobs=-1, min_samples_leaf=2)\n",
        "\n",
        "    last_name = \"rf\" if \"rf\" in steps else (\"model\" if \"model\" in steps else \"est\")\n",
        "    fast_pipe = Pipeline([(\"prep\", prep_fast), (last_name, reg_fast)])\n",
        "\n",
        "    # colonne che il prep si aspetta\n",
        "    required = _required_cols_from_prep(prep)\n",
        "\n",
        "    def pre_fn(Xdf: pd.DataFrame) -> pd.DataFrame:\n",
        "        X2 = Xdf.copy()\n",
        "        # 1) derive fuori pipeline, se esiste\n",
        "        if derive is not None and hasattr(derive, \"transform\"):\n",
        "            X2 = derive.transform(X2)\n",
        "        # 2) assicurati che i prior esistano e NON siano NaN\n",
        "        X2 = _fill_priors_for_cv(X2)\n",
        "        # 3) allinea esattamente alle colonne attese dal prep\n",
        "        X2 = _ensure_columns(X2, required)\n",
        "        return X2\n",
        "\n",
        "    return fast_pipe, pre_fn\n",
        "\n",
        "# costruisci fast pipe + funzione di pre-trasformazione\n",
        "fast_pipe, _pre_fn = _build_fast_pipe_and_prefn(pipe_A)\n",
        "\n",
        "# pre-trasforma X per la CV\n",
        "X_train_cv = _pre_fn(X_train)\n",
        "# (se nella cella userai anche VALID/TEST, fai lo stesso)\n",
        "# X_valid_cv = _pre_fn(X_valid); X_test_cv = _pre_fn(X_test)\n",
        "\n",
        "# pesi (se presenti) giÃ  allineati a X_train\n",
        "w_full = None\n",
        "if \"sample_weight\" in df_train.columns:\n",
        "    w_full = df_train.loc[X_train.index, \"sample_weight\"].astype(\"float64\").to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "776b03be",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Group-aware CV: GroupShuffleSplit 5Ã— e LOLO Top-K (TTR, NO expm1 manuale) ===\n",
        "from __future__ import annotations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import GroupShuffleSplit, LeaveOneGroupOut\n",
        "from sklearn.base import clone\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "\n",
        "# 0) Wrapper numericamente stabili per TTR\n",
        "def _log1p64(y):  return np.log1p(np.asarray(y, dtype=np.float64))\n",
        "def _expm164(y):  return np.expm1(np.asarray(y, dtype=np.float64))\n",
        "\n",
        "# 1) Scegli colonna di gruppo\n",
        "GROUP_CANDIDATES = [\"location\", \"region\", \"zone\", \"urban_type\"]\n",
        "group_col = next((c for c in GROUP_CANDIDATES if c in df_train.columns), None)\n",
        "\n",
        "if group_col is None:\n",
        "    print(\"â›” Nessuna colonna di gruppo disponibile (location/region/zone/urban_type). Salto GSS/LOLO.\")\n",
        "else:\n",
        "    # y_train DEVE essere in scala naturale (kâ‚¬)\n",
        "    y_train_nat = np.asarray(y_train, dtype=np.float64)  # assicurati che y_train sia giÃ  in kâ‚¬\n",
        "    groups_full = df_train.loc[X_train_cv.index, group_col].astype(str).to_numpy()\n",
        "\n",
        "    # helper: fit+metriche su uno split (usa TTR e passa pesi allo step finale della Pipeline)\n",
        "    def _fit_eval_on_split(X, y, tr_idx, va_idx, base_pipe, w: np.ndarray | None):\n",
        "        inner = clone(base_pipe)\n",
        "\n",
        "        # TTR: log1p/expm1 gestiti qui; nessuna expm1 manuale altrove\n",
        "        reg = TransformedTargetRegressor(\n",
        "            regressor=inner,\n",
        "            func=_log1p64,\n",
        "            inverse_func=_expm164,\n",
        "            check_inverse=False,\n",
        "        )\n",
        "\n",
        "        # individua nome step finale che accetta sample_weight\n",
        "        if \"rf\" in inner.named_steps:\n",
        "            last_step = \"rf\"\n",
        "        elif \"model\" in inner.named_steps:\n",
        "            last_step = \"model\"\n",
        "        else:\n",
        "            last_step = list(inner.named_steps.keys())[-1]\n",
        "\n",
        "        fit_params = {}\n",
        "        if w is not None and len(w) == len(X):\n",
        "            # âœ… chiavi relative alla Pipeline interna (niente 'regressor__')\n",
        "            fit_params = {f\"{last_step}__sample_weight\": w[tr_idx]}\n",
        "\n",
        "        reg.fit(X.iloc[tr_idx], y[tr_idx], **fit_params)\n",
        "        pred = np.clip(reg.predict(X.iloc[va_idx]), 0, None)  # giÃ  in kâ‚¬\n",
        "        true = y[va_idx]\n",
        "\n",
        "        mae = mean_absolute_error(true, pred)\n",
        "        rmse = np.sqrt(mean_squared_error(true, pred))\n",
        "        r2 = r2_score(true, pred)\n",
        "        return mae, rmse, r2\n",
        "\n",
        "    if \"rf\" in fast_pipe.named_steps:\n",
        "        fast_pipe.named_steps[\"rf\"].set_params(n_estimators=200)\n",
        "\n",
        "    # pesi (se presenti) allineati a X_train\n",
        "    w_full = None\n",
        "    if \"sample_weight\" in df_train.columns:\n",
        "        w_full = df_train.loc[X_train_cv.index, \"sample_weight\"].astype(\"float64\").to_numpy()\n",
        "\n",
        "    # --- A) GroupShuffleSplit 5Ã—\n",
        "    gss = GroupShuffleSplit(n_splits=5, test_size=0.2, random_state=SEED)\n",
        "    maeL, rmseL, r2L = [], [], []\n",
        "    for tr_idx, va_idx in gss.split(X_train_cv, y_train_nat, groups=groups_full):\n",
        "        mae, rmse, r2 = _fit_eval_on_split(X_train_cv, y_train_nat, tr_idx, va_idx, fast_pipe, w_full)\n",
        "        maeL.append(mae); rmseL.append(rmse); r2L.append(r2)\n",
        "\n",
        "    print(\n",
        "        f\"GroupShuffleSplit (5Ã—, group={group_col}) â†’ \"\n",
        "        f\"MAE={np.mean(maeL):.2f}Â±{np.std(maeL):.2f}  \"\n",
        "        f\"RMSE={np.mean(rmseL):.2f}Â±{np.std(rmseL):.2f}  \"\n",
        "        f\"R2={np.mean(r2L):.4f}Â±{np.std(r2L):.4f}\"\n",
        "    )\n",
        "\n",
        "    # --- B) LOLO Top-K (piÃ¹ rapido del LOLO completo)\n",
        "    K = 10\n",
        "    vc = pd.Series(groups_full).value_counts()\n",
        "    top_groups = vc.index[:min(K, len(vc))]\n",
        "    mask = pd.Series(groups_full).isin(top_groups).to_numpy()\n",
        "\n",
        "    Xk = X_train.loc[mask]\n",
        "    yk = y_train_nat[mask]\n",
        "    gk = pd.Series(groups_full)[mask].to_numpy()\n",
        "    wk = None if w_full is None else w_full[mask]\n",
        "\n",
        "    if len(np.unique(gk)) < 2 or len(Xk) < 10:\n",
        "        print(\"LOLO Top-K: gruppi insufficienti. Salto.\")\n",
        "    else:\n",
        "        logo = LeaveOneGroupOut()\n",
        "        maeL2, rmseL2, r2L2 = [], [], []\n",
        "        fast_pipe2 = clone(fast_pipe)\n",
        "\n",
        "        for tr_idx, va_idx in logo.split(Xk, yk, groups=gk):\n",
        "            mae, rmse, r2 = _fit_eval_on_split(Xk, yk, tr_idx, va_idx, fast_pipe2, wk)\n",
        "            maeL2.append(mae); rmseL2.append(rmse); r2L2.append(r2)\n",
        "\n",
        "        print(\n",
        "            f\"LOLO Top-{len(np.unique(gk))} (group={group_col}) â†’ \"\n",
        "            f\"MAE={np.mean(maeL2):.2f}Â±{np.std(maeL2):.2f}  \"\n",
        "            f\"RMSE={np.mean(rmseL2):.2f}Â±{np.std(rmseL2):.2f}  \"\n",
        "            f\"R2={np.mean(r2L2):.4f}Â±{np.std(r2L2):.4f}\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29ef33fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Minimal features (TTR, chiavi fit_params corrette) ===\n",
        "from __future__ import annotations\n",
        "import numpy as np\n",
        "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# wrapper numericamente stabili\n",
        "def _log1p64(y):  return np.log1p(np.asarray(y, dtype=np.float64))\n",
        "def _expm164(y):  return np.expm1(np.asarray(y, dtype=np.float64))\n",
        "\n",
        "base_keep_all = [\n",
        "    \"size_m2\",\"rooms\",\"bathrooms\",\"year_built\",\"age_years\",\n",
        "    \"floor\",\"building_floors\",\"is_top_floor\",\"is_ground_floor\",\"has_elevator\",\n",
        "    \"garage\",\"parking_spot\",\"has_garden\",\"has_balcony\",\n",
        "    \"distance_to_center_km\",\"orientation\",\"view\",\"region\",\"zone\",\"urban_type\",\"location\"\n",
        "]\n",
        "base_keep = [c for c in base_keep_all if c in df_train.columns]\n",
        "\n",
        "if len(base_keep) < 2:\n",
        "    print(\"â›” Minimal: meno di 2 feature base disponibili.\")\n",
        "else:\n",
        "    # split cat/num basati su dtype nel TRAIN\n",
        "    cat_b = [c for c in base_keep if df_train[c].dtype.name in (\"object\",\"category\",\"bool\")]\n",
        "    num_b = [c for c in base_keep if c not in set(cat_b)]\n",
        "\n",
        "    prep_b = ColumnTransformer([\n",
        "        (\"cat\", Pipeline([\n",
        "            (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "            (\"enc\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
        "        ]), cat_b),\n",
        "        (\"num\", Pipeline([\n",
        "            (\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "        ]), num_b),\n",
        "    ], remainder=\"drop\")\n",
        "\n",
        "    pipe_inner = Pipeline([\n",
        "        (\"prep\", prep_b),\n",
        "        (\"rf\", RandomForestRegressor(\n",
        "            n_estimators=400, random_state=SEED, n_jobs=-1, min_samples_leaf=2\n",
        "        )),\n",
        "    ])\n",
        "\n",
        "    # TTR: log1p/expm1 gestiti qui, niente expm1 manuale\n",
        "    pipe_min = TransformedTargetRegressor(\n",
        "        regressor=pipe_inner,\n",
        "        func=_log1p64, inverse_func=_expm164,\n",
        "        check_inverse=False,\n",
        "    )\n",
        "\n",
        "    # allinea colonne\n",
        "    def _ensure(df_part, cols):\n",
        "        miss = [c for c in cols if c not in df_part.columns]\n",
        "        if miss:\n",
        "            for c in miss: df_part[c] = np.nan\n",
        "        return df_part[cols]\n",
        "\n",
        "    Xtr_min = _ensure(df_train.copy(), base_keep).loc[X_train.index]\n",
        "    Xva_min = _ensure(df_valid.copy(), base_keep).loc[X_valid.index]\n",
        "\n",
        "    # y in scala naturale (kâ‚¬)\n",
        "    y_tr_nat = df_train.loc[Xtr_min.index, VALUATION_K].astype(\"float64\").to_numpy()\n",
        "    y_va_nat = df_valid.loc[Xva_min.index, VALUATION_K].astype(\"float64\").to_numpy()\n",
        "\n",
        "    # fit_params: **niente 'regressor__'**\n",
        "    fit_params = {}\n",
        "    if \"sample_weight\" in df_train.columns:\n",
        "        w = df_train.loc[Xtr_min.index, \"sample_weight\"].astype(\"float64\").to_numpy()\n",
        "        fit_params = {\"rf__sample_weight\": w}\n",
        "\n",
        "    pipe_min.fit(Xtr_min, y_tr_nat, **fit_params)\n",
        "    pred_min = np.clip(pipe_min.predict(Xva_min), 0, None)  # giÃ  kâ‚¬\n",
        "\n",
        "    mae = mean_absolute_error(y_va_nat, pred_min)\n",
        "    rmse = np.sqrt(mean_squared_error(y_va_nat, pred_min))\n",
        "    r2 = r2_score(y_va_nat, pred_min)\n",
        "\n",
        "    print(f\"Minimal VALID â†’ MAE={mae:.2f}  RMSE={rmse:.2f}  R2={r2:.6f}  (features: {len(base_keep)})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ebecbf5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Ablation sicura: rimuovi alcune colonne e valuta su VALID (e opz. GSS) ===\n",
        "from __future__ import annotations\n",
        "import numpy as np\n",
        "from sklearn.base import clone\n",
        "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "# -- wrapper numericamente stabili (se non giÃ  definiti)\n",
        "try:\n",
        "    _log1p64\n",
        "    _expm164\n",
        "except NameError:\n",
        "    def _log1p64(y):  return np.log1p(np.asarray(y, dtype=np.float64))\n",
        "    def _expm164(y):  return np.expm1(np.asarray(y, dtype=np.float64))\n",
        "\n",
        "def _build_preproc_from(base_prep: ColumnTransformer,\n",
        "                        keep_cat: list[str],\n",
        "                        keep_num: list[str]) -> ColumnTransformer:\n",
        "    \"\"\"\n",
        "    Ricostruisce un ColumnTransformer usando (se disponibili) i trasformatori\n",
        "    del prep di base, altrimenti crea pipe di default.\n",
        "    \"\"\"\n",
        "    cat_est, num_est = None, None\n",
        "    if hasattr(base_prep, \"transformers\"):\n",
        "        for name, est, cols in base_prep.transformers:\n",
        "            if name == \"cat\":\n",
        "                cat_est = clone(est)\n",
        "            elif name == \"num\":\n",
        "                num_est = clone(est)\n",
        "\n",
        "    if cat_est is None:\n",
        "        cat_est = Pipeline([\n",
        "            (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "            (\"encode\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
        "        ])\n",
        "    if num_est is None:\n",
        "        num_est = Pipeline([\n",
        "            (\"impute\", SimpleImputer(strategy=\"median\")),\n",
        "        ])\n",
        "\n",
        "    transformers = []\n",
        "    if keep_cat:\n",
        "        transformers.append((\"cat\", cat_est, keep_cat))\n",
        "    if keep_num:\n",
        "        transformers.append((\"num\", num_est, keep_num))\n",
        "    if not transformers:\n",
        "        raise ValueError(\"Nessuna feature rimanente per l'ablation.\")\n",
        "\n",
        "    return ColumnTransformer(transformers=transformers, remainder=\"drop\")\n",
        "\n",
        "def _ensure(df_part, cols):\n",
        "    \"\"\"Garantisce che tutte le colonne esistano (aggiunge NaN se mancano) e restituisce solo quelle in 'cols'.\"\"\"\n",
        "    miss = [c for c in cols if c not in df_part.columns]\n",
        "    if miss:\n",
        "        for c in miss: df_part[c] = np.nan\n",
        "    return df_part[cols]\n",
        "\n",
        "def _make_fit_params_for_ttr_regressor(pipe_base: Pipeline, mask=None):\n",
        "    \"\"\"\n",
        "    Crea il dict dei fit_params per passare i pesi allo step finale della Pipeline interna.\n",
        "    NOTA: con TTR NON usare 'regressor__' qui; le chiavi vanno direttamente allo step (rf/model).\n",
        "    \"\"\"\n",
        "    if (\"sample_weight\" in df_train.columns) and hasattr(pipe_base, \"named_steps\"):\n",
        "        if \"rf\" in pipe_base.named_steps:\n",
        "            key = \"rf__sample_weight\"\n",
        "        elif \"model\" in pipe_base.named_steps:\n",
        "            key = \"model__sample_weight\"\n",
        "        else:\n",
        "            last = list(pipe_base.named_steps.keys())[-1]\n",
        "            key = f\"{last}__sample_weight\"\n",
        "\n",
        "        w = df_train.loc[X_train.index, \"sample_weight\"].astype(\"float64\").to_numpy()\n",
        "        if mask is not None:\n",
        "            w = w[mask]\n",
        "        return {key: w}\n",
        "    return {}\n",
        "\n",
        "def eval_drop(cols_to_drop, base_pipe=pipe_A):\n",
        "    \"\"\"\n",
        "    Esegue unâ€™ablation (drop di alcune colonne) e valuta su VALID usando TTR (log1p/expm1).\n",
        "    Ritorna (ttr_pipeline, keep_features).\n",
        "    \"\"\"\n",
        "    cols_to_drop = set(cols_to_drop)\n",
        "    keep = [c for c in features_A if c not in cols_to_drop]\n",
        "    keep_cat = [c for c in cat_cols if c in keep]\n",
        "    keep_num = [c for c in num_cols if c in keep]\n",
        "\n",
        "    # ricostruisci preproc coerente\n",
        "    preproc_new = _build_preproc_from(base_pipe.named_steps[\"prep\"], keep_cat, keep_num)\n",
        "\n",
        "    # modello base\n",
        "    rf = RandomForestRegressor(\n",
        "        n_estimators=400, random_state=SEED, n_jobs=-1, max_depth=None, min_samples_leaf=2\n",
        "    )\n",
        "    pipe_base = Pipeline([(\"prep\", preproc_new), (\"rf\", rf)])\n",
        "\n",
        "    # TTR per gestire scala target in modo consistente\n",
        "    ttr = TransformedTargetRegressor(\n",
        "        regressor=pipe_base,\n",
        "        func=_log1p64,\n",
        "        inverse_func=_expm164,\n",
        "        check_inverse=False,\n",
        "    )\n",
        "\n",
        "    # prepara matrici e target (scala naturale kâ‚¬)\n",
        "    Xtr = _ensure(df_train.copy(), keep).loc[X_train.index]\n",
        "    Xva = _ensure(df_valid.copy(), keep).loc[X_valid.index]\n",
        "    y_tr_nat = df_train.loc[Xtr.index, VALUATION_K].astype(\"float64\").to_numpy()\n",
        "    y_va_nat = df_valid.loc[Xva.index, VALUATION_K].astype(\"float64\").to_numpy()\n",
        "\n",
        "    # fit con pesi (se presenti)\n",
        "    fit_params = _make_fit_params_for_ttr_regressor(pipe_base)\n",
        "    ttr.fit(Xtr, y_tr_nat, **fit_params)\n",
        "\n",
        "    # predizioni giÃ  in scala naturale\n",
        "    predV = np.clip(ttr.predict(Xva), 0, None)\n",
        "    mae = mean_absolute_error(y_va_nat, predV)\n",
        "    rmse = np.sqrt(mean_squared_error(y_va_nat, predV))\n",
        "    r2 = r2_score(y_va_nat, predV)\n",
        "\n",
        "    print(f\"DROP {sorted(cols_to_drop)} â†’ VALID  MAE={mae:.2f}  RMSE={rmse:.2f}  R2={r2:.4f}  (kept {len(keep)} cols)\")\n",
        "    return ttr, keep\n",
        "\n",
        "# Esempi di uso (come prima)\n",
        "p_drop_both, keep_cols_both = eval_drop([\"size_m2\", \"distance_to_center_km\"])\n",
        "p_drop_size, keep_cols_size = eval_drop([\"size_m2\"])\n",
        "p_drop_dist, keep_cols_dist = eval_drop([\"distance_to_center_km\"])\n",
        "\n",
        "# --- GSS rapido con set ridotto (3 split) per il modello p_drop_both ---\n",
        "GROUP_CANDIDATES = [\"location\",\"region\",\"zone\",\"urban_type\"]\n",
        "group_col = next((c for c in GROUP_CANDIDATES if c in df_train.columns), None)\n",
        "\n",
        "if group_col:\n",
        "    # gruppi allineati a X_train\n",
        "    groups = df_train.loc[X_train.index, group_col].astype(str).to_numpy()\n",
        "\n",
        "    # ricostruisci una versione \"fast\" della pipeline interna con meno alberi\n",
        "    fast_inner = clone(p_drop_both.regressor)   # Pipeline(prep, rf)\n",
        "    if \"rf\" in fast_inner.named_steps:\n",
        "        fast_inner.named_steps[\"rf\"].set_params(n_estimators=200)\n",
        "\n",
        "    # TTR fast\n",
        "    ttr_fast = TransformedTargetRegressor(\n",
        "        regressor=fast_inner,\n",
        "        func=_log1p64,\n",
        "        inverse_func=_expm164,\n",
        "        check_inverse=False,\n",
        "    )\n",
        "\n",
        "    # prepara X e y in scala naturale per i fold\n",
        "    X_full = _ensure(df_train.copy(), keep_cols_both).loc[X_train.index]\n",
        "    y_full = df_train.loc[X_train.index, VALUATION_K].astype(\"float64\").to_numpy()\n",
        "\n",
        "    # pesi allineati se presenti\n",
        "    w_full = df_train.loc[X_train.index, \"sample_weight\"].astype(\"float64\").to_numpy() if \"sample_weight\" in df_train.columns else None\n",
        "\n",
        "    gss = GroupShuffleSplit(n_splits=3, test_size=0.2, random_state=SEED)\n",
        "    maeL, rmseL, r2L = [], [], []\n",
        "\n",
        "    for tr, va in gss.split(X_full, y_full, groups=groups):\n",
        "        # fit params per questo split (NB: chiavi relative alla Pipeline interna del TTR)\n",
        "        fit_params = {}\n",
        "        if w_full is not None:\n",
        "            # passali allo step 'rf' dentro regressor\n",
        "            fit_params = {\"rf__sample_weight\": w_full[tr]}\n",
        "\n",
        "        q = clone(ttr_fast)\n",
        "        q.fit(X_full.iloc[tr], y_full[tr], **fit_params)\n",
        "        pred = np.clip(q.predict(X_full.iloc[va]), 0, None)  # giÃ  in kâ‚¬\n",
        "        true = y_full[va]\n",
        "        maeL.append(mean_absolute_error(true, pred))\n",
        "        rmseL.append(np.sqrt(mean_squared_error(true, pred)))\n",
        "        r2L.append(r2_score(true, pred))\n",
        "\n",
        "    print(\n",
        "        f\"GSS 3Ã— (drop size_m2 & distance_to_center_km) â†’ \"\n",
        "        f\"MAE={np.mean(maeL):.2f}Â±{np.std(maeL):.2f}  \"\n",
        "        f\"RMSE={np.mean(rmseL):.2f}Â±{np.std(rmseL):.2f}  \"\n",
        "        f\"R2={np.mean(r2L):.4f}Â±{np.std(r2L):.4f}\"\n",
        "    )\n",
        "else:\n",
        "    print(\"GSS: nessuna colonna di gruppo disponibile.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f10dc080",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === GSS 5Ã— senza distance_to_center_km (robusto ai nomi step + TTR + pesi) ===\n",
        "from __future__ import annotations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.base import clone\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# wrapper numericamente stabili per TTR\n",
        "def _log1p64(y):  return np.log1p(np.asarray(y, dtype=np.float64))\n",
        "def _expm164(y):  return np.expm1(np.asarray(y, dtype=np.float64))\n",
        "\n",
        "def _extract_base_regressor(base_pipe) -> RandomForestRegressor:\n",
        "    \"\"\"Prova a clonare lo step finale del tuo pipe (rf/model), altrimenti crea un RF.\"\"\"\n",
        "    try:\n",
        "        steps = getattr(base_pipe, \"named_steps\", {})\n",
        "        if \"rf\" in steps:\n",
        "            return clone(steps[\"rf\"])\n",
        "        if \"model\" in steps:\n",
        "            return clone(steps[\"model\"])\n",
        "    except Exception:\n",
        "        pass\n",
        "    return RandomForestRegressor(\n",
        "        n_estimators=400, random_state=SEED, n_jobs=-1, min_samples_leaf=2\n",
        "    )\n",
        "\n",
        "def _fit_params_key_for(pipe_inner: Pipeline) -> str:\n",
        "    \"\"\"Ritorna la chiave corretta per passare sample_weight allo step finale.\"\"\"\n",
        "    if \"rf\" in pipe_inner.named_steps:\n",
        "        return \"rf__sample_weight\"\n",
        "    if \"model\" in pipe_inner.named_steps:\n",
        "        return \"model__sample_weight\"\n",
        "    last = list(pipe_inner.named_steps.keys())[-1]\n",
        "    return f\"{last}__sample_weight\"\n",
        "\n",
        "drop_col = \"distance_to_center_km\"\n",
        "keep_cols = [c for c in features_A if c != drop_col]\n",
        "keep_cat  = [c for c in cat_cols if c in keep_cols]\n",
        "keep_num  = [c for c in num_cols if c in keep_cols]\n",
        "\n",
        "# prep minimale per le colonne mantenute\n",
        "cat_est = Pipeline([\n",
        "    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"encode\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
        "])\n",
        "num_est = Pipeline([(\"impute\", SimpleImputer(strategy=\"median\"))])\n",
        "prep = ColumnTransformer([(\"cat\", cat_est, keep_cat), (\"num\", num_est, keep_num)], remainder=\"drop\")\n",
        "\n",
        "# regressor di base coerente col tuo pipe_A (o RF di default)\n",
        "base_reg = _extract_base_regressor(pipe_A)\n",
        "\n",
        "# pipeline interna + TTR (gestione log1p/expm1)\n",
        "inner = Pipeline([(\"prep\", prep), (\"rf\", base_reg)])\n",
        "ttr_template = TransformedTargetRegressor(\n",
        "    regressor=inner, func=_log1p64, inverse_func=_expm164, check_inverse=False\n",
        ")\n",
        "\n",
        "# allinea X, y (scala naturale, kâ‚¬), pesi e gruppi agli indici di X_train\n",
        "X_keep = X_train[keep_cols]\n",
        "y_nat  = df_train.loc[X_keep.index, VALUATION_K].astype(\"float64\").to_numpy()\n",
        "w_full = None\n",
        "if \"sample_weight\" in df_train.columns:\n",
        "    w_full = df_train.loc[X_keep.index, \"sample_weight\"].astype(\"float64\").to_numpy()\n",
        "\n",
        "group_col = next((c for c in [\"location\",\"region\",\"zone\",\"urban_type\"] if c in df_train.columns), None)\n",
        "if group_col is None:\n",
        "    raise RuntimeError(\"Nessuna colonna di gruppo disponibile (location/region/zone/urban_type).\")\n",
        "groups = df_train.loc[X_keep.index, group_col].astype(str).to_numpy()\n",
        "\n",
        "# GSS 5Ã—\n",
        "gss = GroupShuffleSplit(n_splits=5, test_size=0.2, random_state=SEED)\n",
        "maeL, rmseL, r2L = [], [], []\n",
        "\n",
        "# chiave corretta per i pesi\n",
        "fit_key = _fit_params_key_for(inner)\n",
        "\n",
        "for tr, va in gss.split(X_keep, y_nat, groups=groups):\n",
        "    ttr = clone(ttr_template)\n",
        "\n",
        "    fit_params = {}\n",
        "    if w_full is not None:\n",
        "        # âš ï¸ con TTR le chiavi vanno DIRETTE allo step della pipeline interna (niente 'regressor__')\n",
        "        fit_params = {fit_key: w_full[tr]}\n",
        "\n",
        "    ttr.fit(X_keep.iloc[tr], y_nat[tr], **fit_params)\n",
        "    pred = np.clip(ttr.predict(X_keep.iloc[va]), 0, None)  # giÃ  kâ‚¬\n",
        "    true = y_nat[va]\n",
        "\n",
        "    maeL.append(mean_absolute_error(true, pred))\n",
        "    rmseL.append(np.sqrt(mean_squared_error(true, pred)))\n",
        "    r2L.append(r2_score(true, pred))\n",
        "\n",
        "print(\n",
        "    f\"GSS 5Ã— (drop {drop_col}) â†’ \"\n",
        "    f\"MAE={np.mean(maeL):.2f}Â±{np.std(maeL):.2f}  \"\n",
        "    f\"RMSE={np.mean(rmseL):.2f}Â±{np.std(rmseL):.2f}  \"\n",
        "    f\"R2={np.mean(r2L):.4f}Â±{np.std(r2L):.4f}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9fdc4b9-98c5-43ac-9804-13c954b02c63",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Train & Validation (A vs B) & Champion Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91aed31d-f657-4b2e-a37a-5bf3a588b6f3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 06) Fit & validation (RF baseline A/B) con TTR, pesi corretti e metriche robuste â€” SAFE\n",
        "\n",
        "from __future__ import annotations\n",
        "import time, json, numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "def _metrics(y_true, y_pred):\n",
        "    mae  = float(mean_absolute_error(y_true, y_pred))\n",
        "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "    r2   = float(r2_score(y_true, y_pred))\n",
        "    return {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}\n",
        "\n",
        "# --- 0) Sanitize liste features: uniche e disgiunte\n",
        "def _uniq(xs):\n",
        "    return list(dict.fromkeys(xs))\n",
        "\n",
        "cat_cols = _uniq(cat_cols)\n",
        "_num_all = _uniq(num_cols)\n",
        "num_cols = [c for c in _num_all if c not in set(cat_cols)]\n",
        "num_cols_B = [c for c in num_cols if c != \"confidence_score\"]  # B: niente confidence come feature\n",
        "\n",
        "_inter = set(cat_cols) & set(num_cols)\n",
        "if _inter:\n",
        "    logger.warning(\"Colonne presenti sia in cat che num (rimosse da num): %s\", sorted(_inter))\n",
        "\n",
        "# --- 1) Preprocessori con le liste pulite\n",
        "cat_pipe = Pipeline([\n",
        "    (\"impute\", SimpleImputer(strategy=\"constant\", fill_value=\"__MISSING__\")),\n",
        "    (\"encode\", _build_ohe(min_freq)),\n",
        "])\n",
        "num_pipe = Pipeline([\n",
        "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
        "])\n",
        "\n",
        "preproc_A = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", cat_pipe, cat_cols),\n",
        "        (\"num\", num_pipe, num_cols),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        ")\n",
        "preproc_B = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", cat_pipe, cat_cols),\n",
        "        (\"num\", num_pipe, num_cols_B),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        ")\n",
        "\n",
        "# --- 2) Reallinea le colonne tra split (se mancano, NaN â†’ imputazione)\n",
        "def _ensure_columns(df_part, required):\n",
        "    missing = [c for c in required if c not in df_part.columns]\n",
        "    if missing:\n",
        "        for c in missing:\n",
        "            df_part[c] = np.nan\n",
        "        logger.info(\"Aggiunte colonne mancanti allo split: %s\", missing)\n",
        "    return df_part[required]\n",
        "\n",
        "features_A = cat_cols + num_cols\n",
        "features_B = cat_cols + num_cols_B\n",
        "\n",
        "df_train_A = _ensure_columns(df_train.copy(), features_A)\n",
        "df_valid_A = _ensure_columns(df_valid.copy(), features_A)\n",
        "df_test_A  = _ensure_columns(df_test.copy(),  features_A)\n",
        "\n",
        "df_train_B = _ensure_columns(df_train.copy(), features_B)\n",
        "df_valid_B = _ensure_columns(df_valid.copy(), features_B)\n",
        "df_test_B  = _ensure_columns(df_test.copy(),  features_B)\n",
        "\n",
        "X_train = df_train_A[features_A].copy()\n",
        "X_valid = df_valid_A[features_A].copy()\n",
        "X_test  = df_test_A[features_A].copy()\n",
        "\n",
        "Xtr_B = df_train_B[features_B].copy()\n",
        "Xva_B = df_valid_B[features_B].copy()\n",
        "Xte_B = df_test_B[features_B].copy()\n",
        "\n",
        "# --- 3) Target in SCALA NATURALE (kâ‚¬) â€” TTR farÃ  log1p/expm1\n",
        "y_train_nat = df_train[VALUATION_K].astype(\"float64\").to_numpy()\n",
        "y_val_nat   = df_valid[VALUATION_K].astype(\"float64\").to_numpy()\n",
        "y_tst_nat   = df_test[VALUATION_K].astype(\"float64\").to_numpy()\n",
        "\n",
        "# --- 4) Modelli di base\n",
        "model_A = RandomForestRegressor(\n",
        "    n_estimators=400, random_state=SEED, n_jobs=-1, max_depth=None, min_samples_leaf=2\n",
        ")\n",
        "model_B = RandomForestRegressor(\n",
        "    n_estimators=400, random_state=SEED, n_jobs=-1, max_depth=None, min_samples_leaf=2\n",
        ")\n",
        "\n",
        "# Pipeline interne con step finale chiamato \"model\"\n",
        "pipe_A_inner = Pipeline([(\"prep\", preproc_A), (\"model\", model_A)])\n",
        "pipe_B_inner = Pipeline([(\"prep\", preproc_B), (\"model\", model_B)])\n",
        "\n",
        "# TTR per applicare log1p/expm1 in modo consistente\n",
        "ttr_A = TransformedTargetRegressor(regressor=pipe_A_inner, func=np.log1p, inverse_func=np.expm1)\n",
        "ttr_B = TransformedTargetRegressor(regressor=pipe_B_inner, func=np.log1p, inverse_func=np.expm1)\n",
        "\n",
        "# --- 5) Fit A (confidence come feature, nessun peso)\n",
        "t0 = time.perf_counter()\n",
        "ttr_A.fit(X_train, y_train_nat)\n",
        "tA = time.perf_counter() - t0\n",
        "\n",
        "pred_val_A = np.clip(ttr_A.predict(X_valid), 0, None)\n",
        "pred_tst_A = np.clip(ttr_A.predict(X_test),  0, None)\n",
        "\n",
        "# --- 6) Fit B (confidence esclusa come feature, usata come PESO se presente)\n",
        "fit_params_B = {}\n",
        "if \"sample_weight\" in df_train.columns:\n",
        "    w_train = df_train.loc[Xtr_B.index, \"sample_weight\"].astype(\"float64\").to_numpy()\n",
        "    # âœ… con TTR, i parametri vanno all'interno della pipeline: regressor__model__sample_weight\n",
        "    fit_params_B = {\"model__sample_weight\": w_train}  # <-- chiave relativa alla Pipeline interna\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "ttr_B.fit(Xtr_B, y_train_nat, **fit_params_B)\n",
        "tB = time.perf_counter() - t0\n",
        "\n",
        "pred_val_B = np.clip(ttr_B.predict(Xva_B), 0, None)\n",
        "pred_tst_B = np.clip(ttr_B.predict(Xte_B), 0, None)\n",
        "\n",
        "# --- 7) Metriche su scala naturale\n",
        "mA_val = _metrics(y_val_nat, pred_val_A)\n",
        "mA_tst = _metrics(y_tst_nat, pred_tst_A)\n",
        "mB_val = _metrics(y_val_nat, pred_val_B)\n",
        "mB_tst = _metrics(y_tst_nat, pred_tst_B)\n",
        "\n",
        "print(f\"A (conf as feature)  VALID: {mA_val}  TEST: {mA_tst}   (fit {tA:.2f}s)\")\n",
        "print(f\"B (conf as weight)   VALID: {mB_val}  TEST: {mB_tst}   (fit {tB:.2f}s)\")\n",
        "\n",
        "# --- 8) Selezione champion su VALID (MAE, tie-break RMSE)\n",
        "def _champ(mA, mB):\n",
        "    if mA[\"MAE\"] < mB[\"MAE\"]:\n",
        "        return \"A\"\n",
        "    if mA[\"MAE\"] > mB[\"MAE\"]:\n",
        "        return \"B\"\n",
        "    return \"A\" if mA[\"RMSE\"] <= mB[\"RMSE\"] else \"B\"\n",
        "\n",
        "champion = _champ(mA_val, mB_val)\n",
        "print(\"Champion:\", champion)\n",
        "\n",
        "# --- 9) Salva mini-report\n",
        "summary = {\n",
        "    \"timing_sec\": {\"A\": round(tA, 3), \"B\": round(tB, 3)},\n",
        "    \"A\": {\"VALID\": mA_val, \"TEST\": mA_tst, \"n_features\": len(features_A)},\n",
        "    \"B\": {\"VALID\": mB_val, \"TEST\": mB_tst, \"n_features\": len(features_B)},\n",
        "    \"champion\": champion,\n",
        "}\n",
        "(ART_DIR / \"rf_baselines_summary.json\").write_text(\n",
        "    json.dumps(summary, indent=2, ensure_ascii=False), encoding=\"utf-8\"\n",
        ")\n",
        "logger.info(\"Saved RF baselines summary â†’ %s\", ART_DIR / \"rf_baselines_summary.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ad3bab1",
      "metadata": {},
      "source": [
        "### (RF A/B): significativitÃ  & blending"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53daff85",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 06.1) RF A vs B: bootstrap Î”MAE + breakdown per decile + blending VALIDâ†’TEST (robusto)\n",
        "from __future__ import annotations\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# === Guard-rails\n",
        "for name in [\"pred_val_A\", \"pred_val_B\", \"pred_tst_A\", \"pred_tst_B\"]:\n",
        "    if name not in globals():\n",
        "        raise RuntimeError(f\"Variabile mancante: {name}\")\n",
        "\n",
        "if len(df_test) == 0 or len(df_valid) == 0:\n",
        "    raise RuntimeError(\"Split vuoti: df_valid/df_test non possono essere vuoti.\")\n",
        "\n",
        "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _as_1d_float(x):\n",
        "    a = np.asarray(x, dtype=\"float64\").reshape(-1)\n",
        "    # sostituisci inf con nan e poi imputiamo con mediana\n",
        "    a[~np.isfinite(a)] = np.nan\n",
        "    if np.isnan(a).any():\n",
        "        med = np.nanmedian(a)\n",
        "        a = np.where(np.isnan(a), med, a)\n",
        "    return a\n",
        "\n",
        "# === Array puliti (TEST)\n",
        "y_true_t = _as_1d_float(df_test[VALUATION_K].to_numpy())\n",
        "yhatA_t  = _as_1d_float(pred_tst_A)\n",
        "yhatB_t  = _as_1d_float(pred_tst_B)\n",
        "\n",
        "# riallineo lunghezze se necessario (difetti estremi)\n",
        "n = min(len(y_true_t), len(yhatA_t), len(yhatB_t))\n",
        "y_true_t, yhatA_t, yhatB_t = y_true_t[:n], yhatA_t[:n], yhatB_t[:n]\n",
        "\n",
        "# === Paired bootstrap Î”MAE su TEST (Aâˆ’B)\n",
        "B = 1000\n",
        "rng_boot = np.random.default_rng(SEED + 101)\n",
        "idx_mat = rng_boot.integers(0, n, size=(B, n))\n",
        "\n",
        "def _mae(a, b):  # veloce e robusto\n",
        "    return np.mean(np.abs(a - b))\n",
        "\n",
        "deltas = np.fromiter(\n",
        "    (_mae(y_true_t[idx], yhatA_t[idx]) - _mae(y_true_t[idx], yhatB_t[idx]) for idx in idx_mat),\n",
        "    dtype=\"float64\", count=B\n",
        ")\n",
        "\n",
        "ci_lo, ci_hi = np.percentile(deltas, [2.5, 97.5])\n",
        "# p-value 2 code rispetto a 0\n",
        "p_two_sided = 2.0 * min((deltas <= 0).mean(), (deltas >= 0).mean())\n",
        "\n",
        "print(f\"Î”MAE (Aâˆ’B) bootstrap: mean={deltas.mean():.4f}, 95% CI=({ci_lo:.4f},{ci_hi:.4f}), pâ‰ˆ{p_two_sided:.3f}\")\n",
        "\n",
        "ab_summary = {\n",
        "    \"delta_mae_mean\": float(deltas.mean()),\n",
        "    \"ci_95\": [float(ci_lo), float(ci_hi)],\n",
        "    \"p_two_sided\": float(p_two_sided),\n",
        "    \"B\": int(B),\n",
        "    \"n_test\": int(n),\n",
        "    \"mae_A_test\": float(_mae(y_true_t, yhatA_t)),\n",
        "    \"mae_B_test\": float(_mae(y_true_t, yhatB_t)),\n",
        "}\n",
        "(ART_DIR / \"ab_bootstrap_summary.json\").write_text(json.dumps(ab_summary, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "# === Breakdown per decile (TEST)\n",
        "try:\n",
        "    dec = pd.qcut(y_true_t, q=10, labels=False, duplicates=\"drop\")\n",
        "except Exception:\n",
        "    dec = pd.Series(np.zeros_like(y_true_t, dtype=int))\n",
        "\n",
        "rep = (\n",
        "    pd.DataFrame({\"y\": y_true_t, \"yA\": yhatA_t, \"yB\": yhatB_t, \"dec\": dec})\n",
        "    .groupby(\"dec\", observed=True, sort=True)\n",
        "    .apply(lambda g: pd.Series({\n",
        "        \"n\": int(len(g)),\n",
        "        \"MAE_A\": float(mean_absolute_error(g[\"y\"], g[\"yA\"])),\n",
        "        \"MAE_B\": float(mean_absolute_error(g[\"y\"], g[\"yB\"])),\n",
        "        \"Î”MAE_AminusB\": float(mean_absolute_error(g[\"y\"], g[\"yA\"]) - mean_absolute_error(g[\"y\"], g[\"yB\"])),\n",
        "        \"R2_A\": float(r2_score(g[\"y\"], g[\"yA\"])) if g[\"y\"].nunique() > 1 else np.nan,\n",
        "        \"R2_B\": float(r2_score(g[\"y\"], g[\"yB\"])) if g[\"y\"].nunique() > 1 else np.nan,\n",
        "    }))\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "rep_csv  = ART_DIR / \"ab_compare_by_decile.csv\"\n",
        "rep_parq = ART_DIR / \"ab_compare_by_decile.parquet\"\n",
        "rep.to_csv(rep_csv, index=False)\n",
        "rep.to_parquet(rep_parq, index=False)\n",
        "print(\"Saved:\", rep_csv, \",\", rep_parq)\n",
        "\n",
        "# === Blending semplice Î±âˆˆ[0,1] su VALID â†’ reporting anche su TEST\n",
        "y_true_v = _as_1d_float(df_valid[VALUATION_K].to_numpy())\n",
        "yhatA_v  = _as_1d_float(pred_val_A)\n",
        "yhatB_v  = _as_1d_float(pred_val_B)\n",
        "\n",
        "m = min(len(y_true_v), len(yhatA_v), len(yhatB_v))\n",
        "y_true_v, yhatA_v, yhatB_v = y_true_v[:m], yhatA_v[:m], yhatB_v[:m]\n",
        "\n",
        "alphas = np.linspace(0.0, 1.0, 21)\n",
        "rows = []\n",
        "best_idx = None\n",
        "for i, a in enumerate(alphas):\n",
        "    y_blend_v = a * yhatB_v + (1.0 - a) * yhatA_v\n",
        "    y_blend_t = a * yhatB_t + (1.0 - a) * yhatA_t\n",
        "    mae_v = mean_absolute_error(y_true_v, y_blend_v)\n",
        "    mae_t = mean_absolute_error(y_true_t, y_blend_t)\n",
        "    rmse_t = np.sqrt(mean_squared_error(y_true_t, y_blend_t))\n",
        "    r2_t = r2_score(y_true_t, y_blend_t)\n",
        "    rows.append({\n",
        "        \"alpha\": float(a),\n",
        "        \"MAE_VALID\": float(mae_v),\n",
        "        \"MAE_TEST\": float(mae_t),\n",
        "        \"RMSE_TEST\": float(rmse_t),\n",
        "        \"R2_TEST\": float(r2_t),\n",
        "    })\n",
        "    if best_idx is None or mae_v < rows[best_idx][\"MAE_VALID\"]:\n",
        "        best_idx = i\n",
        "\n",
        "blend_df = pd.DataFrame(rows).sort_values(\"MAE_VALID\", ascending=True)\n",
        "blend_csv  = ART_DIR / \"blend_search_AB.csv\"\n",
        "blend_parq = ART_DIR / \"blend_search_AB.parquet\"\n",
        "blend_df.to_csv(blend_csv, index=False)\n",
        "blend_df.to_parquet(blend_parq, index=False)\n",
        "\n",
        "best_row = blend_df.iloc[0]\n",
        "print(\"Saved:\", blend_csv, \",\", blend_parq)\n",
        "print(\n",
        "    f\"Blend best Î± on VALID = {best_row['alpha']:.2f}  \"\n",
        "    f\"|  MAE_VALID={best_row['MAE_VALID']:.3f}  \"\n",
        "    f\"|  MAE_TEST={best_row['MAE_TEST']:.3f}  \"\n",
        "    f\"|  R2_TEST={best_row['R2_TEST']:.4f}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4ad2735",
      "metadata": {},
      "source": [
        "### XGBoost + Optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e53e7e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- TARGET SCALE HANDLER (EUR vs kEUR) ----\n",
        "import numpy as np\n",
        "\n",
        "TARGET_RAW_TRAIN = df_train[VALUATION_K].astype(\"float64\").to_numpy()\n",
        "p50 = float(np.nanmedian(TARGET_RAW_TRAIN))\n",
        "\n",
        "# eur_if_big: se la mediana > 2_000 assumo che la colonna sia in EURO; altrimenti Ã¨ giÃ  in kâ‚¬\n",
        "UNIT_SCALE = 1000.0 if p50 > 2000.0 else 1.0       # divide-by for training\n",
        "UNIT_LABEL = \"EUR\" if UNIT_SCALE == 1000.0 else \"kEUR\"\n",
        "\n",
        "def to_log(y_nat):\n",
        "    \"\"\"porta il target su scala log1p, uniformando all'unitÃ  di training.\"\"\"\n",
        "    return np.log1p(np.asarray(y_nat, float) / UNIT_SCALE)\n",
        "\n",
        "def from_log(y_log):\n",
        "    \"\"\"torna su scala naturale nell'UNITÃ€ ORIGINALE DEL DATASET (stessa di df[VALUATION_K]).\"\"\"\n",
        "    return np.expm1(np.asarray(y_log, float)) * UNIT_SCALE\n",
        "\n",
        "# Tetto numericamente stabile calcolato nella stessa unitÃ  del training\n",
        "p999 = float(np.nanpercentile(TARGET_RAW_TRAIN / UNIT_SCALE, 99.9))\n",
        "MAX_LOG = float(np.log1p(p999 * 10.0))  # 10Ã— il 99.9p nella stessa unitÃ  del training\n",
        "\n",
        "def safe_expm1_scaled(y_log):\n",
        "    z = np.asarray(y_log, float)\n",
        "    z = np.clip(z, -20.0, MAX_LOG)      # evita overflow\n",
        "    return np.expm1(z) * UNIT_SCALE     # torna alla stessa unitÃ  del dataset\n",
        "\n",
        "# y in log per il training, coerente con UNIT_SCALE\n",
        "y_train = to_log(df_train[VALUATION_K].values)\n",
        "y_valid = to_log(df_valid[VALUATION_K].values)\n",
        "y_test  = to_log(df_test [VALUATION_K].values)\n",
        "\n",
        "# per metriche, i \"true\" restano nella loro unitÃ  originale (df_... Ã¨ giÃ  in quella unitÃ )\n",
        "y_val_true = df_valid[VALUATION_K].astype(\"float64\").to_numpy()\n",
        "y_tst_true = df_test [VALUATION_K].astype(\"float64\").to_numpy()\n",
        "\n",
        "print(f\"Detected target unit: {UNIT_LABEL}  (median={p50:,.1f})  MAX_LOG={MAX_LOG:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef6e7363",
      "metadata": {},
      "outputs": [],
      "source": [
        "# XGBoost + Optuna â€” import & setup (coerente con notebooks/outputs/modeling/property)\n",
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    import optuna\n",
        "    from optuna.pruners import MedianPruner\n",
        "    from optuna.samplers import TPESampler\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"Servono xgboost e optuna (pip install xgboost optuna)\") from e\n",
        "\n",
        "# Dir coerenti (siamo dentro notebooks/)\n",
        "BASE_OUT = Path(\"outputs\")\n",
        "PROP_DIR = BASE_OUT / \"modeling\" / \"property\"\n",
        "XGB_DIR  = PROP_DIR / \"xgb\"\n",
        "for d in (PROP_DIR, XGB_DIR):\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Config da TRAIN_CFG (giÃ  caricato in alto nel nb)\n",
        "N_TRIALS   = int(TRAIN_CFG.get(\"xgb_optuna_trials\", 25))\n",
        "EARLY_STOP = int(TRAIN_CFG.get(\"xgb_early_stopping_rounds\", 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab0bc72f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 07ter) Optuna: tuning XGBoost (setup B: confidence come peso) â€” safe & robust\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.base import clone\n",
        "import inspect\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# tetto dinamico: 10Ã— il 99.9Â° percentile del target (in kâ‚¬), poi log1p\n",
        "MAX_LOG = float(np.log1p(np.nanpercentile(df_train[VALUATION_K].values, 99.9) * 10.0))\n",
        "\n",
        "def safe_expm1(z, max_log=MAX_LOG):\n",
        "    z = np.asarray(z, dtype=np.float64)\n",
        "    z = np.clip(z, -20.0, max_log)  # -20 ~ ~0 in scala naturale; max_log evita overflow\n",
        "    return np.expm1(z)\n",
        "\n",
        "\n",
        "# Preprocess identico a setup B (senza confidence tra le feature)\n",
        "_transformers = []\n",
        "if \"cat_cols\" in globals() and len(cat_cols) > 0:\n",
        "    _transformers.append((\"cat\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1), cat_cols))\n",
        "if \"num_cols_B\" in globals() and len(num_cols_B) > 0:\n",
        "    _transformers.append((\"num\", \"passthrough\", num_cols_B))\n",
        "preproc_B = ColumnTransformer(transformers=_transformers, remainder=\"drop\")\n",
        "\n",
        "# Feature list (dedup, ordine preservato)\n",
        "features_B = list(dict.fromkeys([*(cat_cols if \"cat_cols\" in globals() else []),\n",
        "                                 *(num_cols_B if \"num_cols_B\" in globals() else [])]))\n",
        "\n",
        "# Pesi (confidence/sample_weight) se presenti â†’ allineati all'indice di X_train\n",
        "if \"sample_weight\" in df_train.columns:\n",
        "    weights_B = df_train.loc[X_train.index, \"sample_weight\"].to_numpy(dtype=float)\n",
        "else:\n",
        "    weights_B = np.ones(len(X_train), dtype=float)\n",
        "\n",
        "def _suggest_params(trial: optuna.Trial) -> dict:\n",
        "    return {\n",
        "        \"n_estimators\":     trial.suggest_int(\"n_estimators\", 400, 1800),\n",
        "        \"max_depth\":        trial.suggest_int(\"max_depth\", 4, 12),\n",
        "        \"learning_rate\":    trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
        "        \"subsample\":        trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
        "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1.0, 10.0),\n",
        "        \"reg_alpha\":        trial.suggest_float(\"reg_alpha\", 1e-8, 1e-1, log=True),\n",
        "        \"reg_lambda\":       trial.suggest_float(\"reg_lambda\", 1e-3, 10.0, log=True),\n",
        "        \"gamma\":            trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
        "        \"random_state\":     SEED,\n",
        "        \"tree_method\":      \"hist\",\n",
        "        \"n_jobs\":           -1,\n",
        "    }\n",
        "\n",
        "def _fit_xgb_with_preproc(params: dict,\n",
        "                          Xtr_df: pd.DataFrame, ytr_log: np.ndarray,\n",
        "                          Xva_df: pd.DataFrame, yva_log: np.ndarray,\n",
        "                          w: np.ndarray, early_rounds: int = EARLY_STOP):\n",
        "    \"\"\"\n",
        "    Allena XGB su target log1p (usiamo y_train/y_valid giÃ  in log1p),\n",
        "    early-stopping su valid (MAE calcolata in log-space da XGB).\n",
        "    Le metriche 'finali' le faremo poi in scala naturale con expm1.\n",
        "    \"\"\"\n",
        "    prep = clone(preproc_B)\n",
        "    Xt = prep.fit_transform(Xtr_df)\n",
        "    Xv = prep.transform(Xva_df)\n",
        "\n",
        "    model = xgb.XGBRegressor(**params)\n",
        "    sig = inspect.signature(model.fit)\n",
        "    fit_kwargs = {}\n",
        "    if \"eval_set\" in sig.parameters:        fit_kwargs[\"eval_set\"] = [(Xv, yva_log)]\n",
        "    if \"eval_metric\" in sig.parameters:     fit_kwargs[\"eval_metric\"] = \"mae\"\n",
        "    if \"sample_weight\" in sig.parameters:   fit_kwargs[\"sample_weight\"] = w\n",
        "    if \"early_stopping_rounds\" in sig.parameters:\n",
        "        fit_kwargs[\"early_stopping_rounds\"] = early_rounds\n",
        "    elif \"callbacks\" in sig.parameters:\n",
        "        from xgboost.callback import EarlyStopping\n",
        "        fit_kwargs[\"callbacks\"] = [EarlyStopping(rounds=early_rounds, save_best=True)]\n",
        "    if \"verbose\" in sig.parameters:         fit_kwargs[\"verbose\"] = False\n",
        "\n",
        "    model.fit(Xt, ytr_log, **fit_kwargs)\n",
        "    pipe = Pipeline([(\"prep\", prep), (\"xgb\", model)])\n",
        "    return pipe, model, Xt, Xv\n",
        "\n",
        "def objective(trial: optuna.Trial) -> float:\n",
        "    params = _suggest_params(trial)\n",
        "    Xtr_df, Xva_df = X_train[features_B], X_valid[features_B]\n",
        "    # y_train/y_valid sono in log1p dalle celle RF â€” li riusiamo\n",
        "    pipe, model, Xt, Xv = _fit_xgb_with_preproc(params, Xtr_df, y_train, Xva_df, y_valid, weights_B)\n",
        "    # misura l'obiettivo in scala naturale\n",
        "    y_log_pred = model.predict(Xv)              # pred in log\n",
        "    y_log_pred = model.predict(Xv)\n",
        "    pred_nat = safe_expm1_scaled(y_log_pred)\n",
        "    if not np.all(np.isfinite(pred_nat)):  # trial instabile â†’ penalizza\n",
        "        return 1e9\n",
        "    return mean_absolute_error(y_val_true, pred_nat)\n",
        "\n",
        "study = optuna.create_study(\n",
        "    direction=\"minimize\",\n",
        "    pruner=MedianPruner(n_warmup_steps=5),\n",
        "    sampler=TPESampler(seed=SEED),\n",
        ")\n",
        "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)\n",
        "\n",
        "print(\"Best trial:\", study.best_trial.number, \"MAE (valid, natural):\", study.best_value)\n",
        "best_params = study.best_trial.params\n",
        "(XGB_DIR / \"optuna_best_params_setupB.json\").write_text(json.dumps(best_params, indent=2), encoding=\"utf-8\")\n",
        "study.trials_dataframe().to_parquet(XGB_DIR / \"optuna_trials_setupB.parquet\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "550bceee",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 07quater-bis) Build XGB finale (setup B) + metriche VALID/TEST\n",
        "\n",
        "import inspect, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "def _rmse(y_true, y_pred):\n",
        "    try:\n",
        "        return mean_squared_error(y_true, y_pred, squared=False)\n",
        "    except TypeError:\n",
        "        return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "# Best params da Optuna o fallback sensato\n",
        "bp_path = XGB_DIR / \"optuna_best_params_setupB.json\"\n",
        "if bp_path.exists():\n",
        "    best_params = json.loads(bp_path.read_text(encoding=\"utf-8\"))\n",
        "else:\n",
        "    best_params = {\n",
        "        \"n_estimators\": 900,\n",
        "        \"max_depth\": 8,\n",
        "        \"learning_rate\": 0.06,\n",
        "        \"subsample\": 0.9,\n",
        "        \"colsample_bytree\": 0.9,\n",
        "        \"min_child_weight\": 3.0,\n",
        "        \"reg_alpha\": 1e-6,\n",
        "        \"reg_lambda\": 1.0,\n",
        "        \"gamma\": 0.0,\n",
        "        \"random_state\": SEED,\n",
        "        \"tree_method\": \"hist\",\n",
        "        \"n_jobs\": -1,\n",
        "    }\n",
        "\n",
        "# Fit finale con early stopping su VALID\n",
        "pipe_xgb_B, model_xgb_B, Xt, Xv = _fit_xgb_with_preproc(\n",
        "    best_params,\n",
        "    X_train[features_B], y_train,\n",
        "    X_valid[features_B], y_valid,\n",
        "    weights_B,\n",
        "    early_rounds=EARLY_STOP,\n",
        ")\n",
        "\n",
        "# Predizioni in scala naturale (kâ‚¬)\n",
        "ylog_val = pipe_xgb_B.named_steps[\"xgb\"].predict(\n",
        "    pipe_xgb_B.named_steps[\"prep\"].transform(X_valid[features_B])\n",
        ")\n",
        "ylog_tst = pipe_xgb_B.named_steps[\"xgb\"].predict(\n",
        "    pipe_xgb_B.named_steps[\"prep\"].transform(X_test[features_B])\n",
        ")\n",
        "pred_val_XGB = safe_expm1_scaled(ylog_val)\n",
        "pred_tst_XGB = safe_expm1_scaled(ylog_tst)\n",
        "\n",
        "m_val = {\n",
        "    \"MAE\": float(mean_absolute_error(y_val_true, pred_val_XGB)),\n",
        "    \"RMSE\": float(np.sqrt(mean_squared_error(y_val_true, pred_val_XGB))),\n",
        "    \"R2\":  float(r2_score(y_val_true, pred_val_XGB)),\n",
        "}\n",
        "m_tst = {\n",
        "    \"MAE\": float(mean_absolute_error(y_tst_true, pred_tst_XGB)),\n",
        "    \"RMSE\": float(np.sqrt(mean_squared_error(y_tst_true, pred_tst_XGB))),\n",
        "    \"R2\":  float(r2_score(y_tst_true, pred_tst_XGB)),\n",
        "}\n",
        "print(\"XGB_B VALID:\", m_val, \"| unit:\", UNIT_LABEL)\n",
        "print(\"XGB_B TEST :\", m_tst,  \"| unit:\", UNIT_LABEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eddb94ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Persistenza XGB + gain importance + update manifest (coerente con PROP_DIR)\n",
        "\n",
        "from pathlib import Path\n",
        "import joblib, json, hashlib\n",
        "\n",
        "xgb_path = XGB_DIR / \"xgb_setupB_champion.joblib\"\n",
        "\n",
        "# 1) Salva pipeline completa (prep + xgb)\n",
        "joblib.dump(pipe_xgb_B, xgb_path)\n",
        "print(\"Saved XGB model to:\", xgb_path)\n",
        "\n",
        "# 2) Gain importance (se disponibile)\n",
        "try:\n",
        "    booster = pipe_xgb_B.named_steps[\"xgb\"].get_booster()\n",
        "    f_gain = booster.get_score(importance_type=\"gain\")\n",
        "    (XGB_DIR / \"xgb_gain_importance.json\").write_text(json.dumps(f_gain, indent=2), encoding=\"utf-8\")\n",
        "    print(\"Saved:\", XGB_DIR / \"xgb_gain_importance.json\")\n",
        "except Exception as e:\n",
        "    print(\"Gain importance not available:\", e)\n",
        "\n",
        "# 3) Hash per tracciabilitÃ \n",
        "h = hashlib.sha256()\n",
        "with open(xgb_path, \"rb\") as _f:\n",
        "    for chunk in iter(lambda: _f.read(1024 * 1024), b\"\"):\n",
        "        h.update(chunk)\n",
        "xgb_sha = h.hexdigest()\n",
        "\n",
        "# 4) Aggiorna training manifest nella cartella PROPERTY\n",
        "manifest_path = PROP_DIR / \"training_manifest.json\"\n",
        "manifest = json.loads(manifest_path.read_text(encoding=\"utf-8\")) if manifest_path.exists() else {}\n",
        "\n",
        "# paths\n",
        "paths = manifest.setdefault(\"paths\", {})\n",
        "paths.update({\n",
        "    \"xgb_model\": str(xgb_path),\n",
        "    \"xgb_model_sha256\": xgb_sha,\n",
        "    \"xgb_gain_importance\": str(XGB_DIR / \"xgb_gain_importance.json\"),\n",
        "    \"xgb_optuna_best_params\": str(XGB_DIR / \"optuna_best_params_setupB.json\") if (XGB_DIR / \"optuna_best_params_setupB.json\").exists() else paths.get(\"xgb_optuna_best_params\"),\n",
        "    \"xgb_optuna_trials\": str(XGB_DIR / \"optuna_trials_setupB.parquet\") if (XGB_DIR / \"optuna_trials_setupB.parquet\").exists() else paths.get(\"xgb_optuna_trials\"),\n",
        "    # opzionale: punta il \"pipeline_path\" di default allo XGB champion\n",
        "    \"pipeline_path\": str(xgb_path),\n",
        "})\n",
        "\n",
        "# metrics\n",
        "metrics = manifest.setdefault(\"metrics\", {})\n",
        "metrics.update({\n",
        "    \"xgb_valid\": m_val,\n",
        "    \"xgb_test\":  m_tst,\n",
        "})\n",
        "\n",
        "# meta (utile per la UI/serving)\n",
        "manifest[\"model_meta\"] = {\n",
        "    \"value_model_name\": \"XGBRegressor\",\n",
        "    \"value_model_version\": \"v2\",          # etichetta â€œv2â€ per differenziarlo dallâ€™RF v1\n",
        "    \"setup\": \"B_conf_as_weight\",\n",
        "    \"early_stopping_rounds\": int(EARLY_STOP),\n",
        "    \"optuna_trials\": int(N_TRIALS),\n",
        "    \"seed\": int(SEED),\n",
        "}\n",
        "\n",
        "manifest_path.write_text(json.dumps(manifest, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
        "print(\"Updated manifest:\", manifest_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "279cc11f-e303-48de-92c6-d38c9ec38bf1",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Feature Importance (on champion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de9c6c54-9cfc-4c43-8cb6-721f5066387c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 07) Feature importances (RF champion) + salvataggio figure â€” ROBUST\n",
        "from __future__ import annotations\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "TOPN = 20  # barre nei grafici\n",
        "\n",
        "# -- helper: prendi per prima un'istanza esistente tra piÃ¹ nomi\n",
        "def _pick_first(*names):\n",
        "    for n in names:\n",
        "        if n in globals() and globals()[n] is not None:\n",
        "            return globals()[n]\n",
        "    return None\n",
        "\n",
        "# -- helper: estrai inner estimator, preproc e step finale da Pipeline o TTR\n",
        "def _extract_parts(model_like):\n",
        "    \"\"\"\n",
        "    Ritorna (inner_pipe, preproc, final_est, final_step_name, is_ttr)\n",
        "    - inner_pipe: Pipeline(\"prep\", \"...\"), l'oggetto che ha .named_steps\n",
        "    - preproc   : ColumnTransformer dello step \"prep\", se presente\n",
        "    - final_est : estimatore finale (es. RandomForestRegressor)\n",
        "    - final_step_name: nome dello step finale nella pipeline (es. \"rf\" o \"model\")\n",
        "    - is_ttr    : bool, True se model_like Ã¨ un TransformedTargetRegressor\n",
        "    \"\"\"\n",
        "    is_ttr = isinstance(model_like, TransformedTargetRegressor)\n",
        "    inner = model_like.regressor_ if is_ttr and hasattr(model_like, \"regressor_\") else \\\n",
        "            (model_like.regressor if is_ttr and hasattr(model_like, \"regressor\") else model_like)\n",
        "\n",
        "    if not isinstance(inner, Pipeline):\n",
        "        raise RuntimeError(\"Pipeline interna non trovata: atteso Pipeline con step 'prep' e modello finale.\")\n",
        "\n",
        "    # preproc\n",
        "    preproc = inner.named_steps.get(\"prep\", None)\n",
        "\n",
        "    # step finale & estimator\n",
        "    # preferisci chiavi comuni, altrimenti prendi l'ultimo step\n",
        "    final_step_name = None\n",
        "    for cand in (\"rf\", \"model\", \"xgb\"):\n",
        "        if cand in inner.named_steps:\n",
        "            final_step_name = cand\n",
        "            break\n",
        "    if final_step_name is None:\n",
        "        final_step_name = list(inner.named_steps.keys())[-1]\n",
        "    final_est = inner.named_steps[final_step_name]\n",
        "\n",
        "    return inner, preproc, final_est, final_step_name, is_ttr\n",
        "\n",
        "# -- helper: ricava lista feature nella GIUSTA sequenza del CT (cat poi num)\n",
        "def _feature_names_from_ct(preproc, fallback_cat, fallback_num):\n",
        "    \"\"\"\n",
        "    Con OrdinalEncoder per le categoriche â‡’ 1 col/feature.\n",
        "    \"\"\"\n",
        "    if preproc is None or not hasattr(preproc, \"transformers\"):\n",
        "        # fallback: usa liste note\n",
        "        return [*fallback_cat, *[c for c in fallback_num if c not in set(fallback_cat)]]\n",
        "\n",
        "    cat_cols_ct, num_cols_ct = [], []\n",
        "    for name, est, cols in preproc.transformers:\n",
        "        if name == \"cat\":\n",
        "            cat_cols_ct = list(cols) if isinstance(cols, (list, tuple, np.ndarray, pd.Index)) else list(fallback_cat)\n",
        "        elif name == \"num\":\n",
        "            num_cols_ct = list(cols) if isinstance(cols, (list, tuple, np.ndarray, pd.Index)) else list(fallback_num)\n",
        "\n",
        "    # sicurezza ordine/duplicati\n",
        "    seen = set()\n",
        "    ordered = []\n",
        "    for c in list(cat_cols_ct) + list(num_cols_ct):\n",
        "        if c not in seen:\n",
        "            seen.add(c); ordered.append(c)\n",
        "    return ordered\n",
        "\n",
        "# -- 1) Scegli il CHAMPION e la relativa pipeline\n",
        "champ = (champion if \"champion\" in globals() else \"A\")  # default\n",
        "if champ == \"A\":\n",
        "    chosen = _pick_first(\"ttr_A\", \"pipe_A\")\n",
        "    # liste colonne previste per A\n",
        "    chosen_cat = [c for c in (cat_cols if \"cat_cols\" in globals() else [])]\n",
        "    chosen_num = [c for c in (num_cols  if \"num_cols\"  in globals() else [])]\n",
        "else:\n",
        "    chosen = _pick_first(\"ttr_B\", \"pipe_B\")\n",
        "    # liste colonne previste per B (no confidence come feature)\n",
        "    chosen_cat = [c for c in (cat_cols    if \"cat_cols\"    in globals() else [])]\n",
        "    chosen_num = [c for c in (num_cols_B  if \"num_cols_B\"  in globals() else [])]\n",
        "\n",
        "if chosen is None:\n",
        "    raise RuntimeError(\"Nessuna pipeline/TransformedTargetRegressor disponibile per il champion selezionato.\")\n",
        "\n",
        "# -- 2) Estrai parti interne e ricava feature usate davvero\n",
        "inner_pipe, preproc, final_est, final_step, is_ttr = _extract_parts(chosen)\n",
        "feat_in_use = _feature_names_from_ct(preproc, chosen_cat, chosen_num)\n",
        "\n",
        "# -- 3) Prepara X_test allineato (crea colonne mancanti â†’ imputazione)\n",
        "def _ensure_cols(df_part: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
        "    dfp = df_part.copy()\n",
        "    missing = [c for c in cols if c not in dfp.columns]\n",
        "    for c in missing:\n",
        "        dfp[c] = np.nan\n",
        "    return dfp[cols]\n",
        "\n",
        "if \"df_test\" not in globals():\n",
        "    raise RuntimeError(\"df_test mancante.\")\n",
        "X_tst_use = _ensure_cols(df_test, feat_in_use)\n",
        "\n",
        "# -- 4) BUILT-IN importance (se disponibile) con nomi feature consistenti\n",
        "builtin_imp = None\n",
        "fi_raw = getattr(final_est, \"feature_importances_\", None)\n",
        "if fi_raw is not None:\n",
        "    try:\n",
        "        imp = np.asarray(fi_raw)\n",
        "        if imp.ndim == 1 and imp.size > 0:\n",
        "            # allinea lunghezze; se non combaciano, usa f0..f{n-1}\n",
        "            if len(feat_in_use) != imp.shape[0]:\n",
        "                feat_names = [f\"f{i}\" for i in range(int(imp.shape[0]))]\n",
        "            else:\n",
        "                feat_names = list(feat_in_use)\n",
        "            builtin_imp = (\n",
        "                pd.DataFrame({\"feature\": feat_names, \"importance\": imp.astype(float, copy=False)})\n",
        "                .sort_values(\"importance\", ascending=False)\n",
        "                .reset_index(drop=True)\n",
        "            )\n",
        "            builtin_imp.to_csv(MODEL_DIR / \"feature_importance_builtin.csv\", index=False)\n",
        "            builtin_imp.to_parquet(MODEL_DIR / \"feature_importance_builtin.parquet\", index=False)\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            _top = min(TOPN, len(builtin_imp))\n",
        "            ax = builtin_imp.head(_top).plot(kind=\"bar\", x=\"feature\", y=\"importance\", legend=False, rot=45)\n",
        "            ax.set_title(f\"RF Built-in Feature Importance (top {_top})\")\n",
        "            ax.set_ylabel(\"Importance\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(FIG_DIR / \"rf_feature_importance_builtin.png\", dpi=150)\n",
        "            plt.close()\n",
        "            print(\"Saved:\", FIG_DIR / \"rf_feature_importance_builtin.png\")\n",
        "        else:\n",
        "            print(\"âš ï¸ feature_importances_ presente ma vuoto/0-D â†’ salto built-in.\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Impossibile calcolare built-in importance: {e}\")\n",
        "else:\n",
        "    print(\"â„¹ï¸ feature_importances_ non disponibile sul modello finale â†’ salto built-in.\")\n",
        "\n",
        "# -- 5) PERMUTATION importance\n",
        "#    Scala y coerente:\n",
        "#      - se TTR: predict Ã¨ in scala NATURALE â†’ passiamo y naturale\n",
        "#      - se Pipeline pura (train su log1p): passiamo y in log\n",
        "y_perm = None\n",
        "if is_ttr:\n",
        "    y_perm = df_test[VALUATION_K].to_numpy(dtype=\"float64\", copy=False)\n",
        "else:\n",
        "    y_perm = np.log1p(df_test[VALUATION_K].to_numpy(dtype=\"float64\", copy=False))\n",
        "\n",
        "perm = permutation_importance(\n",
        "    estimator=chosen,              # TTR o Pipeline\n",
        "    X=X_tst_use,\n",
        "    y=y_perm,\n",
        "    n_repeats=8,\n",
        "    random_state=SEED if \"SEED\" in globals() else 42,\n",
        "    n_jobs=-1,\n",
        "    scoring=\"r2\",\n",
        ")\n",
        "# nomi: usa feat_in_use (o fallback f0..)\n",
        "feat_names_pi = list(feat_in_use) if len(feat_in_use) == perm.importances_mean.shape[0] \\\n",
        "                else [f\"f{i}\" for i in range(perm.importances_mean.shape[0])]\n",
        "\n",
        "perm_imp = (\n",
        "    pd.DataFrame({\n",
        "        \"feature\": feat_names_pi,\n",
        "        \"importance\": perm.importances_mean.astype(float, copy=False),\n",
        "        \"std\": perm.importances_std.astype(float, copy=False),\n",
        "    })\n",
        "    .sort_values(\"importance\", ascending=False)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "perm_imp.to_csv(MODEL_DIR / \"feature_importance_permutation.csv\", index=False)\n",
        "perm_imp.to_parquet(MODEL_DIR / \"feature_importance_permutation.parquet\", index=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "_top = min(TOPN, len(perm_imp))\n",
        "ax = perm_imp.head(_top).plot(kind=\"bar\", x=\"feature\", y=\"importance\", yerr=\"std\", legend=False, rot=45)\n",
        "ax.set_title(f\"Permutation Importance (top {_top})\")\n",
        "ax.set_ylabel(\"Importance (mean Î”RÂ²)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"rf_feature_importance_permutation.png\", dpi=150)\n",
        "plt.close()\n",
        "print(\"Saved:\", FIG_DIR / \"rf_feature_importance_permutation.png\")\n",
        "\n",
        "# -- 6) Anteprima (se disponibile)\n",
        "if builtin_imp is not None:\n",
        "    display(builtin_imp.head(12))\n",
        "display(perm_imp.head(12))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca8ec440",
      "metadata": {},
      "source": [
        "### Salvataggio figure importances & residuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a72e0ffa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 07bis (v2) â€” Importances + Residuali TEST con ricalcolo robusto delle predizioni\n",
        "\n",
        "from __future__ import annotations\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "\n",
        "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------------- helpers ----------------\n",
        "def _expm1_safe(z, cap: float = 12.0):\n",
        "    z = np.asarray(z, dtype=np.float64)\n",
        "    z = np.clip(z, -20.0, cap)  # cap log-pred per evitare overflow\n",
        "    out = np.expm1(z)\n",
        "    out[out < 0] = 0.0\n",
        "    return out\n",
        "\n",
        "def _ensure_cols(df_part: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
        "    dfp = df_part.copy()\n",
        "    miss = [c for c in cols if c not in dfp.columns]\n",
        "    if miss:\n",
        "        for c in miss: dfp[c] = np.nan\n",
        "        print(f\"â„¹ï¸ Aggiunte {len(miss)} colonne mancanti allo split TEST (imputate): {miss[:10]}\")\n",
        "    return dfp[cols]\n",
        "\n",
        "def _pick_first(*names):\n",
        "    for n in names:\n",
        "        if n in globals() and globals()[n] is not None:\n",
        "            return globals()[n]\n",
        "    return None\n",
        "\n",
        "def _predict_nat_safely(est, X_df: pd.DataFrame, cols: list[str], log_cap: float = 12.0):\n",
        "    X_use = _ensure_cols(X_df, cols)\n",
        "    if isinstance(est, TransformedTargetRegressor):\n",
        "        # prendi i LOG-pred dal regressor interno, poi expm1 safe\n",
        "        log_pred = est.regressor_.predict(X_use)\n",
        "        # diagnostica\n",
        "        q = np.nanpercentile(log_pred, [50, 90, 99, 99.9])\n",
        "        print(f\"log-pred percentiles (TTR inner): p50={q[0]:.3f}, p90={q[1]:.3f}, p99={q[2]:.3f}, p99.9={q[3]:.3f}\")\n",
        "        return _expm1_safe(log_pred, cap=log_cap)\n",
        "    else:\n",
        "        # pipeline RF/XGB allenata su log1p â†’ predict = log-pred\n",
        "        log_pred = est.predict(X_use)\n",
        "        q = np.nanpercentile(log_pred, [50, 90, 99, 99.9])\n",
        "        print(f\"log-pred percentiles (pipe): p50={q[0]:.3f}, p90={q[1]:.3f}, p99={q[2]:.3f}, p99.9={q[3]:.3f}\")\n",
        "        return _expm1_safe(log_pred, cap=log_cap)\n",
        "\n",
        "def _plot_topN(df_imp: pd.DataFrame, val_col: str, title: str, out_path, xlabel: str):\n",
        "    if df_imp is None or not isinstance(df_imp, pd.DataFrame) or df_imp.empty:\n",
        "        print(f\"â„¹ï¸ skip plot: {title} â€” dataframe vuoto\")\n",
        "        return\n",
        "    top = df_imp.head(20).iloc[::-1]\n",
        "    plt.figure(figsize=(9,6))\n",
        "    ax = top.plot(x=\"feature\", y=val_col, kind=\"barh\", legend=False)\n",
        "    ax.set_title(title); ax.set_xlabel(xlabel)\n",
        "    plt.tight_layout(); plt.savefig(out_path, dpi=150, bbox_inches=\"tight\"); plt.close()\n",
        "    print(\"Saved:\", out_path)\n",
        "\n",
        "# --------------- 1) Feature importances ---------------\n",
        "_plot_topN(\n",
        "    builtin_imp if \"builtin_imp\" in globals() else None,\n",
        "    \"importance\",\n",
        "    \"RF Feature Importance (built-in, top 20)\",\n",
        "    FIG_DIR / \"rf_feature_importance_builtin_top20.png\",\n",
        "    \"Importance\",\n",
        ")\n",
        "_plot_topN(\n",
        "    perm_imp if \"perm_imp\" in globals() else None,\n",
        "    \"importance\",\n",
        "    \"Permutation Importance (Î”RÂ², top 20)\",\n",
        "    FIG_DIR / \"rf_permutation_importance_top20.png\",\n",
        "    \"Importance (mean Î”RÂ²)\",\n",
        ")\n",
        "\n",
        "# --------------- 2) Residuali TEST ---------------\n",
        "if \"df_test\" not in globals():\n",
        "    raise RuntimeError(\"df_test non Ã¨ definito â€” impossibile calcolare i residuali.\")\n",
        "\n",
        "y_true_t = df_test[VALUATION_K].to_numpy(dtype=\"float64\", copy=False)\n",
        "\n",
        "# Ignora eventuale y_pred_t â€œsporcoâ€ in memoria\n",
        "if \"y_pred_t\" in globals():\n",
        "    del y_pred_t\n",
        "\n",
        "# Se esistono pred calcolate prima e coerenti le uso, altrimenti ricalcolo\n",
        "USE_CACHED = False\n",
        "if \"champion\" in globals():\n",
        "    if champion == \"A\" and \"pred_tst_A\" in globals() and isinstance(pred_tst_A, (np.ndarray, list)):\n",
        "        y_pred_t = np.asarray(pred_tst_A, dtype=np.float64); USE_CACHED = True\n",
        "    elif champion == \"B\" and \"pred_tst_B\" in globals() and isinstance(pred_tst_B, (np.ndarray, list)):\n",
        "        y_pred_t = np.asarray(pred_tst_B, dtype=np.float64); USE_CACHED = True\n",
        "\n",
        "if not USE_CACHED:\n",
        "    champ = champion if \"champion\" in globals() else \"A\"\n",
        "    # scegli estimator e lista colonne\n",
        "    if champ == \"A\":\n",
        "        est  = _pick_first(\"ttr_A\", \"pipe_A\", \"chosen_pipe\")\n",
        "        cols = _pick_first(\"features_A\") or ([*cat_cols, *num_cols] if \"cat_cols\" in globals() and \"num_cols\" in globals() else list(df_test.columns))\n",
        "    else:\n",
        "        est  = _pick_first(\"ttr_B\", \"pipe_B\", \"chosen_pipe\")\n",
        "        cols = _pick_first(\"features_B\") or ([*cat_cols, *num_cols_B] if \"cat_cols\" in globals() and \"num_cols_B\" in globals() else list(df_test.columns))\n",
        "    if est is None:\n",
        "        raise RuntimeError(\"Nessuna pipeline (ttr_*/pipe_*) disponibile per ricalcolare le predizioni TEST.\")\n",
        "\n",
        "    print(f\"Estimator scelto: {type(est).__name__} | n_cols={len(cols)} | esempi colonne: {cols[:10]}\")\n",
        "    missing = [c for c in cols if c not in df_test.columns]\n",
        "    if missing:\n",
        "        print(f\"âš ï¸ Mancano {len(missing)} colonne nel TEST (verranno imputate): {missing[:10]}\")\n",
        "\n",
        "    y_pred_t = _predict_nat_safely(est, df_test, cols, log_cap=float(TRAIN_CFG.get(\"log_cap_clip\", 12.0)))\n",
        "\n",
        "# Garantisci che sia 1D della lunghezza giusta\n",
        "y_pred_t = np.asarray(y_pred_t)\n",
        "if y_pred_t.ndim == 0:\n",
        "    # se Ã¨ uno scalare (es. NaN), replico per evitare IndexError e fallisco con messaggio chiaro dopo\n",
        "    y_pred_t = np.full_like(y_true_t, fill_value=np.nan, dtype=np.float64)\n",
        "\n",
        "if y_pred_t.shape[0] != y_true_t.shape[0]:\n",
        "    raise RuntimeError(f\"Dimension mismatch: y_pred_t={y_pred_t.shape}, y_true_t={y_true_t.shape}. \"\n",
        "                       \"Controlla la lista colonne usata per il predict.\")\n",
        "\n",
        "non_finite_mask = ~(np.isfinite(y_true_t) & np.isfinite(y_pred_t))\n",
        "n_bad = int(non_finite_mask.sum())\n",
        "if n_bad > 0:\n",
        "    bad_idx = np.where(non_finite_mask)[0][:10].tolist()\n",
        "    print(f\"âš ï¸ Righe non finite (y_true/y_pred): {n_bad} / {len(y_true_t)}. Esempi idx: {bad_idx}\")\n",
        "\n",
        "mask = np.isfinite(y_true_t) & np.isfinite(y_pred_t)\n",
        "y_true_t = y_true_t[mask]\n",
        "y_pred_t = y_pred_t[mask]\n",
        "\n",
        "if len(y_true_t) < max(10, int(0.3 * len(df_test))):\n",
        "    raise RuntimeError(\n",
        "        \"Predizioni/target TEST contengono troppi NaN/Inf anche dopo clipping.\\n\"\n",
        "        \"- Verifica che la pipeline scelta sia *quella allenata* (ttr_A/ttr_B o pipe_A/pipe_B).\\n\"\n",
        "        \"- Stai passando le *stesse colonne* usate in training (features_A/B)?\\n\"\n",
        "        \"- Guarda i percentili dei log-pred stampati sopra: se sono enormi, aumenta TRAIN_CFG.log_cap_clip o rivedi il modello.\"\n",
        "    )\n",
        "\n",
        "# Residuali e plot\n",
        "residuals = y_true_t - y_pred_t\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.hist(residuals, bins=60, density=True)\n",
        "plt.title(f\"Residuals (TEST, champion {champion if 'champion' in globals() else 'A'})\")\n",
        "plt.xlabel(\"y âˆ’ Å· (kâ‚¬)\"); plt.ylabel(\"Density\")\n",
        "plt.tight_layout()\n",
        "out_res = FIG_DIR / \"rf_residuals_test_hist.png\"\n",
        "plt.savefig(out_res, dpi=150, bbox_inches=\"tight\"); plt.close()\n",
        "print(\"Saved:\", out_res)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.scatter(y_pred_t, residuals, s=10, alpha=0.6)\n",
        "plt.axhline(0.0, linestyle=\"--\")\n",
        "plt.title(f\"Residuals vs Pred (TEST, champion {champion if 'champion' in globals() else 'A'})\")\n",
        "plt.xlabel(\"Å· (kâ‚¬)\"); plt.ylabel(\"y âˆ’ Å· (kâ‚¬)\")\n",
        "plt.tight_layout()\n",
        "out_sc = FIG_DIR / \"rf_residuals_vs_pred_test.png\"\n",
        "plt.savefig(out_sc, dpi=150, bbox_inches=\"tight\"); plt.close()\n",
        "print(\"Saved:\", out_sc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02f8b0c9-97a8-474b-aac3-e31c9cdb58c2",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Valuations for Segments (decils e location) & Predictions Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe014bd9-acd1-4be4-989a-16455c836842",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 08 (refactor) â€” Breakdown per decili/location + export predizioni TEST (robusto)\n",
        "\n",
        "from __future__ import annotations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "\n",
        "from notebooks.shared.common.constants import ASSET_ID, LOCATION, VALUATION_K\n",
        "\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- helper numerico ---\n",
        "LOG_CAP = float(TRAIN_CFG.get(\"log_cap_clip\", 12.0))  # limita i log-pred per evitare overflow\n",
        "\n",
        "def _expm1_safe(z, cap: float = LOG_CAP):\n",
        "    z = np.asarray(z, dtype=np.float64)\n",
        "    z = np.clip(z, -20.0, cap)\n",
        "    out = np.expm1(z)\n",
        "    out[out < 0] = 0.0\n",
        "    return out\n",
        "\n",
        "def _ensure_cols(df_part: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
        "    dfp = df_part.copy()\n",
        "    miss = [c for c in cols if c not in dfp.columns]\n",
        "    if miss:\n",
        "        for c in miss:\n",
        "            dfp[c] = np.nan\n",
        "        print(f\"â„¹ï¸ Aggiunte {len(miss)} colonne mancanti allo split TEST (imputate): {miss[:10]}\")\n",
        "    return dfp[cols]\n",
        "\n",
        "def _pick_first(*names):\n",
        "    for n in names:\n",
        "        if n in globals() and globals()[n] is not None:\n",
        "            return globals()[n]\n",
        "    return None\n",
        "\n",
        "def _predict_nat_test_from(est, X_df: pd.DataFrame, cols: list[str]) -> np.ndarray:\n",
        "    \"\"\"Predizioni in scala naturale (kâ‚¬) con auto-gestione TTR vs pipeline log1p.\"\"\"\n",
        "    X_use = _ensure_cols(X_df, cols)\n",
        "    if isinstance(est, TransformedTargetRegressor):\n",
        "        pred_nat = est.predict(X_use)                # giÃ  in kâ‚¬\n",
        "    else:\n",
        "        log_pred = est.predict(X_use)                # log1p(y)\n",
        "        pred_nat = _expm1_safe(log_pred)\n",
        "    return np.asarray(pred_nat, dtype=np.float64)\n",
        "\n",
        "# --- helper: groupby.apply compatibile con pandas nuovi/vecchi\n",
        "def _group_apply_safe(df: pd.DataFrame, key: str, func):\n",
        "    gb = df.groupby(key, observed=True)\n",
        "    try:\n",
        "        # pandas â‰¥ 2.2: esclude automaticamente la colonna di raggruppamento\n",
        "        out = gb.apply(func, include_groups=False)\n",
        "    except TypeError:\n",
        "        # pandas < 2.2: selezioniamo esplicitamente solo le colonne utili\n",
        "        out = gb[[\"y_true\", \"y_pred\"]].apply(func)\n",
        "    return out.reset_index()\n",
        "\n",
        "# --- prendi/ricostruisci y_pred_t in scala naturale (kâ‚¬) ---\n",
        "def _get_champion_pred_test() -> np.ndarray:\n",
        "    # usa cache se valida\n",
        "    if \"y_pred_t\" in globals() and isinstance(y_pred_t, (np.ndarray, list)) and len(y_pred_t) == len(df_test):\n",
        "        return np.asarray(y_pred_t, dtype=np.float64)\n",
        "\n",
        "    if \"champion\" not in globals():\n",
        "        raise RuntimeError(\"Variabile 'champion' mancante.\")\n",
        "\n",
        "    # pred giÃ  calcolate in kâ‚¬?\n",
        "    if champion == \"A\" and \"pred_tst_A\" in globals():\n",
        "        return np.asarray(pred_tst_A, dtype=np.float64)\n",
        "    if champion == \"B\" and \"pred_tst_B\" in globals():\n",
        "        return np.asarray(pred_tst_B, dtype=np.float64)\n",
        "\n",
        "    # altrimenti ricalcola\n",
        "    if champion == \"A\":\n",
        "        est  = _pick_first(\"ttr_A\", \"pipe_A\", \"chosen_pipe\")\n",
        "        cols = _pick_first(\"features_A\") or ([*cat_cols, *num_cols] if \"cat_cols\" in globals() and \"num_cols\" in globals() else list(df_test.columns))\n",
        "    else:\n",
        "        est  = _pick_first(\"ttr_B\", \"pipe_B\", \"chosen_pipe\")\n",
        "        cols = _pick_first(\"features_B\") or ([*cat_cols, *num_cols_B] if \"cat_cols\" in globals() and \"num_cols_B\" in globals() else list(df_test.columns))\n",
        "\n",
        "    if est is None:\n",
        "        raise RuntimeError(\"Nessuna pipeline disponibile per ricalcolare le predizioni TEST.\")\n",
        "\n",
        "    return _predict_nat_test_from(est, df_test, cols)\n",
        "\n",
        "# --- costruisci dataset metrico pulito ---\n",
        "y_true_t = df_test[VALUATION_K].to_numpy(dtype=np.float64, copy=False)\n",
        "y_pred_t = _get_champion_pred_test()\n",
        "\n",
        "# sanifica NaN/Inf\n",
        "mask = np.isfinite(y_true_t) & np.isfinite(y_pred_t)\n",
        "valid_n = int(mask.sum())\n",
        "if valid_n < max(30, int(0.3 * len(y_true_t))):\n",
        "    bad_idx = np.where(~mask)[0][:10].tolist()\n",
        "    raise RuntimeError(\n",
        "        f\"Predizioni TEST non finite: validi {valid_n}/{len(mask)}. \"\n",
        "        f\"Esempi idx problematici: {bad_idx}. Verifica allineamento feature e scala predizioni.\"\n",
        "    )\n",
        "\n",
        "yt = y_true_t[mask]\n",
        "yh = y_pred_t[mask]\n",
        "idx_valid = df_test.index[mask]\n",
        "\n",
        "cols_keep = [ASSET_ID] if ASSET_ID in df_test.columns else []\n",
        "if LOCATION in df_test.columns:\n",
        "    cols_keep.append(LOCATION)\n",
        "\n",
        "dfm = df_test.loc[idx_valid, cols_keep].copy()\n",
        "dfm[\"y_true\"] = yt\n",
        "dfm[\"y_pred\"] = yh\n",
        "\n",
        "# --- decili sul target naturale ---\n",
        "try:\n",
        "    dfm[\"decile\"] = pd.qcut(dfm[\"y_true\"], q=10, labels=False, duplicates=\"drop\")\n",
        "except Exception:\n",
        "    dfm[\"decile\"] = 0\n",
        "\n",
        "# --- funzioni metriche ---\n",
        "def _rmse(y_true, y_pred):\n",
        "    try:\n",
        "        return mean_squared_error(y_true, y_pred, squared=False)\n",
        "    except TypeError:\n",
        "        return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "def _agg_metrics(g: pd.DataFrame) -> pd.Series:\n",
        "    return pd.Series({\n",
        "        \"n\": int(len(g)),\n",
        "        \"MAE\": float(mean_absolute_error(g[\"y_true\"], g[\"y_pred\"])),\n",
        "        \"RMSE\": float(_rmse(g[\"y_true\"], g[\"y_pred\"])),\n",
        "        \"R2\": float(r2_score(g[\"y_true\"], g[\"y_pred\"])) if len(g) > 1 else np.nan,\n",
        "    })\n",
        "\n",
        "# --- Breakdown per decile ---\n",
        "dec_rep = _group_apply_safe(dfm, \"decile\", _agg_metrics)\n",
        "dec_rep.to_csv(MODEL_DIR / \"metrics_by_decile.csv\", index=False)\n",
        "dec_rep.to_parquet(MODEL_DIR / \"metrics_by_decile.parquet\", index=False)\n",
        "print(\"Saved:\", MODEL_DIR / \"metrics_by_decile.csv\")\n",
        "print(\"Saved:\", MODEL_DIR / \"metrics_by_decile.parquet\")\n",
        "\n",
        "# --- Breakdown per location ---\n",
        "if LOCATION in dfm.columns:\n",
        "    loc_rep = _group_apply_safe(dfm, LOCATION, _agg_metrics)\n",
        "else:\n",
        "    loc_rep = pd.DataFrame([{\n",
        "        LOCATION: \"NA\",\n",
        "        \"n\": int(len(dfm)),\n",
        "        \"MAE\": float(mean_absolute_error(dfm[\"y_true\"], dfm[\"y_pred\"])) if len(dfm) else np.nan,\n",
        "        \"RMSE\": float(_rmse(dfm[\"y_true\"], dfm[\"y_pred\"])) if len(dfm) else np.nan,\n",
        "        \"R2\": float(r2_score(dfm[\"y_true\"], dfm[\"y_pred\"])) if len(dfm) > 1 else np.nan,\n",
        "    }])\n",
        "\n",
        "loc_rep.to_csv(MODEL_DIR / \"metrics_by_location.csv\", index=False)\n",
        "loc_rep.to_parquet(MODEL_DIR / \"metrics_by_location.parquet\", index=False)\n",
        "print(\"Saved:\", MODEL_DIR / \"metrics_by_location.csv\")\n",
        "print(\"Saved:\", MODEL_DIR / \"metrics_by_location.parquet\")\n",
        "\n",
        "# --- export predizioni TEST ---\n",
        "pred_cols = []\n",
        "if ASSET_ID in dfm.columns: pred_cols.append(ASSET_ID)\n",
        "if LOCATION in dfm.columns: pred_cols.append(LOCATION)\n",
        "pred_df = dfm[pred_cols + [\"y_true\", \"y_pred\"]].rename(columns={\"y_true\": VALUATION_K})\n",
        "\n",
        "pred_df.to_parquet(MODEL_DIR / \"predictions_test.parquet\", index=False)\n",
        "pred_df.to_csv(MODEL_DIR / \"predictions_test.csv\", index=False, encoding=\"utf-8\")\n",
        "print(\"Saved:\", MODEL_DIR / \"predictions_test.parquet\")\n",
        "print(\"Saved:\", MODEL_DIR / \"predictions_test.csv\")\n",
        "\n",
        "display(pred_df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bf1f075-89a4-4e49-a30c-2031f17ab419",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Model Persistence & Manifest Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "701beb69",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 08.9) Deriva chosen_pipe / chosen_cols dal champion (robusto)\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "def _cols_from_ct(prep: ColumnTransformer) -> list[str]:\n",
        "    \"\"\"Estrae i nomi colonna dichiarati nel ColumnTransformer (solo selector per nome).\"\"\"\n",
        "    cols = []\n",
        "    if hasattr(prep, \"transformers\"):\n",
        "        for name, trans, sel in prep.transformers:\n",
        "            if name == \"remainder\":\n",
        "                continue\n",
        "            if isinstance(sel, (list, tuple)):\n",
        "                cols.extend([c for c in sel if isinstance(c, str)])\n",
        "    # dedup preservando ordine\n",
        "    return list(dict.fromkeys(cols))\n",
        "\n",
        "def _pipe_cols(pipe: Pipeline) -> list[str]:\n",
        "    if \"prep\" in pipe.named_steps and isinstance(pipe.named_steps[\"prep\"], ColumnTransformer):\n",
        "        return _cols_from_ct(pipe.named_steps[\"prep\"])\n",
        "    # fallback: prova a trovare il primo ColumnTransformer nella pipeline\n",
        "    for _, step in pipe.steps:\n",
        "        if isinstance(step, ColumnTransformer):\n",
        "            return _cols_from_ct(step)\n",
        "    return []\n",
        "\n",
        "# 1) Champion: se non definito, scegli in base al VALID (MAE)\n",
        "if \"champion\" not in globals():\n",
        "    if \"mA_val\" in globals() and \"mB_val\" in globals():\n",
        "        champion = \"A\" if mA_val[\"MAE\"] <= mB_val[\"MAE\"] else \"B\"\n",
        "    else:\n",
        "        champion = \"A\"  # fallback conservativo\n",
        "\n",
        "# 2) Scegli la pipeline e le colonne\n",
        "chosen_pipe = None\n",
        "chosen_cols = None\n",
        "\n",
        "if champion == \"A\" and \"pipe_A\" in globals():\n",
        "    chosen_pipe = pipe_A\n",
        "    if \"features_A\" in globals():\n",
        "        chosen_cols = list(dict.fromkeys(features_A))\n",
        "    elif \"cat_cols\" in globals() and \"num_cols\" in globals():\n",
        "        chosen_cols = list(dict.fromkeys([*cat_cols, *num_cols]))\n",
        "elif champion == \"B\" and \"pipe_B\" in globals():\n",
        "    chosen_pipe = pipe_B\n",
        "    if \"features_B\" in globals():\n",
        "        chosen_cols = list(dict.fromkeys(features_B))\n",
        "    elif \"cat_cols\" in globals() and \"num_cols_B\" in globals():\n",
        "        chosen_cols = list(dict.fromkeys([*cat_cols, *num_cols_B]))\n",
        "\n",
        "# 3) Fallback: infila dal ColumnTransformer se le liste sopra non esistono\n",
        "if (chosen_cols is None or len(chosen_cols) == 0) and \"chosen_pipe\" in globals() and chosen_pipe is not None:\n",
        "    chosen_cols = _pipe_cols(chosen_pipe)\n",
        "\n",
        "# 4) Ultime cinture: tieni solo colonne realmente presenti nel train (se disponibile)\n",
        "if \"df_train\" in globals() and isinstance(df_train, pd.DataFrame):\n",
        "    chosen_cols = [c for c in chosen_cols if c in df_train.columns]\n",
        "\n",
        "# 5) Sanity\n",
        "if chosen_pipe is None or not isinstance(chosen_pipe, Pipeline):\n",
        "    raise RuntimeError(\"Impossibile determinare chosen_pipe (pipe_A/pipe_B mancanti).\")\n",
        "if not chosen_cols:\n",
        "    raise RuntimeError(\"Impossibile derivare chosen_cols: nessuna lista features trovata nel contesto/pipeline.\")\n",
        "\n",
        "print(f\"Champion: {champion} | n_cols={len(chosen_cols)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a90328c1-1e44-4551-a9e9-bb18e4a8817c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 09) Persistenza modello (RF champion) + Serving v2 â€” outputs/modeling/property\n",
        "from __future__ import annotations\n",
        "\n",
        "import json, os, hashlib\n",
        "from datetime import datetime, timezone\n",
        "from pathlib import Path\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
        "from sklearn.base import clone\n",
        "\n",
        "# === trasformatori SERVING importabili (niente FunctionTransformer locali) ===\n",
        "from notebooks.shared.common.serving_transformers import GeoCanonizer, PriorsGuard\n",
        "# Deriver: prova quello â€œufficialeâ€, altrimenti un fallback classe-based dal modulo serving\n",
        "try:\n",
        "    from notebooks.shared.common.transformers import PropertyDerivedFeatures as _Deriver  # type: ignore\n",
        "except Exception:\n",
        "    try:\n",
        "        from notebooks.shared.common.serving_transformers import BasicDeriver as _Deriver  # <- aggiungilo se non câ€™Ã¨\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\n",
        "            \"Nessun deriver importabile trovato. Installa/aggiungi \"\n",
        "            \"`PropertyDerivedFeatures` o `BasicDeriver` nel modulo serving_transformers.\"\n",
        "        ) from e\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Cartelle\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "BASE_OUT   = Path(\"outputs\")\n",
        "MODEL_DIR  = BASE_OUT / \"modeling\"\n",
        "FIG_DIR    = MODEL_DIR / \"figures\"\n",
        "ART_DIR    = MODEL_DIR / \"artifacts\"\n",
        "PROP_DIR   = MODEL_DIR / \"property\"\n",
        "for d in (MODEL_DIR, FIG_DIR, ART_DIR, PROP_DIR):\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Sanity sul champion\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from sklearn.compose import ColumnTransformer as _CT  # solo per isinstance check tipizzato\n",
        "\n",
        "if \"chosen_pipe\" not in globals() or not isinstance(chosen_pipe, Pipeline):\n",
        "    raise RuntimeError(\"chosen_pipe non definito o non Ã¨ una sklearn Pipeline.\")\n",
        "\n",
        "# Ricava chosen_cols dal champion se mancano\n",
        "if \"chosen_cols\" not in globals() or not chosen_cols:\n",
        "    if \"champion\" in globals() and champion in (\"A\", \"B\"):\n",
        "        if champion == \"A\":\n",
        "            chosen_cols = list(dict.fromkeys([*(cat_cols or []), *(num_cols or [])]))\n",
        "        else:\n",
        "            chosen_cols = list(dict.fromkeys([*(cat_cols or []), *(num_cols_B or [])]))\n",
        "    else:\n",
        "        raise RuntimeError(\"chosen_cols mancante e champion non determinabile.\")\n",
        "\n",
        "if \"cat_cols\" not in globals():\n",
        "    cat_cols = []\n",
        "\n",
        "VALUATION_K = \"valuation_k\"\n",
        "SEED = int(globals().get(\"SEED\", 42))\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Helper\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def _sha256_file(p: Path, chunk: int = 1 << 20) -> str:\n",
        "    h = hashlib.sha256()\n",
        "    with p.open(\"rb\") as f:\n",
        "        for ch in iter(lambda: f.read(chunk), b\"\"):\n",
        "            h.update(ch)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def _find_preproc(pipe: Pipeline) -> ColumnTransformer:\n",
        "    if \"prep\" in pipe.named_steps and isinstance(pipe.named_steps[\"prep\"], _CT):\n",
        "        return pipe.named_steps[\"prep\"]\n",
        "    for _, step in pipe.steps:\n",
        "        if isinstance(step, _CT):\n",
        "            return step\n",
        "    raise RuntimeError(\"Nessuno step ColumnTransformer trovato nella pipeline (prep).\")\n",
        "\n",
        "def _find_regressor(pipe: Pipeline):\n",
        "    return pipe.steps[-1][1]\n",
        "\n",
        "def _rel_to_prop(p: Path | None) -> str | None:\n",
        "    if not p: return None\n",
        "    try: return os.path.relpath(p, PROP_DIR).replace(\"\\\\\", \"/\")\n",
        "    except Exception: return p.as_posix()\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Priors & derivate (coerenti con il training)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "DERIVED_FEATURES = [\n",
        "    \"log_size_m2\",\"sqm_per_room\",\"baths_per_100sqm\",\n",
        "    \"elev_x_floor\",\"no_elev_high_floor\",\"rooms_per_100sqm\",\n",
        "    \"city_zone_prior\",\"region_index_prior\",\n",
        "]\n",
        "DERIVED_SET = set(DERIVED_FEATURES)\n",
        "\n",
        "# Config per priors\n",
        "from notebooks.shared.common.config import ASSET_CONFIG\n",
        "_PROP = ASSET_CONFIG[\"property\"]\n",
        "CITY_BASE = {str(c).lower(): {str(z).lower(): float(v) for z, v in zv.items()}\n",
        "             for c, zv in (_PROP.get(\"city_base_prices\") or {}).items()}\n",
        "REGION_INDEX = {str(k).lower(): float(v) for k, v in (_PROP.get(\"region_index\") or {\n",
        "    \"north\": 1.05, \"center\": 1.00, \"south\": 0.92\n",
        "}).items()}\n",
        "\n",
        "# Fallback robusti per city_zone_prior\n",
        "_ZONE_KEYS = set(z for d in CITY_BASE.values() for z in d.keys())\n",
        "_ZONE_MED = {z: float(np.nanmedian([d.get(z, np.nan) for d in CITY_BASE.values()]))\n",
        "             for z in _ZONE_KEYS} if CITY_BASE else {}\n",
        "_GLOBAL_CITYZONE_MED = float(np.nanmedian([v for d in CITY_BASE.values() for v in d.values()])) if CITY_BASE else 0.0\n",
        "\n",
        "# Deriver istanza (classe importabile â†’ pickle-safe) con adattamento alla firma\n",
        "def _build_deriver(Cls):\n",
        "    \"\"\"\n",
        "    Instanzia il deriver passando solo gli argomenti supportati dalla sua __init__.\n",
        "    Compatibile con:\n",
        "      - PropertyDerivedFeatures(city_base=..., region_index=..., [flag...])\n",
        "      - BasicDeriver() o BasicDeriver(city_base=..., region_index=...)\n",
        "    \"\"\"\n",
        "    candidate_kwargs = dict(\n",
        "        # flag possibili (se non esistono verranno filtrati)\n",
        "        make_log_size=True,\n",
        "        make_sqm_per_room=True,\n",
        "        make_baths_per_100sqm=True,\n",
        "        make_elev_x_floor=True,\n",
        "        make_no_elev_penalty=True,\n",
        "        make_city_zone_prior=True,\n",
        "        make_region_macro_prior=True,\n",
        "        # mapping prior\n",
        "        city_base=CITY_BASE,\n",
        "        region_index=REGION_INDEX,\n",
        "        # qualcuno potrebbe accettare anche queste:\n",
        "        zone_medians=_ZONE_MED,\n",
        "        global_cityzone_median=_GLOBAL_CITYZONE_MED,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        sig = inspect.signature(Cls.__init__)\n",
        "        allowed = {k for k in sig.parameters.keys() if k != \"self\"}\n",
        "        kwargs = {k: v for k, v in candidate_kwargs.items() if k in allowed}\n",
        "        return Cls(**kwargs)  # 1Â° tentativo: con tutti i parametri ammessi\n",
        "    except TypeError:\n",
        "        # fallback ultra-conservativo\n",
        "        base_kwargs = {}\n",
        "        for k in (\"city_base\", \"region_index\"):\n",
        "            if k in allowed:\n",
        "                base_kwargs[k] = candidate_kwargs[k]\n",
        "        return Cls(**base_kwargs) if base_kwargs else Cls()\n",
        "\n",
        "feature_deriver = _build_deriver(_Deriver)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Loader X/y: usa solo RAW + basi minime per derivate\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def _load_training_Xy(cols_raw: list[str]) -> tuple[pd.DataFrame, np.ndarray, np.ndarray | None]:\n",
        "    \"\"\"\n",
        "    Ritorna X (DF), y (kâ‚¬), w (o None). Aggiunge sempre le basi necessarie\n",
        "    alle derivate cosÃ¬ gli step di serving possono lavorare correttamente.\n",
        "    \"\"\"\n",
        "    base_needed = [\"size_m2\",\"rooms\",\"bathrooms\",\"floor\",\"city\",\"zone\",\"region\",\"location\"]\n",
        "    cols_plus_base = list(dict.fromkeys([*cols_raw, *base_needed]))\n",
        "\n",
        "    def _prepare(df: pd.DataFrame):\n",
        "        X = df.reindex(columns=cols_plus_base)\n",
        "        # riempi basi ragionevoli per evitare colonne tutte NaN\n",
        "        if \"city\" in X.columns and \"location\" in df.columns:\n",
        "            X[\"city\"] = X[\"city\"].fillna(df[\"location\"])\n",
        "        if \"zone\" in X.columns:\n",
        "            X[\"zone\"] = X[\"zone\"].fillna(\"semi_center\")\n",
        "        if \"region\" in X.columns:\n",
        "            X[\"region\"] = X[\"region\"].fillna(\"center\")\n",
        "\n",
        "        y = pd.to_numeric(df[VALUATION_K], errors=\"coerce\").to_numpy(dtype=\"float64\")\n",
        "        w_out = None\n",
        "        if \"sample_weight\" in df.columns:\n",
        "            w_out = pd.to_numeric(df[\"sample_weight\"], errors=\"coerce\").to_numpy(dtype=\"float64\")\n",
        "        return X, y, w_out\n",
        "\n",
        "    if \"df_train\" in globals() and isinstance(df_train, pd.DataFrame) and VALUATION_K in df_train.columns:\n",
        "        return _prepare(df_train)\n",
        "\n",
        "    candidates = []\n",
        "    if \"data_path\" in globals():\n",
        "        try: candidates.append(Path(str(data_path)))\n",
        "        except Exception: pass\n",
        "    candidates += [BASE_OUT / \"dataset_generated.parquet\", BASE_OUT / \"dataset_generated.csv\"]\n",
        "\n",
        "    for p in candidates:\n",
        "        if p and p.exists():\n",
        "            df = pd.read_parquet(p) if p.suffix.lower() in {\".parquet\", \".pq\"} else pd.read_csv(p)\n",
        "            if VALUATION_K not in df.columns:\n",
        "                continue\n",
        "            return _prepare(df)\n",
        "\n",
        "    raise RuntimeError(\"Impossibile ricostruire X/y (e pesi) per il fit della serving pipeline.\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Salvataggio legacy per retro-compat\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "legacy_model_path = ART_DIR / f\"rf_champion_{globals().get('champion','A')}.joblib\"\n",
        "joblib.dump(chosen_pipe, legacy_model_path)\n",
        "legacy_sha = _sha256_file(legacy_model_path)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Serving pipeline v2 (GeoCanonizer â†’ Deriver â†’ PriorsGuard â†’ prep â†’ TTR)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "preproc  = clone(_find_preproc(chosen_pipe))\n",
        "base_reg = _find_regressor(chosen_pipe)\n",
        "RegCls   = base_reg.__class__\n",
        "reg_params = base_reg.get_params()\n",
        "\n",
        "serving_pipe_v2 = Pipeline(steps=[\n",
        "    (\"canon_geo\",   GeoCanonizer()),\n",
        "    (\"derive\",      feature_deriver),  # classe importabile\n",
        "    (\"priors_guard\",PriorsGuard(\n",
        "        city_base=CITY_BASE,\n",
        "        region_index=REGION_INDEX,\n",
        "        zone_medians=_ZONE_MED,\n",
        "        global_cityzone_median=_GLOBAL_CITYZONE_MED,\n",
        "    )),\n",
        "    (\"prep\",        preproc),\n",
        "    (\"ttr\",         TransformedTargetRegressor(\n",
        "        regressor=RegCls(**reg_params),\n",
        "        func=np.log1p, inverse_func=np.expm1, check_inverse=False\n",
        "    )),\n",
        "])\n",
        "\n",
        "# Fit: SOLO colonne raw (le derivate le crea la pipeline)\n",
        "chosen_cols_raw = [c for c in chosen_cols if c not in DERIVED_SET]\n",
        "X_fit, y_fit, w_fit = _load_training_Xy(chosen_cols_raw)\n",
        "\n",
        "fit_kwargs = {}\n",
        "if w_fit is not None and np.isfinite(w_fit).any():\n",
        "    fit_kwargs = {\"ttr__sample_weight\": w_fit}\n",
        "\n",
        "# diagnostica soft prima del fit\n",
        "# diagnostica soft prima del fit (no fit richiesto sugli step stateless)\n",
        "X_probe = X_fit.iloc[:128].copy()\n",
        "X_probe = serving_pipe_v2.named_steps[\"canon_geo\"].transform(X_probe)\n",
        "X_probe = serving_pipe_v2.named_steps[\"derive\"].transform(X_probe)\n",
        "X_probe = serving_pipe_v2.named_steps[\"priors_guard\"].transform(X_probe)\n",
        "\n",
        "print(\"city_zone_prior NaN (sample 128):\",\n",
        "      int(pd.to_numeric(X_probe.get(\"city_zone_prior\"), errors=\"coerce\").isna().sum()),\n",
        "      \"/\", len(X_probe))\n",
        "\n",
        "serving_pipe_v2.fit(X_fit, y_fit, **fit_kwargs)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Salva pipeline + meta\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "pipe_path = PROP_DIR / \"value_regressor_v2.joblib\"\n",
        "meta_path = PROP_DIR / \"value_regressor_v2_meta.json\"\n",
        "joblib.dump(serving_pipe_v2, pipe_path)\n",
        "\n",
        "model_meta = {\n",
        "    \"asset_type\": \"property\",\n",
        "    \"task\": \"value_regressor\",\n",
        "    \"model_version\": \"v2\",\n",
        "    \"model_class\": RegCls.__name__,\n",
        "    \"trained_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),\n",
        "    \"n_features\": int(len(chosen_cols_raw)),\n",
        "    \"features_categorical\": list(cat_cols),\n",
        "    \"features_numeric\": [c for c in chosen_cols_raw if c not in set(cat_cols)],\n",
        "    \"target_name\": VALUATION_K,\n",
        "    \"unit\": \"k_eur\",\n",
        "    \"pipeline_sha256\": _sha256_file(pipe_path),\n",
        "}\n",
        "meta_path.write_text(json.dumps(model_meta, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Manifest v2\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "manifest_path = PROP_DIR / \"training_manifest.json\"\n",
        "try:\n",
        "    existing = json.loads(manifest_path.read_text(encoding=\"utf-8\")) if manifest_path.exists() else {}\n",
        "except Exception:\n",
        "    existing = {}\n",
        "\n",
        "paths = dict(existing.get(\"paths\", {}))\n",
        "paths.update({\n",
        "    \"pipeline_path\": pipe_path.name,\n",
        "    \"meta_path\": meta_path.name,\n",
        "    \"dataset\": _rel_to_prop(Path(str(data_path))) if \"data_path\" in globals() else paths.get(\"dataset\"),\n",
        "    \"rf_model\": _rel_to_prop(legacy_model_path),\n",
        "    \"rf_model_sha256\": legacy_sha,\n",
        "    \"feature_importance_builtin_csv\": _rel_to_prop(MODEL_DIR / \"feature_importance_builtin.csv\"),\n",
        "    \"feature_importance_builtin_parquet\": _rel_to_prop(MODEL_DIR / \"feature_importance_builtin.parquet\"),\n",
        "    \"feature_importance_permutation_csv\": _rel_to_prop(MODEL_DIR / \"feature_importance_permutation.csv\"),\n",
        "    \"feature_importance_permutation_parquet\": _rel_to_prop(MODEL_DIR / \"feature_importance_permutation.parquet\"),\n",
        "    \"metrics_by_decile_csv\": _rel_to_prop(MODEL_DIR / \"metrics_by_decile.csv\"),\n",
        "    \"metrics_by_decile_parquet\": _rel_to_prop(MODEL_DIR / \"metrics_by_decile.parquet\"),\n",
        "    \"metrics_by_location_csv\": _rel_to_prop(MODEL_DIR / \"metrics_by_location.csv\"),\n",
        "    \"metrics_by_location_parquet\": _rel_to_prop(MODEL_DIR / \"metrics_by_location.parquet\"),\n",
        "    \"predictions_test_parquet\": _rel_to_prop(MODEL_DIR / \"predictions_test.parquet\"),\n",
        "    \"predictions_test_csv\": _rel_to_prop(MODEL_DIR / \"predictions_test.csv\"),\n",
        "})\n",
        "\n",
        "metrics = dict(existing.get(\"metrics\", {}))\n",
        "if {\"mA_val\",\"mA_tst\",\"mB_val\",\"mB_tst\"}.issubset(globals()):\n",
        "    champ = globals().get(\"champion\", \"A\")\n",
        "    metrics.update({\n",
        "        \"rf_valid\": globals()[\"mA_val\"] if champ == \"A\" else globals()[\"mB_val\"],\n",
        "        \"rf_test\":  globals()[\"mA_tst\"] if champ == \"A\" else globals()[\"mB_tst\"],\n",
        "    })\n",
        "\n",
        "feature_config = {\n",
        "    \"categorical\": list(cat_cols),\n",
        "    \"numeric\": [c for c in chosen_cols_raw if c not in set(cat_cols)],\n",
        "    \"excluded\": sorted(list(globals().get(\"exclude\", []))) if \"exclude\" in globals() else [],\n",
        "}\n",
        "\n",
        "manifest_new = {\n",
        "    \"generated_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),\n",
        "    \"schema_version\": \"v2\",\n",
        "    \"asset_type\": \"property\",\n",
        "    \"task\": \"value_regressor\",\n",
        "    \"seed\": SEED,\n",
        "    \"paths\": paths,\n",
        "    \"model_meta\": {\"model_version\": \"v2\", \"model_class\": RegCls.__name__},\n",
        "    \"metrics\": metrics,\n",
        "    \"feature_config\": feature_config,\n",
        "    \"expected_features\": {\n",
        "        \"categorical\": feature_config[\"categorical\"],\n",
        "        \"numeric\": feature_config[\"numeric\"],\n",
        "        \"derived\": DERIVED_FEATURES,\n",
        "    },\n",
        "}\n",
        "\n",
        "merged = dict(existing)\n",
        "for k, v in manifest_new.items():\n",
        "    if v is not None:\n",
        "        merged[k] = v\n",
        "\n",
        "manifest_path.write_text(json.dumps(merged, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
        "\n",
        "print(\"âœ… Saved legacy (artifacts):\", ART_DIR / legacy_model_path.name)\n",
        "print(\"âœ… Saved serving v2 pipeline:\", pipe_path)\n",
        "print(\"âœ… Saved meta:\", meta_path)\n",
        "print(\"âœ… Saved manifest:\", manifest_path)\n",
        "print(\"Derivate in serving: \", \", \".join(DERIVED_FEATURES))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a09e003",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 11) Post-training drift check (location) â€” firma: compute_location_drift(df, target_weights, tolerance)\n",
        "from __future__ import annotations\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Import robusto della funzione (firma: df, target_weights, tolerance)\n",
        "try:\n",
        "    from notebooks.shared.n03_train_model.metrics import compute_location_drift  # <- richiede (df, target_weights, tolerance)\n",
        "    _HAS_CLD = True\n",
        "except Exception:\n",
        "    _HAS_CLD = False\n",
        "\n",
        "# 2) Contesto: path & config (giÃ  definiti a inizio notebook)\n",
        "manifest_path = PROP_DIR / \"training_manifest.json\"  # es.: notebooks/outputs/modeling/property/training_manifest.json\n",
        "try:\n",
        "    manifest = json.loads(manifest_path.read_text(encoding=\"utf-8\")) if manifest_path.exists() else {}\n",
        "except Exception:\n",
        "    manifest = {}\n",
        "\n",
        "TRAIN_CFG = globals().get(\"TRAIN_CFG\", {}) or {}\n",
        "LOCATION  = globals().get(\"LOCATION\", \"location\")\n",
        "TOL       = float(TRAIN_CFG.get(\"drift_tolerance\", 0.05))  # default 5%\n",
        "\n",
        "# 3) Helpers -----------------------------------------------------------------\n",
        "def _norm_weights(d: dict) -> dict[str, float]:\n",
        "    \"\"\"Normalizza pesi (>=0) per sommare a 1.0; ignora chiavi con pesi negativi/non numerici.\"\"\"\n",
        "    clean = {str(k): float(v) for k, v in d.items() if pd.api.types.is_number(v) and float(v) >= 0.0}\n",
        "    s = float(sum(clean.values()))\n",
        "    if s <= 0:\n",
        "        return {k: 0.0 for k in clean}\n",
        "    return {k: v / s for k, v in clean.items()}\n",
        "\n",
        "def _empirical_weights(df_like: pd.DataFrame, col: str) -> dict[str, float]:\n",
        "    if not isinstance(df_like, pd.DataFrame) or col not in df_like.columns:\n",
        "        return {}\n",
        "    vc = df_like[col].dropna().astype(str).value_counts(normalize=True)\n",
        "    return {k: float(v) for k, v in vc.items()}\n",
        "\n",
        "def _fallback_drift(df_like: pd.DataFrame, target_w: dict[str, float]) -> dict:\n",
        "    \"\"\"Fallback semplice: differenze assolute + ratio + TVD/JSD.\"\"\"\n",
        "    emp = _empirical_weights(df_like, LOCATION)\n",
        "    keys = sorted(set(emp) | set(target_w))\n",
        "    p = np.array([emp.get(k, 0.0) for k in keys], dtype=float)\n",
        "    q = np.array([target_w.get(k, 0.0) for k in keys], dtype=float)\n",
        "    eps = 1e-12\n",
        "    p = np.clip(p, eps, 1.0); q = np.clip(q, eps, 1.0)\n",
        "    p /= p.sum(); q /= q.sum()\n",
        "    m = 0.5 * (p + q)\n",
        "    jsd = float(0.5 * (np.sum(p * (np.log(p) - np.log(m))) + np.sum(q * (np.log(q) - np.log(m)))))\n",
        "    tvd = float(0.5 * np.abs(p - q).sum())\n",
        "    report = {\n",
        "        \"method\": \"fallback_jsd_tvd\",\n",
        "        \"JSD\": jsd,\n",
        "        \"TVD\": tvd,\n",
        "        \"per_location\": {}\n",
        "    }\n",
        "    for k in keys:\n",
        "        emp_k = emp.get(k, 0.0); tgt_k = target_w.get(k, 0.0)\n",
        "        diff = emp_k - tgt_k\n",
        "        report[\"per_location\"][k] = {\n",
        "            \"target_weight\": tgt_k,\n",
        "            \"empirical_weight\": emp_k,\n",
        "            \"difference\": diff,\n",
        "            \"drifted\": bool(abs(diff) > TOL),\n",
        "            \"ratio\": (emp_k / tgt_k) if tgt_k > 0 else float(\"inf\")\n",
        "        }\n",
        "    return report\n",
        "\n",
        "# 4) Scegli scenario: baseline da config OPPURE train vs test ---------------\n",
        "baseline_cfg = (TRAIN_CFG.get(\"expected_profile\", {}) or {}).get(\"location_distribution\", {}) or None\n",
        "\n",
        "try:\n",
        "    if baseline_cfg:\n",
        "        # Scenario A: confronto dataset complessivo vs baseline attesa\n",
        "        if \"df\" not in globals():\n",
        "            raise RuntimeError(\"df non disponibile per il drift vs baseline.\")\n",
        "        target_w = _norm_weights(baseline_cfg)\n",
        "        if _HAS_CLD:\n",
        "            drift_result = compute_location_drift(df, target_w, TOL)\n",
        "        else:\n",
        "            drift_result = _fallback_drift(df, target_w)\n",
        "        out_path = MODEL_DIR / \"location_drift_vs_expected.json\"\n",
        "        out_key  = \"location_drift_vs_expected\"\n",
        "    else:\n",
        "        # Scenario B: train vs test â€” usa la distribuzione del TEST come target_weights\n",
        "        if \"df_train\" not in globals() or \"df_test\" not in globals():\n",
        "            raise RuntimeError(\"df_train/df_test non disponibili per il drift train vs test.\")\n",
        "        tgt = _empirical_weights(df_test, LOCATION)\n",
        "        target_w = _norm_weights(tgt)\n",
        "        if _HAS_CLD:\n",
        "            drift_result = compute_location_drift(df_train, target_w, TOL)\n",
        "        else:\n",
        "            drift_result = _fallback_drift(df_train, target_w)\n",
        "        out_path = MODEL_DIR / \"location_drift_train_vs_test.json\"\n",
        "        out_key  = \"location_drift_train_vs_test\"\n",
        "\n",
        "except Exception as e:\n",
        "    # Fallback totale (se qualcosa va storto nelle ramificazioni sopra)\n",
        "    try:\n",
        "        logger.info(\"compute_location_drift non disponibile/errore (%s). Uso fallback semplice.\", e)\n",
        "    except Exception:\n",
        "        print(f\"compute_location_drift non disponibile/errore ({e}). Uso fallback semplice.\")\n",
        "    if baseline_cfg and \"df\" in globals():\n",
        "        drift_result = _fallback_drift(df, _norm_weights(baseline_cfg))\n",
        "        out_path = MODEL_DIR / \"location_drift_vs_expected.json\"\n",
        "        out_key  = \"location_drift_vs_expected\"\n",
        "    elif \"df_train\" in globals() and \"df_test\" in globals():\n",
        "        drift_result = _fallback_drift(df_train, _norm_weights(_empirical_weights(df_test, LOCATION)))\n",
        "        out_path = MODEL_DIR / \"location_drift_train_vs_test.json\"\n",
        "        out_key  = \"location_drift_train_vs_test\"\n",
        "    else:\n",
        "        raise\n",
        "\n",
        "# 5) Persistenza output + aggiornamento manifest ----------------------------\n",
        "out_path.write_text(json.dumps(drift_result, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
        "\n",
        "m = dict(manifest.get(\"metrics\", {}))\n",
        "m[out_key] = drift_result\n",
        "manifest[\"metrics\"] = m\n",
        "manifest_path.write_text(json.dumps(manifest, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Saved drift metrics â†’ {out_path.as_posix()}  (manifest aggiornato: {manifest_path.as_posix()})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bbc1d70-e1f4-48ce-ac17-2592e450c444",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### ModelReportRunner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20e93904",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Model Report Runner (coerente con il training: OrdinalEncoder + TTR, paths outputs/â€¦) ===\n",
        "from __future__ import annotations\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "\n",
        "# --- helper RMSE retro-compatibile (sklearn vecchie non hanno 'squared=')\n",
        "def _rmse(y_true, y_pred):\n",
        "    try:\n",
        "        return mean_squared_error(y_true, y_pred, squared=False)\n",
        "    except TypeError:\n",
        "        return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "# â”€â”€ 0) Carica manifest v2 per feature_config (paths coerenti: outputs/â€¦)\n",
        "PROP_DIR = Path(\"outputs/modeling/property\")\n",
        "MF_PATH = PROP_DIR / \"training_manifest.json\"\n",
        "\n",
        "cat_cols, num_cols = [], []\n",
        "if MF_PATH.exists():\n",
        "    mf = json.loads(MF_PATH.read_text(encoding=\"utf-8\"))\n",
        "    fc = (mf.get(\"feature_config\") or {})\n",
        "    cat_cols = list(fc.get(\"categorical\") or [])\n",
        "    num_cols = list(fc.get(\"numeric\") or [])\n",
        "else:\n",
        "    # fallback se non c'Ã¨ manifest\n",
        "    cat_cols = [\"location\", \"region\", \"zone\", \"energy_class\", \"urban_type\", \"orientation\", \"view\", \"condition\", \"heating\"]\n",
        "    num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c]) and c != \"valuation_k\"]\n",
        "\n",
        "# Sanity: tieni solo colonne esistenti\n",
        "cat_cols = [c for c in cat_cols if c in df.columns]\n",
        "num_cols = [c for c in num_cols if c in df.columns]\n",
        "ALL = cat_cols + num_cols\n",
        "\n",
        "assert \"valuation_k\" in df.columns, \"Manca il target valuation_k\"\n",
        "assert len(ALL) > 0, \"Nessuna feature trovata (cat+num)\"\n",
        "\n",
        "# â”€â”€ 1) Pipeline coerente con il training (Ordinal + imputazioni) + TTR\n",
        "pre_all = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", Pipeline([\n",
        "            (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "            (\"enc\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
        "        ]), cat_cols) if cat_cols else (\"cat\", \"drop\", []),\n",
        "        (\"num\", Pipeline([\n",
        "            (\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "        ]), num_cols) if num_cols else (\"num\", \"drop\", []),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    verbose_feature_names_out=False,\n",
        ")\n",
        "\n",
        "rf_all = RandomForestRegressor(\n",
        "    n_estimators=300, random_state=42, n_jobs=-1, max_depth=None, min_samples_leaf=2\n",
        ")\n",
        "\n",
        "pipe_all = Pipeline([\n",
        "    (\"prep\", pre_all),\n",
        "    (\"ttr\", TransformedTargetRegressor(\n",
        "        regressor=rf_all,\n",
        "        func=np.log1p, inverse_func=np.expm1, check_inverse=False\n",
        "    )),\n",
        "])\n",
        "\n",
        "# â”€â”€ 2) Valutazione split semplice (random) con tutte le feature\n",
        "X_all = df[ALL].copy()\n",
        "y_nat = df[\"valuation_k\"].astype(float).to_numpy()\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X_all, y_nat, test_size=0.2, random_state=42)\n",
        "pipe_all.fit(X_tr, y_tr)\n",
        "y_hat = pipe_all.predict(X_te)\n",
        "\n",
        "r2_all  = r2_score(y_te, y_hat)\n",
        "mae_all = mean_absolute_error(y_te, y_hat)\n",
        "rmse_all = _rmse(y_te, y_hat)\n",
        "print(f\"Random split â†’ RÂ²(all)={r2_all:.4f}  MAE={mae_all:.2f}  RMSE={rmse_all:.2f}\")\n",
        "\n",
        "# â”€â”€ 3) Solo numeriche (stesso schema di imputazione) + TTR\n",
        "if num_cols:\n",
        "    pre_num = ColumnTransformer(\n",
        "        [(\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))]), num_cols)],\n",
        "        remainder=\"drop\",\n",
        "        verbose_feature_names_out=False,\n",
        "    )\n",
        "    rf_num = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1, min_samples_leaf=2)\n",
        "    pipe_num = Pipeline([\n",
        "        (\"prep\", pre_num),\n",
        "        (\"ttr\", TransformedTargetRegressor(\n",
        "            regressor=rf_num, func=np.log1p, inverse_func=np.expm1, check_inverse=False\n",
        "        )),\n",
        "    ])\n",
        "    Xn = df[num_cols].copy()\n",
        "    Xn_tr, Xn_te, yn_tr, yn_te = train_test_split(Xn, y_nat, test_size=0.2, random_state=42)\n",
        "    pipe_num.fit(Xn_tr, yn_tr)\n",
        "    y_num = pipe_num.predict(Xn_te)\n",
        "\n",
        "    r2_num  = r2_score(yn_te, y_num)\n",
        "    mae_num = mean_absolute_error(yn_te, y_num)\n",
        "    rmse_num = _rmse(yn_te, y_num)\n",
        "    print(f\"Random split â†’ RÂ²(num)={r2_num:.4f}  MAE={mae_num:.2f}  RMSE={rmse_num:.2f}\")\n",
        "    print(f\"Î”RÂ² (all - num): {r2_all - r2_num:+.4f}\")\n",
        "\n",
        "# â”€â”€ 4) Stima piÃ¹ robusta: GroupShuffleSplit (group=location se presente)\n",
        "if \"location\" in df.columns:\n",
        "    gss = GroupShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
        "    r2s, maes, rmses = [], [], []\n",
        "    groups = df[\"location\"].astype(str).to_numpy()\n",
        "    for tr_idx, te_idx in gss.split(df[ALL], y_nat, groups=groups):\n",
        "        pipe_all.fit(df.iloc[tr_idx][ALL], y_nat[tr_idx])\n",
        "        y_g = pipe_all.predict(df.iloc[te_idx][ALL])\n",
        "        r2s.append(r2_score(y_nat[te_idx], y_g))\n",
        "        maes.append(mean_absolute_error(y_nat[te_idx], y_g))\n",
        "        rmses.append(_rmse(y_nat[te_idx], y_g))\n",
        "    print(f\"GSS 5Ã— (group=location) â†’ RÂ²={np.mean(r2s):.4f}Â±{np.std(r2s):.4f}  \"\n",
        "          f\"MAE={np.mean(maes):.2f}Â±{np.std(maes):.2f}  RMSE={np.mean(rmses):.2f}Â±{np.std(rmses):.2f}\")\n",
        "\n",
        "# â”€â”€ 5) Feature importance (dal RF dentro TTR) + nomi coerenti CT\n",
        "try:\n",
        "    try:\n",
        "        feat_names = list(pipe_all.named_steps[\"prep\"].get_feature_names_out())\n",
        "    except Exception:\n",
        "        feat_names = [*cat_cols, *num_cols]\n",
        "\n",
        "    rf_fitted = pipe_all.named_steps[\"ttr\"].regressor_\n",
        "    importances = getattr(rf_fitted, \"feature_importances_\", None)\n",
        "    if importances is None:\n",
        "        raise RuntimeError(\"feature_importances_ non disponibile sul regressore.\")\n",
        "\n",
        "    imp = np.asarray(importances, dtype=float)\n",
        "    if len(feat_names) != len(imp):\n",
        "        feat_names = [f\"f{i}\" for i in range(len(imp))]\n",
        "\n",
        "    fi = (pd.DataFrame({\"feature\": feat_names, \"importance\": imp})\n",
        "            .sort_values(\"importance\", ascending=False)\n",
        "            .reset_index(drop=True))\n",
        "\n",
        "    print(\"\\nTop 10 feature importance (Ordinal+CT):\")\n",
        "    print(fi.head(10).to_string(index=False))\n",
        "except Exception as e:\n",
        "    print(\"Feature importance non disponibile:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af220429",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Introspezione categorie OHE (safe) ---\n",
        "# 1) prende il prep dal pipeline\n",
        "prep = pipe_A.named_steps[\"prep\"]\n",
        "\n",
        "def _ensure_prep_fitted(prep):\n",
        "    \"\"\"Se prep non Ã¨ fit, fa un fit temporaneo SOLO sul preproc (senza toccare il modello).\"\"\"\n",
        "    if not hasattr(prep, \"transformers_\"):\n",
        "        # se esiste uno step di derivazione, applicalo prima del fit del prep\n",
        "        X_for_fit = X_train\n",
        "        if \"derive\" in pipe_A.named_steps:\n",
        "            X_for_fit = pipe_A.named_steps[\"derive\"].transform(X_train)\n",
        "        # fit solo il preproc (NO model)\n",
        "        prep.fit(X_for_fit, y_train)\n",
        "    return prep\n",
        "\n",
        "prep = _ensure_prep_fitted(prep)\n",
        "\n",
        "# 2) recupera il OneHotEncoder dentro al ramo 'cat'\n",
        "cat_branch = prep.named_transformers_.get(\"cat\")\n",
        "if hasattr(cat_branch, \"named_steps\"):\n",
        "    ohe = cat_branch.named_steps.get(\"encode\")\n",
        "else:\n",
        "    ohe = None\n",
        "\n",
        "if ohe is None or not hasattr(ohe, \"categories_\"):\n",
        "    raise RuntimeError(\"Il ramo categorico non contiene uno step 'encode' con OneHotEncoder giÃ  fit.\")\n",
        "\n",
        "cats_map = {col: list(cats) for col, cats in zip(cat_cols, ohe.categories_)}\n",
        "\n",
        "print(\"OHE categories â€” region:\", cats_map.get(\"region\", [])[:10])\n",
        "print(\"OHE categories â€” zone  :\", cats_map.get(\"zone\", [])[:10])\n",
        "\n",
        "# extra: verifica handle_unknown\n",
        "try:\n",
        "    print(\"handle_unknown:\", getattr(ohe, \"handle_unknown\", None))\n",
        "except Exception:\n",
        "    pass"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ai-oracle",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
