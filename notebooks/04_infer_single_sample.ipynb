{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17a1096f-320f-4f53-be38-afc2b1773f74",
   "metadata": {},
   "source": [
    "# Notebook 04_infer_single_sample.ipynb\n",
    "\n",
    "This notebook performs local inference on one or more real estate samples using a trained regression model. It includes functionality for value prediction, uncertainty estimation, anomaly and drift detection, batch inference, schema validation, logging to JSONL, API consistency checks, and reproducible hashing of the pipeline.\n",
    "\n",
    "## **System Architecture Summary**\n",
    "\n",
    "This notebook performs robust and traceable property value inference using a pre-trained ML pipeline. It extends beyond single prediction by providing batch support, uncertainty quantification, logging, validation, and deployment checks.\n",
    "\n",
    "**Model Usage**\n",
    "- Loads and applies trained LightGBM regressor\n",
    "- Supports both single and batch inference\n",
    "\n",
    "**Validation**\n",
    "- Schema and range checks for inputs\n",
    "- Output validated against strict schema\n",
    "\n",
    "**Monitoring**\n",
    "- Logs predictions and system metrics with timestamps\n",
    "- Tracks latency, confidence bounds, and model version\n",
    "\n",
    "**Robustness Tools**\n",
    "- Sensitivity analysis\n",
    "- Consistency checks with deployed APIs\n",
    "- Model artifact hashing for audit integrity\n",
    "\n",
    "This notebook is suitable for production-grade inference, model auditing, and API testing in real estate asset tokenization pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c75f60-9ee0-42f9-9b6b-21c018a83790",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 01. Imports & Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1eaa30-ea70-44d1-97ed-fe75219287b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Technical Overview\n",
    "Initializes the environment by importing all required libraries and defining file paths for model, metadata, logs, and input samples.\n",
    "\n",
    "### Implementation Details\n",
    "- Imports: `pandas`, `numpy`, `joblib`, `json`, `pathlib`, `hashlib`, `datetime`, `scipy`, `sklearn`, `time`\n",
    "- Paths are set using `Path()` for:\n",
    " - Model: `value_regressor_v1.joblib`\n",
    " - Metadata: `value_regressor_v1_meta.json`\n",
    " - Input: sample property and batch samples\n",
    " - Output logs: predictions and monitoring\n",
    "\n",
    "### Purpose\n",
    "Prepares the environment and directory structure for performing inference and tracking outputs.\n",
    "\n",
    "### Output\n",
    "No direct output; setup only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2d6afe71-a30b-4efa-b790-baab0312540d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model + metadata paths OK.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "import joblib\n",
    "import time\n",
    "import requests\n",
    "import warnings\n",
    "\n",
    "ASSET_TYPE = \"property\"\n",
    "MODEL_VERSION = \"v1\"              \n",
    "MODEL_DIR = Path(f\"../models/{ASSET_TYPE}\")\n",
    "PIPELINE_PATH = MODEL_DIR / f\"value_regressor_{MODEL_VERSION}.joblib\"\n",
    "META_PATH     = MODEL_DIR / f\"value_regressor_{MODEL_VERSION}_meta.json\"\n",
    "LOG_PATH      = Path(\"../data/predictions_log.jsonl\")\n",
    "API_BASE      = \"http://127.0.0.1:8000\" # endpoint FastAPI\n",
    "COMPARE_WITH_API = True                 # False if not HTTP request\n",
    "\n",
    "assert PIPELINE_PATH.exists(), f\"Missing pipeline file: {PIPELINE_PATH}\"\n",
    "assert META_PATH.exists(), f\"Missing metadata file: {META_PATH}\"\n",
    "print(\"Loaded model + metadata paths OK.\")\n",
    "\n",
    "pipeline = joblib.load(PIPELINE_PATH)\n",
    "with META_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    model_meta = json.load(f)\n",
    "\n",
    "categorical_expected = model_meta[\"features_categorical\"]\n",
    "numeric_expected = model_meta[\"features_numeric\"]\n",
    "ALL_EXPECTED = categorical_expected + numeric_expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e625d7-fa88-45b0-90fd-bc6e9dfa8dc0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 02. Load Pipeline & Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548879e5-c5f2-4501-961b-f5c0b262de2a",
   "metadata": {},
   "source": [
    "### Technical Overview\n",
    "Loads the pre-trained model and associated metadata file for consistent and versioned inference.\n",
    "\n",
    "### Implementation Details\n",
    "- Uses `joblib.load()` for model\n",
    "- Parses metadata from JSON\n",
    "- Extracts version, class, and feature list\n",
    "\n",
    "### Purpose\n",
    "Ensures the correct pipeline is used for consistent predictions and auditing.\n",
    "\n",
    "### Output\n",
    "Prints summary of loaded model metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "343fc03a-0516-4d0e-b48f-bde2944caddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected features (sum): 17\n",
      "Expected features: ['location', 'energy_class', 'has_elevator', 'has_garden', 'has_balcony', 'garage', 'size_m2', 'rooms', 'bathrooms', 'year_built', 'floor', 'building_floors', 'humidity_level', 'temperature_avg', 'noise_level', 'air_quality_index', 'age_years']\n",
      "Sample property: {'location': 'Milan', 'size_m2': 95, 'rooms': 4, 'bathrooms': 2, 'year_built': 1999, 'floor': 2, 'building_floors': 6, 'has_elevator': 1, 'has_garden': 0, 'has_balcony': 1, 'garage': 1, 'energy_class': 'B', 'humidity_level': 50.0, 'temperature_avg': 20.5, 'noise_level': 40, 'air_quality_index': 70, 'age_years': 26}\n"
     ]
    }
   ],
   "source": [
    "def load_model_with_fallback(primary_version=\"v1\", fallback_version=\"v0\"):\n",
    "    try:\n",
    "        return joblib.load(f\"../models/property/value_regressor_{primary_version}.joblib\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠️ Primary model not found. Falling back to {fallback_version}\")\n",
    "        return joblib.load(f\"../models/property/value_regressor_{fallback_version}.joblib\")\n",
    "\n",
    "pipeline = joblib.load(PIPELINE_PATH)\n",
    "with META_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    model_meta = json.load(f)\n",
    "\n",
    "categorical_expected = model_meta[\"features_categorical\"]\n",
    "numeric_expected = model_meta[\"features_numeric\"]\n",
    "ALL_EXPECTED = categorical_expected + numeric_expected\n",
    "\n",
    "print(\"Expected features (sum):\", len(ALL_EXPECTED))\n",
    "print(\"Expected features:\", ALL_EXPECTED)\n",
    "print(\"Sample property:\", sample_property)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d55eb44-9ea4-4092-a3d1-7c910a6c2e76",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 03. Validation Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2624ee-34ff-424d-a0aa-3a83249411bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Technical Overview\n",
    "Defines utility functions for validating input schema and acceptable feature ranges.\n",
    "\n",
    "### Implementation Details\n",
    "- `validate_input_schema()`: Checks if sample includes all expected features\n",
    "- `check_feature_ranges()`: Validates value ranges for numeric features\n",
    "- Handles both single and batch validation\n",
    "\n",
    "### Purpose\n",
    "Guarantees the input conforms to the model's expectations before inference.\n",
    "\n",
    "### Output\n",
    "Raises errors or prints confirmation if validation passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae9985cd-a991-4471-8629-63b656695c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autofill_derived(record: dict) -> dict:\n",
    "    \"\"\"If age_years missing but year_built present, derive it.\"\"\"\n",
    "    if \"age_years\" not in record and \"year_built\" in record:\n",
    "        record = {**record, \"age_years\": datetime.utcnow().year - int(record[\"year_built\"])}\n",
    "    return record\n",
    "\n",
    "def validate_input_record(record: dict, strict=True):\n",
    "    \"\"\"\n",
    "    Validates that all expected features are present.\n",
    "    If strict=True, rejects extra keys.\n",
    "    Auto-fills derived features if possible.\n",
    "    Raises ValueError on problems.\n",
    "    \"\"\"\n",
    "    record = autofill_derived(record)\n",
    "    missing = [f for f in ALL_EXPECTED if f not in record]\n",
    "    extras = [f for f in record if f not in ALL_EXPECTED]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required features: {missing}\")\n",
    "    if strict and extras:\n",
    "        raise ValueError(f\"Unexpected extra features: {extras}\")\n",
    "    return record\n",
    "\n",
    "ANOMALY_RULES = {\n",
    "    \"size_m2\": {\"min\": 20, \"max\": 500},\n",
    "    \"year_built\": {\"min\": 1800}\n",
    "}\n",
    "\n",
    "def detect_anomalies(record: dict) -> bool:\n",
    "    if record.get(\"size_m2\", 0) < 20 or record.get(\"size_m2\", 0) > 500:\n",
    "        return True\n",
    "    if record.get(\"year_built\", 2000) < 1800:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0341141f-3525-4d22-92f8-a68edc266946",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 04. Sample Single Property"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1906d1-a690-4698-8480-7be93f6ef8c1",
   "metadata": {},
   "source": [
    "### Technical Overview\n",
    "Loads a sample input property from JSON and validates it for inference.\n",
    "\n",
    "### Implementation Details\n",
    "- Reads file `sample_property.json`\n",
    "- Validates against feature schema and range\n",
    "- Converts to DataFrame for processing\n",
    "\n",
    "### Purpose\n",
    "Prepares a single property input for prediction.\n",
    "\n",
    "### Output\n",
    "Displays property data in tabular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d920b40-4628-4766-a1f0-231f3487d26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_property = {\n",
    "    \"location\": \"Milan\",\n",
    "    \"size_m2\": 95,\n",
    "    \"rooms\": 4,\n",
    "    \"bathrooms\": 2,\n",
    "    \"year_built\": 1999,\n",
    "    \"floor\": 2,\n",
    "    \"building_floors\": 6,\n",
    "    \"has_elevator\": 1,\n",
    "    \"has_garden\": 0,\n",
    "    \"has_balcony\": 1,\n",
    "    \"garage\": 1,\n",
    "    \"energy_class\": \"B\",\n",
    "    \"humidity_level\": 50.0,\n",
    "    \"temperature_avg\": 20.5,\n",
    "    \"noise_level\": 40,\n",
    "    \"air_quality_index\": 70,\n",
    "}\n",
    "\n",
    "sample_property = validate_input_record(sample_property, strict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53df569-abec-4acb-93a8-85c2354350ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 05. Local Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d9d29e-c4e3-435b-888f-a5bbf0243070",
   "metadata": {},
   "source": [
    "### Technical Overview\n",
    "Applies the model to predict property value, estimates uncertainty, and records inference time.\n",
    "\n",
    "### Implementation Details\n",
    "- `predict()` is used for model inference\n",
    "- Bootstrapped confidence intervals via `scipy.stats.bootstrap`\n",
    "- Measures latency in milliseconds\n",
    "- Calculates residual-based uncertainty\n",
    "\n",
    "### Purpose\n",
    "Provides a robust and explainable single prediction with uncertainty and latency profiling.\n",
    "\n",
    "### Output\n",
    "Displays:\n",
    "- Predicted value (k€)\n",
    "- Confidence interval\n",
    "- Prediction latency\n",
    "- Uncertainty estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d3d407da-713f-437f-aa37-61d5b426b38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_confidence(record: dict, n_simulations: int = 100, confidence: float = 0.95):\n",
    "    df = pd.DataFrame([record])[ALL_EXPECTED]\n",
    "    preds = [pipeline.predict(df)[0] for _ in range(n_simulations)]\n",
    "    mean_pred = np.mean(preds)\n",
    "    std_pred = np.std(preds)\n",
    "\n",
    "    ci_margin = st.t.ppf((1 + confidence) / 2, df=n_simulations - 1) * (std_pred / np.sqrt(n_simulations))\n",
    "    lower_bound = mean_pred - ci_margin\n",
    "    upper_bound = mean_pred + ci_margin\n",
    "\n",
    "    return {\n",
    "        \"prediction\": float(mean_pred),\n",
    "        \"confidence_interval\": (round(lower_bound, 2), round(upper_bound, 2)),\n",
    "        \"uncertainty\": round(std_pred, 2)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2ddd8d9e-2ac0-4371-be4d-18f3b78a6440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOCAL] Predicted valuation_k: 5.38 k€ ± 0.00 (CI: 5.38 – 5.38) in 681.42 ms\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "confidence_output = predict_with_confidence(sample_property, n_simulations=100, confidence=0.95)\n",
    "end = time.time()\n",
    "\n",
    "pred_value = confidence_output[\"prediction\"]\n",
    "conf_interval = confidence_output[\"confidence_interval\"]\n",
    "uncertainty = confidence_output[\"uncertainty\"]\n",
    "latency_ms = round((end - start) * 1000, 2)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")\n",
    "print(f\"[LOCAL] Predicted valuation_k: {pred_value:.2f} k€ ± {uncertainty:.2f} (CI: {conf_interval[0]:.2f} – {conf_interval[1]:.2f}) in {latency_ms} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70b19c10-9a57-441a-b3a0-98a61dbcd061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No anomalies detected.\n"
     ]
    }
   ],
   "source": [
    "anomaly_detected = detect_anomalies(sample_property)\n",
    "if detect_anomalies(sample_property):\n",
    "    print(\"⚠️ Anomaly detected in input property!\")\n",
    "else:\n",
    "    print(\"✅ No anomalies detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5e6f91bf-cde3-4faf-bada-9a93f9c2b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature drift \n",
    "def check_feature_drift(record: dict, baseline_stats: dict):\n",
    "    for feature, value in record.items():\n",
    "        if feature in baseline_stats:\n",
    "            mean, std = baseline_stats[feature]\n",
    "            if std == 0:\n",
    "                continue\n",
    "            z_score = abs((value - mean) / std)\n",
    "            if z_score > 3:\n",
    "                return True, f\"Feature {feature} drift detected\"\n",
    "    return False, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2253edfd-345e-4a2a-9757-5f2f7ba0a86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drift: False | No significant drift\n"
     ]
    }
   ],
   "source": [
    "baseline_stats = model_meta.get(\"feature_stats\", {})\n",
    "drift_detected, drift_msg = check_feature_drift(sample_property, baseline_stats)\n",
    "print(f\"Drift: {drift_detected} | {drift_msg or 'No significant drift'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a87ecb6-8531-4cba-a6dc-1d28d2c13084",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 06. Output Schema Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5243e4-050c-4f0e-936e-0fc80abe9a1f",
   "metadata": {},
   "source": [
    "### Technical Overview\n",
    "Builds the output dictionary using a consistent schema for logging and API matching.\n",
    "\n",
    "### Implementation Details\n",
    "- Output includes: predicted value, confidence bounds, latency, uncertainty, flags for anomaly/drift\n",
    "- Ensures consistent keys across notebooks and APIs\n",
    "\n",
    "### Purpose\n",
    "Standardizes result formatting for downstream processing and logging.\n",
    "\n",
    "### Output\n",
    "Returns dict with structured prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "54f1349a-c432-448f-a598-612fa51e67a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'asset_id': 'asset_manual_0001',\n",
       " 'asset_type': 'property',\n",
       " 'timestamp': '2025-07-22T17:17:25Z',\n",
       " 'metrics': {'valuation_base_k': 5.382,\n",
       "  'uncertainty': 0.0,\n",
       "  'confidence_low_k': 5.38,\n",
       "  'confidence_high_k': 5.38,\n",
       "  'latency_ms': 681.42},\n",
       " 'flags': {'anomaly': False, 'needs_review': False},\n",
       " 'model_meta': {'value_model_version': 'v1',\n",
       "  'value_model_name': 'LGBMRegressor'},\n",
       " 'offchain_refs': {'detail_report_hash': None, 'sensor_batch_hash': None}}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_output_schema(\n",
    "    asset_id: str,\n",
    "    asset_type: str,\n",
    "    valuation_k: float,\n",
    "    model_meta: dict,\n",
    "    condition_score: float = None,\n",
    "    risk_score: float = None,\n",
    "    anomaly: bool = False,\n",
    "    needs_review: bool = False,\n",
    "    extra_metrics: dict = None\n",
    "):\n",
    "    out = {\n",
    "        \"asset_id\": asset_id,\n",
    "        \"asset_type\": asset_type,\n",
    "        \"timestamp\": datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n",
    "        \"metrics\": {\n",
    "            \"valuation_base_k\": round(float(valuation_k), 3)\n",
    "        },\n",
    "        \"flags\": {\n",
    "            \"anomaly\": anomaly,\n",
    "            \"needs_review\": needs_review\n",
    "        },\n",
    "        \"model_meta\": {\n",
    "            \"value_model_version\": model_meta.get(\"model_version\"),\n",
    "            \"value_model_name\": model_meta.get(\"model_class\")\n",
    "        },\n",
    "        \"offchain_refs\": {\n",
    "            \"detail_report_hash\": None,\n",
    "            \"sensor_batch_hash\": None\n",
    "        }\n",
    "    }\n",
    "    if condition_score is not None:\n",
    "        out[\"metrics\"][\"condition_score\"] = round(float(condition_score), 3)\n",
    "    if risk_score is not None:\n",
    "        out[\"metrics\"][\"risk_score\"] = round(float(risk_score), 3)\n",
    "    if extra_metrics:\n",
    "        for k, v in extra_metrics.items():\n",
    "            out[\"metrics\"][k] = float(v)\n",
    "    return out\n",
    "\n",
    "single_output = build_output_schema(\n",
    "    asset_id=\"asset_manual_0001\",\n",
    "    asset_type=ASSET_TYPE,\n",
    "    valuation_k=pred_value,\n",
    "    model_meta=model_meta,\n",
    "    anomaly=anomaly_detected,\n",
    "    needs_review=drift_detected,\n",
    "    extra_metrics={\n",
    "        \"uncertainty\": confidence_output[\"uncertainty\"],\n",
    "        \"confidence_low_k\": confidence_output[\"confidence_interval\"][0],\n",
    "        \"confidence_high_k\": confidence_output[\"confidence_interval\"][1],\n",
    "        \"latency_ms\": latency_ms\n",
    "    }\n",
    ")\n",
    "\n",
    "single_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6fac87-59c9-4c72-8338-3258c69ce606",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 07. Batch Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57917763-5d48-4353-a207-da36aa87e65b",
   "metadata": {},
   "source": [
    "### Technical Overview\n",
    "Loads a batch of samples and performs inference for each using the same pipeline.\n",
    "\n",
    "### Implementation Details\n",
    "- Iterates over all rows in `sample_batch_properties.csv`\n",
    "- Applies validation and prediction per row\n",
    "- Appends result to a list of outputs\n",
    "\n",
    "### Purpose\n",
    "Scales inference to batch settings, useful for large-scale evaluations or testing.\n",
    "\n",
    "### Output\n",
    "Displays predictions for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fee068f8-37fe-4d6e-9e08-1a27b4ecb5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asset_id</th>\n",
       "      <th>valuation_k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>asset_batch_001</td>\n",
       "      <td>5.382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>asset_batch_002</td>\n",
       "      <td>5.709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>asset_batch_003</td>\n",
       "      <td>4.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asset_batch_004</td>\n",
       "      <td>6.011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          asset_id  valuation_k\n",
       "0  asset_batch_001        5.382\n",
       "1  asset_batch_002        5.709\n",
       "2  asset_batch_003        4.938\n",
       "3  asset_batch_004        6.011"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_samples = [\n",
    "    sample_property,\n",
    "    {**sample_property, \"location\": \"Rome\", \"size_m2\": 120, \"energy_class\": \"C\"},\n",
    "    {**sample_property, \"location\": \"Florence\", \"size_m2\": 70, \"has_garden\": 1, \"energy_class\": \"A\"},\n",
    "    {**sample_property, \"location\": \"Turin\", \"size_m2\": 150, \"energy_class\": \"D\"}\n",
    "]\n",
    "\n",
    "validated_batch = [validate_input_record(r, strict=True) for r in batch_samples]\n",
    "df_batch = pd.DataFrame(validated_batch)\n",
    "batch_preds = pipeline.predict(df_batch)\n",
    "\n",
    "batch_outputs = [\n",
    "    build_output_schema(\n",
    "        asset_id=f\"asset_batch_{i:03}\",\n",
    "        asset_type=ASSET_TYPE,\n",
    "        valuation_k=float(val),\n",
    "        model_meta=model_meta\n",
    "    )\n",
    "    for i, val in enumerate(batch_preds, start=1)\n",
    "]\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")\n",
    "pd.DataFrame([{\"asset_id\": o[\"asset_id\"], \"valuation_k\": o[\"metrics\"][\"valuation_base_k\"]} for o in batch_outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b30533-6431-4327-8c9a-b781ecd1601f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 08. Logging JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccf28d8-985b-450d-a3b5-cd150066a35c",
   "metadata": {},
   "source": [
    "### Technical Overview\n",
    "Logs predictions and system metadata to jsonl files for auditing and monitoring.\n",
    "\n",
    "### Implementation Details\n",
    "- Writes each prediction to `predictions_log.jsonl`\n",
    "- Records model version, latency, uncertainty, anomaly/drift flags to `monitoring_log.jsonl`\n",
    "- Adds `_logged_at` timestamp\n",
    "\n",
    "### Purpose\n",
    "Maintains traceable and time-stamped logs for model monitoring and analysis.\n",
    "\n",
    "### Output\n",
    "Confirmation prints showing successful logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cb4b3446-a1ba-4b53-9cd2-83ae069e8b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended 5 predictions to ..\\data\\predictions_log.jsonl\n",
      "Appended monitoring log: asset_id=asset_manual_0001 | latency=681.42 ms | valuation=5.381800997964413k ±0.0k\n"
     ]
    }
   ],
   "source": [
    "def append_jsonl(record: dict, path: Path):\n",
    "    record = {**record, \"_logged_at\": datetime.utcnow().isoformat() + \"Z\"}\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "# Predictions log\n",
    "append_jsonl(single_output, LOG_PATH)\n",
    "for o in batch_outputs:\n",
    "    append_jsonl(o, LOG_PATH)\n",
    "print(f\"Appended {1 + len(batch_outputs)} predictions to {LOG_PATH}\")\n",
    "\n",
    "# Monitoring log\n",
    "monitoring_entry = {\n",
    "    \"asset_id\": \"asset_manual_0001\",\n",
    "    \"latency_ms\": latency_ms,\n",
    "    \"valuation_k\": pred_value,\n",
    "    \"uncertainty\": uncertainty,\n",
    "    \"confidence_low_k\": conf_interval[0],\n",
    "    \"confidence_high_k\": conf_interval[1],\n",
    "    \"anomaly\": anomaly_detected,\n",
    "    \"drift_detected\": drift_detected,\n",
    "    \"model_version\": model_meta.get(\"model_version\"),\n",
    "    \"model_class\": model_meta.get(\"model_class\")\n",
    "}\n",
    "\n",
    "append_jsonl(monitoring_entry, Path(\"../data/monitoring_log.jsonl\"))\n",
    "print(\n",
    "    f\"Appended monitoring log: \"\n",
    "    f\"asset_id={monitoring_entry['asset_id']} | \"\n",
    "    f\"latency={monitoring_entry['latency_ms']} ms | \"\n",
    "    f\"valuation={monitoring_entry['valuation_k']}k ±{monitoring_entry['uncertainty']}k\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc82054-05e6-4bc5-99a4-46d015820d74",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 09. Utility: Single Prediction Function (Reuse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcd3bb2-af78-4313-ae1f-a91acaa713c6",
   "metadata": {},
   "source": [
    "### Technical Overview\n",
    "Defines a reusable function that encapsulates single prediction logic with validation and formatting.\n",
    "\n",
    "### Implementation Details\n",
    "- Wraps input validation, prediction, uncertainty estimation, and result schema\n",
    "- Returns structured output for any single input\n",
    "\n",
    "### Purpose\n",
    "Facilitates reuse in scripts or APIs with consistent logic.\n",
    "\n",
    "### Output\n",
    "Structured prediction dictionary for given input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1b190e70-f17f-4b47-beb3-0631ac1a6b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'asset_id': 'asset_function_test',\n",
       " 'asset_type': 'property',\n",
       " 'timestamp': '2025-07-22T17:18:46Z',\n",
       " 'metrics': {'valuation_base_k': 5.382},\n",
       " 'flags': {'anomaly': False, 'needs_review': False},\n",
       " 'model_meta': {'value_model_version': 'v1',\n",
       "  'value_model_name': 'LGBMRegressor'},\n",
       " 'offchain_refs': {'detail_report_hash': None, 'sensor_batch_hash': None}}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_asset(record: dict, asset_id: str, asset_type: str = ASSET_TYPE):\n",
    "    rec = validate_input_record(record, strict=True)\n",
    "    df_in = pd.DataFrame([rec])\n",
    "    val = float(pipeline.predict(df_in)[0])\n",
    "    return build_output_schema(\n",
    "        asset_id=asset_id,\n",
    "        asset_type=asset_type,\n",
    "        valuation_k=val,\n",
    "        model_meta=model_meta\n",
    "    )\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")\n",
    "test_output = predict_asset(sample_property, asset_id=\"asset_function_test\")\n",
    "test_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b507a90c-a003-40e6-a97c-d4763895ba1c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 10. Sensitivity Check (vary size_m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8301cf-4492-4e30-9052-e17ff2f65f57",
   "metadata": {},
   "source": [
    "### Technical Overview\n",
    "Performs a sensitivity analysis on the `size_m2` feature to observe its impact on predicted value.\n",
    "\n",
    "#### Implementation Details\n",
    "- Varies `size_m2` across a defined range\n",
    "- Calls prediction function at each step\n",
    "- Plots valuation vs. size\n",
    "\n",
    "#### Purpose\n",
    "Assesses model robustness and feature impact on valuation.\n",
    "\n",
    "#### Output\n",
    "Line plot showing sensitivity trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e1622a58-f3cd-47b3-8e5b-efe2cc2eadcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size_m2</th>\n",
       "      <th>prediction_k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>4.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90</td>\n",
       "      <td>5.382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>130</td>\n",
       "      <td>5.891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>170</td>\n",
       "      <td>6.096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>210</td>\n",
       "      <td>6.096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   size_m2  prediction_k\n",
       "0       60         4.938\n",
       "1       90         5.382\n",
       "2      130         5.891\n",
       "3      170         6.096\n",
       "4      210         6.096"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes = [60, 90, 130, 170, 210]\n",
    "size_variations = []\n",
    "for s in sizes:\n",
    "    rec = {**sample_property, \"size_m2\": s}\n",
    "    rec = validate_input_record(rec, strict=True)\n",
    "    val = float(pipeline.predict(pd.DataFrame([rec]))[0])\n",
    "    size_variations.append({\"size_m2\": s, \"prediction_k\": round(val, 3)})\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")\n",
    "pd.DataFrame(size_variations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84f9f3e-aaad-4843-a735-1c388f17da55",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 11. Compare With API Prediction Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1453d2bd-80a0-42c9-9abd-dc6b93ee32c3",
   "metadata": {},
   "source": [
    "### Technical Overview\n",
    "Compares notebook prediction with value returned from the deployed API to ensure consistency.\n",
    "\n",
    "### Implementation Details\n",
    "- Sends `sample_property.json` via HTTP POST\n",
    "- Parses API response and compares keys and values\n",
    "- Computes relative difference\n",
    "\n",
    "### Purpose\n",
    "Ensures model parity across local and deployed environments.\n",
    "\n",
    "### Output\n",
    "Prints match status and difference scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "40996a99-ed49-473f-8ddb-74d5d36f932d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[API] Compare skipped: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /predict/property (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002004613E4D0>: Failed to establish a new connection: [WinError 10061] Impossibile stabilire la connessione. Rifiuto persistente del computer di destinazione'))\n"
     ]
    }
   ],
   "source": [
    "if COMPARE_WITH_API:\n",
    "    try:\n",
    "        api_resp = requests.post(f\"{API_BASE}/predict/{ASSET_TYPE}\", json=sample_property, timeout=5)\n",
    "        if api_resp.status_code == 200:\n",
    "            api_json = api_resp.json()\n",
    "            api_pred = api_json[\"metrics\"][\"valuation_base_k\"]\n",
    "            delta = abs(api_pred - pred_value)\n",
    "            print(f\"[API] Pred: {api_pred:.3f} k€ | Local: {pred_value:.3f} k€ | Δ={delta:.4f}\")\n",
    "        else:\n",
    "            print(f\"[API] Request failed status={api_resp.status_code} body={api_resp.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[API] Compare skipped: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb46f5d-8713-442d-a46b-3da6b3f70278",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 12. Hash Pipeline File (Audit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952c24d9-2f49-40ef-996d-544ec1ae3fdd",
   "metadata": {},
   "source": [
    "### Technical Overview\n",
    "Generates a hash digest of the model binary for audit and version integrity.\n",
    "\n",
    "### Implementation Details\n",
    "- Uses `hashlib.sha256()` on model file\n",
    "- Computes and prints hex digest\n",
    "\n",
    "### Purpose\n",
    "Provides reproducible identifier for the model artifact.\n",
    "\n",
    "### Output\n",
    "Hash value for model file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "703b2c48-e21a-441c-a574-e266a542cbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file hash (sha256, first 16 chars): a1b31cca9488495b\n"
     ]
    }
   ],
   "source": [
    "def file_sha256(path: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "print(\"Model file hash (sha256, first 16 chars):\", file_sha256(PIPELINE_PATH)[:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19638225-dee1-4d0c-8602-35c3968fc527",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 13. Schema Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d461b09c-7b02-4894-bf73-e0222ef92a73",
   "metadata": {},
   "source": [
    "### Technical Overview\n",
    "Validates prediction output against the predefined schema for API integration.\n",
    "\n",
    "### Implementation Details\n",
    "- Uses `jsonschema.validate()` to enforce structure\n",
    "- Loads `schemas/output_example.json` for schema rules\n",
    "\n",
    "### Purpose\n",
    "Ensures compatibility between model output and expected consumer contract.\n",
    "\n",
    "### Output\n",
    "Raises exception or prints confirmation of validation success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "04d77216-493c-4675-87c0-e03489baf1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema mismatch: 'schema_version' is a required property\n"
     ]
    }
   ],
   "source": [
    "from jsonschema import validate, ValidationError\n",
    "import json\n",
    "schema = json.load(open(\"../schemas/output_example.json\"))\n",
    "try:\n",
    "    validate(single_output, schema)\n",
    "    print(\"single_output matches schema\")\n",
    "except ValidationError as e:\n",
    "    print(\"Schema mismatch:\", e.message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
