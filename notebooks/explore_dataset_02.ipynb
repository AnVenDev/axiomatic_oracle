{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "04ff6e62-5322-420f-89a5-f19d4d65ab1e",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16acd391",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 00. Imports\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np                  # type: ignore\n",
        "import pandas as pd                 # type: ignore\n",
        "import matplotlib.pyplot as plt     # type: ignore\n",
        "\n",
        "from notebooks.shared.n02_explore_dataset.eda_core import (\n",
        "    DescriptiveAnalyzer, OutlierDetector, TemporalAnalyzer,\n",
        "    StatisticalTester, FeatureImportanceAnalyzer, ensure_temporal_columns,\n",
        "    plot_correlation_heatmap,\n",
        ")\n",
        "from notebooks.shared.n02_explore_dataset.eda_reports import InsightsAnalyzer, EDAReportRunner\n",
        "from notebooks.shared.common.utils import NumpyJSONEncoder, log_basic_diagnostics\n",
        "from notebooks.shared.common.constants import (\n",
        "    VALUATION_K, ENERGY_CLASS, CONDITION_SCORE, RISK_SCORE,\n",
        "    LUXURY_SCORE, ENV_SCORE, SIZE_M2, LAG_HOURS, LOCATION\n",
        ")\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
        "plt.rcParams[\"figure.dpi\"] = 110"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6e892fc-5e20-48ae-b44e-c53ed5f3edd6",
      "metadata": {},
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d9140bb-7102-4287-b684-5fff7bfa5164",
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# 01: Setup\n",
        "ANALYSIS_DIR = Path(\"outputs/analysis\"); ANALYSIS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "FIG_DIR = ANALYSIS_DIR / \"figures\"; FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# helper salvataggio figure\n",
        "def _savefig(name: str):\n",
        "    path = FIG_DIR / name\n",
        "    plt.savefig(path, bbox_inches=\"tight\")\n",
        "    print(f\"üìà Figure saved: {path}\")\n",
        "\n",
        "# carica ultimo manifest di nb01\n",
        "snapshots_dir = Path(\"outputs/snapshots\")\n",
        "manifests = sorted(snapshots_dir.glob(\"manifest_*.json\"))\n",
        "assert manifests, \"Nessun manifest trovato in outputs/snapshots. Esegui nb01 export prima.\"\n",
        "manifest01_path = manifests[-1]\n",
        "with open(manifest01_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    manifest01 = json.load(f)\n",
        "\n",
        "dataset_path = Path(manifest01[\"paths\"][\"dataset\"])\n",
        "assert dataset_path.exists(), f\"Dataset non trovato: {dataset_path}\"\n",
        "\n",
        "# leggi dataset (csv/parquet)\n",
        "df = pd.read_parquet(dataset_path) if dataset_path.suffix.lower()==\".parquet\" else pd.read_csv(dataset_path)\n",
        "\n",
        "# colonne temporali coerenti (best-effort) + diagnostica\n",
        "df = ensure_temporal_columns(df)\n",
        "log_basic_diagnostics(df)\n",
        "\n",
        "print(f\"Loaded dataset: {dataset_path}  ‚Üí rows={len(df):,}, cols={df.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79539ccd-1470-4384-89ce-a219f74e443a",
      "metadata": {},
      "source": [
        "### Report Loading & Distribution Analysis ‚Äî 2.1 Upload nb01 report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48677d57-2a3b-4104-9dd7-e28f307c9783",
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# 02.1: Carica report di nb01 (robusto, con path assoluto e fallback)\n",
        "from pathlib import Path\n",
        "\n",
        "quality_path = Path(manifest01[\"paths\"].get(\"quality_report_json\", \"\"))\n",
        "sanity_path  = Path(\"outputs/sanity_report.json\")\n",
        "drift_path   = Path(\"outputs/location_drift.json\")\n",
        "\n",
        "# candidati per profiling_report\n",
        "profiling_candidates = [\n",
        "    Path(r\"C:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\logs\\profiling_report.json\"),\n",
        "    Path(manifest01.get(\"paths\", {}).get(\"log_dir\", \"\")) / \"profiling_report.json\"\n",
        "    if manifest01.get(\"paths\", {}).get(\"log_dir\") else None,\n",
        "    Path(\"logs/profiling_report.json\"),\n",
        "    Path(\"outputs/logs/profiling_report.json\"),\n",
        "    Path(\"../logs/profiling_report.json\").resolve(),\n",
        "    Path(\"../../logs/profiling_report.json\").resolve(),\n",
        "]\n",
        "profiling_candidates = [p for p in profiling_candidates if p]  # rimuovi None\n",
        "\n",
        "profiling_path = next((p for p in profiling_candidates if p.exists()), None)\n",
        "if not profiling_path:\n",
        "    # ultima spiaggia: cerca nel tree corrente\n",
        "    matches = list(Path.cwd().resolve().rglob(\"profiling_report.json\"))\n",
        "    profiling_path = matches[0] if matches else None\n",
        "\n",
        "def _load_json(p: Path) -> dict:\n",
        "    if not p or not p.exists(): return {}\n",
        "    try:\n",
        "        with open(p, \"r\", encoding=\"utf-8\") as f: return json.load(f)\n",
        "    except Exception:\n",
        "        return {}\n",
        "\n",
        "reports01 = {\n",
        "    \"quality\": _load_json(quality_path),\n",
        "    \"sanity\": _load_json(sanity_path),\n",
        "    \"profiling\": _load_json(profiling_path) if profiling_path else {},\n",
        "    \"location_drift\": _load_json(drift_path),\n",
        "}\n",
        "\n",
        "print(\"Artefatti caricati:\", [k for k,v in reports01.items() if v])\n",
        "print(\"CWD:\", Path.cwd())\n",
        "print(\"Profiling path:\", profiling_path if profiling_path else \"not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b895f674",
      "metadata": {},
      "source": [
        "### Report Loading & Distribution Analysis ‚Äî 2.2 Distribuzione per location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19cb6c06",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 02.2: Distribuzione per location (API top-level) + export\n",
        "from notebooks.shared.common.reports import DistributionAnalyzer\n",
        "\n",
        "dist = DistributionAnalyzer(df)\n",
        "loc_analysis = dist.analyze_location(\n",
        "    target_weights=(reports01.get(\"sanity\", {}).get(\"expected_profile\", {}) or {}).get(\"location_weights\"),\n",
        "    tolerance=(reports01.get(\"sanity\", {}).get(\"expected_profile\", {}) or {}).get(\"location_distribution_tolerance\", 0.05),\n",
        ")\n",
        "\n",
        "loc_counts = loc_analysis.get(\"counts\", {}) or {}\n",
        "loc_pcts   = loc_analysis.get(\"percentages\", {}) or {}\n",
        "\n",
        "loc_df = pd.DataFrame({\n",
        "    \"count\": pd.Series(loc_counts, dtype=\"Int64\"),\n",
        "    \"pct\":   pd.Series(loc_pcts, dtype=\"float\"),\n",
        "}).fillna(0).sort_values(\"count\", ascending=False)\n",
        "\n",
        "display(loc_df.head(20))\n",
        "\n",
        "loc_csv  = ANALYSIS_DIR / \"location_distribution.csv\"\n",
        "loc_parq = ANALYSIS_DIR / \"location_distribution.parquet\"\n",
        "loc_df.to_csv(loc_csv, encoding=\"utf-8\")\n",
        "loc_df.to_parquet(loc_parq)\n",
        "print(f\"Saved: {loc_csv}, {loc_parq}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d86a2f4d",
      "metadata": {},
      "source": [
        "### Report Loading & Distribution Analysis ‚Äî 2.3 Drift summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d21de27",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 02.3: Drift summary con fallback robusto\n",
        "expected_profile = (reports01.get(\"sanity\", {}).get(\"expected_profile\", {}) or {})\n",
        "expected = expected_profile.get(\"location_weights\", {}) or {}\n",
        "tolerance = expected_profile.get(\"location_distribution_tolerance\", 0.05)\n",
        "\n",
        "# Fallback A: prova a usare gli expected salvati nel drift di nb01 (se presente)\n",
        "if not expected and reports01.get(\"location_drift\"):\n",
        "    diffs_nb01 = reports01[\"location_drift\"].get(\"differences\", {}) or {}\n",
        "    expected = {k: float(v.get(\"expected\", 0.0) or 0.0) for k, v in diffs_nb01.items()}\n",
        "\n",
        "# Fallback B: se ancora vuoto, non facciamo drift check (baseline assente)\n",
        "if not expected:\n",
        "    print(\"‚ÑπÔ∏è Nessun expected_profile disponibile ‚Üí salto il drift check (baseline mancante).\")\n",
        "    drift_report = {\n",
        "        \"tolerance\": tolerance,\n",
        "        \"drifted_locations\": [],\n",
        "        \"differences\": {},\n",
        "        \"nb01_drift\": reports01.get(\"location_drift\", {}),\n",
        "        \"summary\": loc_analysis.get(\"summary\", {}),\n",
        "        \"note\": \"No baseline ‚Üí drift check skipped\",\n",
        "    }\n",
        "else:\n",
        "    obs_pct = loc_analysis.get(\"percentages\", {}) or {}\n",
        "    all_locs = sorted(set(expected.keys()) | set(obs_pct.keys()))\n",
        "    drifted, differences = [], {}\n",
        "    for loc in all_locs:\n",
        "        exp = float(expected.get(loc, 0.0) or 0.0)\n",
        "        obs = float(obs_pct.get(loc, 0.0) or 0.0)\n",
        "        diff = obs - exp\n",
        "        differences[loc] = {\"expected\": exp, \"observed\": obs, \"difference\": diff}\n",
        "        if abs(diff) > tolerance:\n",
        "            drifted.append(loc)\n",
        "    drift_report = {\n",
        "        \"tolerance\": tolerance,\n",
        "        \"drifted_locations\": drifted,\n",
        "        \"differences\": differences,\n",
        "        \"nb01_drift\": reports01.get(\"location_drift\", {}),\n",
        "        \"summary\": loc_analysis.get(\"summary\", {}),\n",
        "    }\n",
        "    if drifted:\n",
        "        print(\"‚ö†Ô∏è Drift oltre soglia per:\", drifted)\n",
        "\n",
        "drift_json = ANALYSIS_DIR / \"location_drift_eda.json\"\n",
        "drift_json.write_text(json.dumps(drift_report, cls=NumpyJSONEncoder, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
        "print(f\"Saved: {drift_json}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9deaa956",
      "metadata": {},
      "source": [
        "### Report Loading & Distribution Analysis ‚Äî 2.4 Barplot top locations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd85eac6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 02.4: Barplot top locations (robusto, senza definire funzioni)\n",
        "from pathlib import Path\n",
        "fig_dir = Path(\"outputs/analysis/figures\"); fig_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if 'loc_df' in globals() and isinstance(loc_df, pd.DataFrame) and not loc_df.empty and \"count\" in loc_df:\n",
        "    top = loc_df[\"count\"].sort_values(ascending=False).head(12)\n",
        "elif LOCATION in df.columns:\n",
        "    top = df[LOCATION].value_counts(dropna=False).head(12)\n",
        "else:\n",
        "    top = pd.Series(dtype=\"int64\")\n",
        "\n",
        "if top.empty:\n",
        "    print(\"‚ö†Ô∏è Nessuna location disponibile per il plot.\")\n",
        "else:\n",
        "    ax = top.plot(kind=\"bar\")\n",
        "    ax.set_title(\"Top locations by count\")\n",
        "    ax.set_ylabel(\"Count\")\n",
        "    ax.set_xlabel(\"Location\")\n",
        "    plt.xticks(rotation=30, ha=\"right\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(fig_dir / \"top_locations_count.png\", bbox_inches=\"tight\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10bff68e",
      "metadata": {},
      "source": [
        "### Report Loading & Distribution Analysis ‚Äî 2.5 Salva anche hist prezzi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34421012",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 02.5: Istogramma valuation_k (extra rapido)\n",
        "if VALUATION_K in df:\n",
        "    df[VALUATION_K].plot(kind=\"hist\", bins=40)\n",
        "    plt.title(\"Valuation (k‚Ç¨) distribution\"); plt.xlabel(\"valuation_k\"); plt.ylabel(\"freq\")\n",
        "    _savefig(\"valuation_hist.png\"); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "679edb43-6b8e-4e8f-80f6-06a76bc237f5",
      "metadata": {},
      "source": [
        "### Descriptive Statistics & Distributions ‚Äî relationship plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb97e3f8-589a-4f3d-bd24-84a46d7f3d72",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 03.1: Relationship plots (scatter, regplot, boxplot, heatmap score)\n",
        "desc = DescriptiveAnalyzer()\n",
        "fig = desc.create_relationship_plots(df, figsize=(12,10))\n",
        "_savefig(\"relationship_plots.png\"); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f601936d",
      "metadata": {},
      "source": [
        "### Descriptive Statistics & Distributions ‚Äî heatmap score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f275b732",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 03.2: Heatmap correlazioni sugli score\n",
        "score_cols = [c for c in [CONDITION_SCORE, RISK_SCORE, LUXURY_SCORE, ENV_SCORE] if c in df.columns]\n",
        "if len(score_cols) >= 2:\n",
        "    fig, ax = plt.subplots(1,1, figsize=(6,5))\n",
        "    plot_correlation_heatmap(df, score_cols, ax=ax)\n",
        "    _savefig(\"score_correlation_heatmap.png\"); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d265cf23-9329-4508-8cce-778913d8ade2",
      "metadata": {},
      "source": [
        "### Condition and Risk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c873a4a6-4751-43b8-be76-8a1b6bcda1b5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 04: Condition vs Energy class ‚Äì boxplot dedicato\n",
        "if CONDITION_SCORE in df and ENERGY_CLASS in df:\n",
        "    import seaborn as sns  # usato internamente anche da eda_core\n",
        "    plt.figure(figsize=(8,5))\n",
        "    sns.boxplot(data=df, x=ENERGY_CLASS, y=CONDITION_SCORE, showfliers=False)\n",
        "    plt.title(\"Condition score by Energy class\"); plt.xlabel(\"Energy class\"); plt.ylabel(\"Condition score\")\n",
        "    _savefig(\"condition_by_energy_boxplot.png\"); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de0b3cec-13c1-460d-8476-30f396fc6244",
      "metadata": {},
      "source": [
        "### Relations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43ed6aa1-aa96-4daa-bbd6-e48c84aa5c14",
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# 05.1: Pearson corr con target\n",
        "num_df = df.select_dtypes(include=[np.number])\n",
        "if VALUATION_K in num_df:\n",
        "    corr = num_df.corr(numeric_only=True)[VALUATION_K].drop(labels=[VALUATION_K], errors=\"ignore\")\n",
        "    top_pos = corr.sort_values(ascending=False).head(8)\n",
        "    top_neg = corr.sort_values(ascending=True).head(8)\n",
        "    top_abs = corr.abs().sort_values(ascending=False).head(12)\n",
        "    corr_df = pd.DataFrame({\"corr\": corr})\n",
        "    display(corr_df.loc[top_abs.index])\n",
        "    corr_csv = ANALYSIS_DIR / \"target_correlations.csv\"\n",
        "    corr_parq = ANALYSIS_DIR / \"target_correlations.parquet\"\n",
        "    corr_df.to_csv(corr_csv, encoding=\"utf-8\"); corr_df.to_parquet(corr_parq)\n",
        "    print(f\"Saved: {corr_csv}, {corr_parq}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb54b02e-04eb-485b-a973-add746122547",
      "metadata": {},
      "source": [
        "### Analisi statistica avanzata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd7413f4-9e3f-4e41-8ee1-6bf12039baa3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 06.1: Test statistici (normalit√†, œá¬≤, distribution stats) + summary\n",
        "tester = StatisticalTester()\n",
        "stats_results = tester.run_comprehensive_tests(df)\n",
        "summary_df = pd.DataFrame([stats_results[\"summary\"]])\n",
        "display(summary_df)\n",
        "\n",
        "stats_json = ANALYSIS_DIR / \"stat_tests_results.json\"\n",
        "stats_json.write_text(json.dumps(stats_results, cls=NumpyJSONEncoder, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
        "print(f\"Saved: {stats_json}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4405f73f-6598-4440-b3ec-a3b17a793127",
      "metadata": {},
      "source": [
        "### Temporal analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2207a6ff-8709-43fd-8e7f-a6f8cb1e269c",
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# 07: Temporal analysis (refactor + breakdown per location)\n",
        "from pathlib import Path\n",
        "\n",
        "# 07.1: Analisi temporale\n",
        "temp = TemporalAnalyzer()\n",
        "df_temp, temp_report = temp.analyze(df, target=VALUATION_K)\n",
        "print(temp_report)\n",
        "\n",
        "# 07.2: Plot temporali + salvataggio figura\n",
        "fig = temp.plot(df_temp, target=VALUATION_K)\n",
        "\n",
        "fig_dir = Path(\"outputs/analysis/figures\")\n",
        "fig_dir.mkdir(parents=True, exist_ok=True)\n",
        "plt.savefig(fig_dir / \"temporal_analysis.png\", bbox_inches=\"tight\")\n",
        "print(f\"üìà Figure saved: {fig_dir / 'temporal_analysis.png'}\")\n",
        "plt.show()\n",
        "\n",
        "# 07.3: Export report JSON\n",
        "(Path(ANALYSIS_DIR) / \"temporal_report.json\").write_text(\n",
        "    json.dumps(temp_report, cls=NumpyJSONEncoder, indent=2, ensure_ascii=False),\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "# 07.4: Breakdown freschezza per location (tabella + barplot)\n",
        "try:\n",
        "    if \"days_since_verification\" in df_temp.columns and LOCATION in df_temp.columns:\n",
        "        ths = [30, 60, 90]\n",
        "        grp = df_temp.groupby(LOCATION, observed=True)\n",
        "\n",
        "        fresk = pd.DataFrame({\"count\": grp.size()})\n",
        "        for th in ths:\n",
        "            flag_col = f\"is_stale_{th}d\"\n",
        "            if flag_col in df_temp.columns:\n",
        "                fresk[f\"pct_over_{th}d\"] = (grp[flag_col].mean() * 100).astype(float)\n",
        "            else:\n",
        "                fresk[f\"pct_over_{th}d\"] = np.nan\n",
        "\n",
        "        fresk = fresk.sort_values(\"pct_over_30d\", ascending=False)\n",
        "\n",
        "        fresk_csv  = ANALYSIS_DIR / \"freshness_by_location.csv\"\n",
        "        fresk_parq = ANALYSIS_DIR / \"freshness_by_location.parquet\"\n",
        "        fresk.to_csv(fresk_csv, encoding=\"utf-8\")\n",
        "        fresk.to_parquet(fresk_parq)\n",
        "        print(f\"üíæ Saved: {fresk_csv}, {fresk_parq}\")\n",
        "\n",
        "        # barplot % >30d (top 15)\n",
        "        top = fresk.head(15)\n",
        "        ax = top[\"pct_over_30d\"].plot(kind=\"bar\")\n",
        "        ax.set_title(\"% records >30d per location (top 15)\")\n",
        "        ax.set_ylabel(\"% >30d\")\n",
        "        ax.set_xlabel(\"Location\")\n",
        "        plt.xticks(rotation=30, ha=\"right\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(fig_dir / \"freshness_over30_by_location.png\", bbox_inches=\"tight\")\n",
        "        print(f\"üìà Figure saved: {fig_dir / 'freshness_over30_by_location.png'}\")\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è Breakdown per location non disponibile (manca 'days_since_verification' o 'location').\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Impossibile calcolare il breakdown per location: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f882034c-fca4-45d7-9c84-1fc17bff36ee",
      "metadata": {},
      "source": [
        "### Insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b81d02e-553c-4ff3-81d6-6429e16fbdf7",
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# 08: Insights sintetici + export\n",
        "ins = InsightsAnalyzer(top_n=5)\n",
        "insights = ins.generate_value_insights(df)\n",
        "\n",
        "ins_json = ANALYSIS_DIR / \"insights.json\"\n",
        "ins_json.write_text(json.dumps(insights, cls=NumpyJSONEncoder, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
        "print(f\"Saved: {ins_json}\")\n",
        "\n",
        "# anteprime\n",
        "top_val = insights.get(\"top_assets\", {}).get(\"by_valuation\", {}).get(\"data\", [])\n",
        "worst = insights.get(\"worst_assets\", {}).get(\"by_condition\", {}).get(\"data\", [])\n",
        "display(pd.DataFrame(top_val).head(10))\n",
        "display(pd.DataFrame(worst).head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38cf5101-12c2-449b-b3bb-e11f8ff831d7",
      "metadata": {},
      "source": [
        "### Outlier Analysis (IQR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43179ea7-f867-46d5-87af-8cdebe478e15",
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# 09.1‚Äì09.2: Outlier detection (IQR) ‚Üí combine ‚Üí stats ‚Üí export (Parquet/JSON)\n",
        "from notebooks.shared.n02_explore_dataset.eda_core import DEFAULT_NUMERIC_FEATURES, LEAKY_FEATURES\n",
        "from pathlib import Path\n",
        "\n",
        "numeric_cols = [c for c in DEFAULT_NUMERIC_FEATURES if c in df.columns and c not in LEAKY_FEATURES]\n",
        "\n",
        "od = OutlierDetector(method=\"iqr\", iqr_multiplier=1.5)  # niente output_dir qui\n",
        "out_summary = od.detect_outliers(df, columns=numeric_cols)\n",
        "\n",
        "combined = od.combine_outlier_results(df, out_summary)\n",
        "stats = od.get_outlier_summary_stats(df, out_summary, combined)\n",
        "\n",
        "out_dir = Path(\"outputs/analysis/outliers\")\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Salvataggi (solo se c‚Äô√® qualcosa)\n",
        "(out_dir / \"outliers_summary.json\").write_text(\n",
        "    json.dumps(stats, indent=2, ensure_ascii=False, cls=NumpyJSONEncoder),\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "if not combined.empty:\n",
        "    combined.to_parquet(out_dir / \"outliers_combined.parquet\", index=False)\n",
        "    combined.to_csv(out_dir / \"outliers_combined.csv\", index=False, encoding=\"utf-8\")\n",
        "    display(combined.head(10))\n",
        "else:\n",
        "    print(\"Nessun outlier combinato da salvare.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7cce389-e2e5-4c54-956d-1aec96837b55",
      "metadata": {},
      "source": [
        "### ML Preparation Insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a759e61-f8c6-4542-a961-2f1e7140c481",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 10.1: Prepara features (no leakage) e calcola importances\n",
        "fia = FeatureImportanceAnalyzer(target_column=VALUATION_K, n_estimators=200, random_state=42)\n",
        "X, y, feats = fia.prepare_features(df, include_proxies=False)\n",
        "imps = fia.calculate_importances(X, y, calculate_permutation=True, n_repeats=5)\n",
        "\n",
        "imp_builtin = imps[\"builtin\"].reset_index(drop=True)\n",
        "imp_perm = imps.get(\"permutation\", pd.DataFrame()).reset_index(drop=True)\n",
        "\n",
        "display(imp_builtin.head(15))\n",
        "if not imp_perm.empty:\n",
        "    display(imp_perm.head(15))\n",
        "\n",
        "# export importances\n",
        "imp_builtin.to_csv(ANALYSIS_DIR / \"feature_importances_builtin.csv\", index=False)\n",
        "imp_builtin.to_parquet(ANALYSIS_DIR / \"feature_importances_builtin.parquet\", index=False)\n",
        "if not imp_perm.empty:\n",
        "    imp_perm.to_csv(ANALYSIS_DIR / \"feature_importances_permutation.csv\", index=False)\n",
        "    imp_perm.to_parquet(ANALYSIS_DIR / \"feature_importances_permutation.parquet\", index=False)\n",
        "\n",
        "# 10.2: Ablation ‚Äúlight‚Äù sulle top-5\n",
        "top_feats = imp_builtin.head(5)[\"feature\"].tolist()\n",
        "abl = fia.perform_ablation_study(X, y, features_to_ablate=top_feats, cv_folds=3)\n",
        "display(abl)\n",
        "\n",
        "abl.to_csv(ANALYSIS_DIR / \"ablation_study.csv\")\n",
        "abl.to_parquet(ANALYSIS_DIR / \"ablation_study.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77ba0d8d",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### EDAReportRunner (one-shot + manifest EDA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba4abf6b",
      "metadata": {},
      "outputs": [],
      "source": [
        "runner = EDAReportRunner(output_dir=str(ANALYSIS_DIR))\n",
        "eda_manifest = runner.run_full_eda(df, save_plots=True, save_tables=True)\n",
        "\n",
        "print(\"EDA manifest written:\", eda_manifest.get(\"manifest_path\"))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ai-oracle",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
