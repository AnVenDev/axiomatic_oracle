{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b756d33a-e242-4a4c-b988-3c0bb1f0a764",
      "metadata": {},
      "source": [
        "### Base config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "67fe5d12-8cf5-4cee-9ac2-a0cdcc0b0ad0",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-10-05 12:22:24,827] INFO dataset_generator: ============================================================\n",
            "[2025-10-05 12:22:24,827] INFO dataset_generator: DATASET GENERATION CONFIGURATION\n",
            "[2025-10-05 12:22:24,827] INFO dataset_generator: ============================================================\n",
            "[2025-10-05 12:22:24,827] INFO dataset_generator: Config summary:\n",
            "{\"asset_type\":\"property\",\"cities_with_base_prices\":15,\"locations_count\":15,\"reference_time\":\"2025-10-05T10:22:24Z\",\"rows_to_generate\":15000,\"seed\":42}\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from shared.common.config import configure_logger, load_settings, load_config\n",
        "from shared.common.utils import (\n",
        "    canonical_json_dumps,\n",
        "    set_global_seed,\n",
        "    get_utc_now,\n",
        "    normalize_location_weights,\n",
        "    derive_city_mappings,\n",
        ")\n",
        "\n",
        "# 1) Logging setup\n",
        "logger = configure_logger(name=\"dataset_generator\", level=\"INFO\")\n",
        "\n",
        "# 2) Load central configuration (prefer typed PipelineConfig)\n",
        "try:\n",
        "    SETTINGS = load_settings(\"./dataset_config.yaml\")   # typed (PipelineConfig)\n",
        "    GEN = SETTINGS.generation                           # pydantic model (GenerationConfig)\n",
        "    GEN_CFG = GEN.model_dump()                          # plain dict for downstream functions\n",
        "except Exception:\n",
        "    # Fallback: dict-only config (backward compatibility)\n",
        "    CONFIG = load_config(\"./dataset_config.yaml\")\n",
        "    GEN_CFG = CONFIG.get(\"generation\", CONFIG) or {}\n",
        "\n",
        "# 3) Deterministic seeding\n",
        "SEED = int(GEN_CFG.get(\"seed\", 42))\n",
        "rng = set_global_seed(SEED)\n",
        "\n",
        "# 4) Reference time (UTC, ISO8601 con 'Z', no ms)\n",
        "REFERENCE_TIME = get_utc_now().replace(microsecond=0)\n",
        "REFERENCE_TIME_ISO = REFERENCE_TIME.isoformat().replace(\"+00:00\", \"Z\")\n",
        "\n",
        "# 5) Location weights\n",
        "location_weights_cfg = GEN_CFG.get(\"location_weights\", {}) or {}\n",
        "if not location_weights_cfg:\n",
        "    logger.error(\"location_weights missing in config.\")\n",
        "    raise ValueError(\"location_weights not defined\")\n",
        "normalized_location_weights = normalize_location_weights(location_weights_cfg)\n",
        "\n",
        "# 6) City mappings\n",
        "LOCATIONS, URBAN_TYPE_BY_CITY, REGION_BY_CITY = derive_city_mappings(\n",
        "    GEN_CFG,\n",
        "    urban_override=GEN_CFG.get(\"urban_type_by_city\"),\n",
        "    region_override=GEN_CFG.get(\"region_by_city\"),\n",
        ")\n",
        "\n",
        "# 7) Other derivates\n",
        "CITY_BASE_PRICES = GEN_CFG.get(\"city_base_prices\", {}) or {}\n",
        "SEASONALITY = GEN_CFG.get(\"seasonality\", {}) or {}\n",
        "ZONE_THRESHOLDS = GEN_CFG.get(\"zone_thresholds_km\", {\"center\": 1.5, \"semi_center\": 5.0}) or {}\n",
        "\n",
        "# Validation: thresholds (best-effort)\n",
        "if not {\"center\", \"semi_center\"} <= set(ZONE_THRESHOLDS.keys()):\n",
        "    raise ValueError(\"ZONE_THRESHOLDS incomplete: servono 'center' e 'semi_center'\")\n",
        "if float(ZONE_THRESHOLDS[\"center\"]) >= float(ZONE_THRESHOLDS[\"semi_center\"]):\n",
        "    raise ValueError(\"'center' threshold must be < 'semi_center'\")\n",
        "\n",
        "# 8) Summary\n",
        "summary = {\n",
        "    \"seed\": SEED,\n",
        "    \"reference_time\": REFERENCE_TIME_ISO,\n",
        "    \"locations_count\": len(LOCATIONS),\n",
        "    \"rows_to_generate\": int(GEN_CFG.get(\"n_rows\", 0)),\n",
        "    \"asset_type\": str(GEN_CFG.get(\"asset_type\", \"property\")),\n",
        "    \"cities_with_base_prices\": len(CITY_BASE_PRICES),\n",
        "}\n",
        "\n",
        "# 9) Log structured config summary\n",
        "logger.info(\"=\" * 60)\n",
        "logger.info(\"DATASET GENERATION CONFIGURATION\")\n",
        "logger.info(\"=\" * 60)\n",
        "logger.info(\"Config summary:\\n%s\", canonical_json_dumps(summary))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "486016bb-691b-4fba-990a-a5c11b6657d3",
      "metadata": {},
      "source": [
        "### Dataset generation and base enrichment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9fb3c590-d077-4ba7-b2fb-6cd174165cd9",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-10-05 12:22:24,858] INFO dataset_generator: Starting dataset generation...\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15000/15000 [00:30<00:00, 498.52it/s]\n",
            "[2025-10-05 12:22:55,088] INFO dataset_generator: âœ… Generated 15,000 records\n",
            "[2025-10-05 12:22:55,088] INFO dataset_generator: âœ… Quality report saved to outputs/quality_report.json\n",
            "[2025-10-05 12:22:55,135] INFO dataset_generator: âœ… Data types optimized: 6.70 MB â†’ 4.61 MB  (âˆ’2.09 MB, 31.2%)\n",
            "[2025-10-05 12:22:55,135] INFO dataset_generator: ðŸ Top risparmio per colonna (MB): {'listing_month': 0.072, 'size_m2': 0.072, 'has_elevator': 0.072, 'is_ground_floor': 0.072, 'is_top_floor': 0.072, 'building_floors': 0.072, 'floor': 0.072, 'age_years': 0.072}\n",
            "[2025-10-05 12:22:55,135] INFO dataset_generator: ðŸ”¤ Dtype cambiati (31): {'valuation_k': 'float64â†’float32', 'price_per_sqm': 'float64â†’float32', 'listing_month': 'int64â†’Int16', 'size_m2': 'int64â†’Int16', 'rooms': 'int64â†’Int16', 'bathrooms': 'int64â†’Int16', 'year_built': 'int64â†’Int16', 'age_years': 'int64â†’Int16'} (+23 altre)\n",
            "[2025-10-05 12:22:55,150] INFO dataset_generator: âœ… Dataset validation passed\n",
            "[2025-10-05 12:22:55,150] INFO dataset_generator: âœ… Validation report saved to outputs\\validation_report.json\n",
            "[2025-10-05 12:22:55,164] INFO dataset_generator: âœ… Columns reordered: 43 required + 0 optional\n",
            "[2025-10-05 12:22:55,166] INFO dataset_generator: â„¹ï¸ feature_order.json not found (C:\\Users\\anven\\OneDrive\\Documenti\\GitHub\\axiomatic_oracle\\notebooks\\modeling\\property\\feature_order.json); 'official'\n",
            "[2025-10-05 12:22:55,166] INFO dataset_generator: [UTILS] Distribution by location:\n",
            "location\n",
            "Milan       3017\n",
            "Rome        2700\n",
            "Turin       1214\n",
            "Naples      1190\n",
            "Bologna      886\n",
            "Genoa        773\n",
            "Florence     770\n",
            "Palermo      767\n",
            "Venice       596\n",
            "Bari         593\n",
            "Verona       591\n",
            "Padua        581\n",
            "Catania      444\n",
            "Cagliari     442\n",
            "Trieste      436\n",
            "[2025-10-05 12:22:55,166] INFO dataset_generator: [UTILS] Valuation min: 53.70kâ‚¬\n",
            "[2025-10-05 12:22:55,169] INFO dataset_generator: [UTILS] Valuation max: 2403.49kâ‚¬\n",
            "[2025-10-05 12:22:55,170] INFO dataset_generator: [UTILS] Valuation mean: 474.27kâ‚¬\n",
            "[2025-10-05 12:22:55,170] INFO dataset_generator: [UTILS] Corr(size_m2, valuation_k): 0.636\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "VALIDATION SUMMARY\n",
            "============================================================\n",
            "âœ… All required fields present\n",
            "ðŸ“Š Rows: 15,000\n",
            "ðŸ“Š Cols: 43\n",
            "âœ… Validation: PASSED\n",
            "\n",
            "============================================================\n",
            "DATASET GENERATION COMPLETED\n",
            "============================================================\n",
            "âœ… Records: 15,000\n",
            "âœ… Features: 43\n",
            "âœ… Memory: 4.61 MB\n",
            "âœ… Validation: PASSED\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from shared.n01_generate_dataset.dataset_builder import generate_dataset_df\n",
        "from shared.common.sanity_checks import validate_dataset\n",
        "from shared.common.schema import get_required_fields\n",
        "from shared.common.utils import (\n",
        "    canonical_json_dumps,\n",
        "    optimize_dtypes,\n",
        "    log_basic_diagnostics,\n",
        ")\n",
        "\n",
        "# 0) Sanity preliminare\n",
        "assert isinstance(GEN_CFG, dict) and int(GEN_CFG.get(\"n_rows\", 0)) > 0, \"generation.n_rows mancante o non valido\"\n",
        "\n",
        "# 1) Gen. dataset + quality_report\n",
        "logger.info(\"Starting dataset generation...\")\n",
        "df, quality_report = generate_dataset_df(\n",
        "    config=GEN_CFG,\n",
        "    locations=LOCATIONS,\n",
        "    urban_map=URBAN_TYPE_BY_CITY,\n",
        "    region_map=REGION_BY_CITY,\n",
        "    seasonality=SEASONALITY,\n",
        "    city_base_prices=CITY_BASE_PRICES,\n",
        "    rng=rng,\n",
        "    reference_time=REFERENCE_TIME,\n",
        "    batch_size=1000,\n",
        "    show_progress=True,\n",
        "    validate_each=True,\n",
        "    error_budget_pct=0.01,\n",
        ")\n",
        "logger.info(f\"âœ… Generated {len(df):,} records\")\n",
        "\n",
        "# Output dir\n",
        "Path(\"outputs\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Saves quality_report\n",
        "with open(\"outputs/quality_report.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(canonical_json_dumps(quality_report))\n",
        "logger.info(\"âœ… Quality report saved to outputs/quality_report.json\")\n",
        "\n",
        "# 3) Opt. types\n",
        "mem_before = df.memory_usage(deep=True).sum()\n",
        "mem_before_cols = df.memory_usage(deep=True, index=False)\n",
        "dtypes_before = df.dtypes.copy()\n",
        "\n",
        "df = optimize_dtypes(df)\n",
        "\n",
        "mem_after = df.memory_usage(deep=True).sum()\n",
        "mem_after_cols = df.memory_usage(deep=True, index=False)\n",
        "dtypes_after = df.dtypes\n",
        "\n",
        "saved_bytes = mem_before - mem_after\n",
        "saved_mb = saved_bytes / 1024**2\n",
        "pct_saved = (saved_bytes / mem_before * 100) if mem_before > 0 else 0.0\n",
        "\n",
        "logger.info(\n",
        "    \"âœ… Data types optimized: %.2f MB â†’ %.2f MB  (âˆ’%.2f MB, %.1f%%)\",\n",
        "    mem_before / 1024**2, mem_after / 1024**2, saved_mb, pct_saved\n",
        ")\n",
        "\n",
        "SHOW_TOP_SAVINGS = True\n",
        "TOP_N = 8\n",
        "if SHOW_TOP_SAVINGS:\n",
        "    diff = (mem_before_cols - mem_after_cols).sort_values(ascending=False)\n",
        "    top = {k: round(v / 1024**2, 3) for k, v in diff.head(TOP_N).items() if v > 0}\n",
        "    if top:\n",
        "        logger.info(\"ðŸ Top risparmio per colonna (MB): %s\", top)\n",
        "\n",
        "changed = [c for c in df.columns if dtypes_before.get(c) is not None and dtypes_before[c] != dtypes_after[c]]\n",
        "if changed:\n",
        "    preview = {c: f\"{dtypes_before[c]}â†’{dtypes_after[c]}\" for c in changed[:TOP_N]}\n",
        "    more = f\" (+{len(changed)-TOP_N} altre)\" if len(changed) > TOP_N else \"\"\n",
        "    logger.info(\"ðŸ”¤ Dtype cambiati (%d): %s%s\", len(changed), preview, more)\n",
        "\n",
        "# 4) Schema validation\n",
        "try:\n",
        "    validation_report = validate_dataset(\n",
        "        df,\n",
        "        asset_type=GEN_CFG.get(\"asset_type\", \"property\"),\n",
        "        raise_on_failure=True,\n",
        "    )\n",
        "    logger.info(\"âœ… Dataset validation passed\")\n",
        "except RuntimeError as e:\n",
        "    logger.error(f\"âŒ Validation failed: {e}\")\n",
        "    validation_report = {\n",
        "        \"overall_passed\": False,\n",
        "        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
        "        \"n_rows\": len(df),\n",
        "        \"n_cols\": df.shape[1],\n",
        "        \"error\": str(e),\n",
        "    }\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"VALIDATION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "if \"schema\" in validation_report:\n",
        "    missing = validation_report[\"schema\"].get(\"missing\", [])\n",
        "    if missing:\n",
        "        print(f\"âš ï¸ Missing fields: {missing}\")\n",
        "    else:\n",
        "        print(\"âœ… All required fields present\")\n",
        "print(f\"ðŸ“Š Rows: {validation_report.get('n_rows', len(df)):,}\")\n",
        "print(f\"ðŸ“Š Cols: {validation_report.get('n_cols', df.shape[1])}\")\n",
        "print(f\"âœ… Validation: {'PASSED' if validation_report.get('overall_passed') else 'FAILED'}\")\n",
        "\n",
        "report_path = Path(\"outputs/validation_report.json\")\n",
        "report_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "clean_report = validation_report\n",
        "\n",
        "try:\n",
        "    payload = canonical_json_dumps(clean_report)\n",
        "    with open(report_path, \"w\", encoding=\"utf-8\") as fp:\n",
        "        fp.write(payload)\n",
        "    logger.info(f\"âœ… Validation report saved to {report_path}\")\n",
        "except (TypeError, ValueError) as e:\n",
        "    logger.warning(f\"JSON encoder issue ({e}), applying fallback sanitization\")\n",
        "\n",
        "    def _convert_to_serializable(obj):\n",
        "        if isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        elif isinstance(obj, (np.integer, np.floating)):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, pd.Timestamp):\n",
        "            return obj.isoformat()\n",
        "        elif isinstance(obj, dict):\n",
        "            return {k: _convert_to_serializable(v) for k, v in obj.items()}\n",
        "        elif isinstance(obj, (list, tuple, set)):\n",
        "            return [_convert_to_serializable(item) for item in obj]\n",
        "        return obj\n",
        "\n",
        "    clean_report = _convert_to_serializable(validation_report)\n",
        "    with open(report_path, \"w\", encoding=\"utf-8\") as fp:\n",
        "        fp.write(canonical_json_dumps(clean_report))\n",
        "    logger.info(f\"âœ… Validation report (fallback) saved to {report_path}\")\n",
        "\n",
        "df.attrs[\"validation_timestamp\"] = validation_report.get(\"timestamp\")\n",
        "df.attrs[\"validation_passed\"] = bool(validation_report.get(\"overall_passed\", False))\n",
        "\n",
        "# 5) Column reorder\n",
        "required = get_required_fields(GEN_CFG.get(\"asset_type\", \"property\"))\n",
        "missing = [c for c in required if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing required fields: {missing}\")\n",
        "\n",
        "others = [c for c in df.columns if c not in required]\n",
        "df = df[required + others]\n",
        "logger.info(\"âœ… Columns reordered: %d required + %d optional\", len(required), len(others))\n",
        "\n",
        "# 6) Allineamento opzionale all'ordine feature ufficiale (se presente)\n",
        "ROOT = Path(\".\").resolve()\n",
        "NB_DIR = ROOT if ROOT.name == \"notebooks\" else (ROOT / \"notebooks\")\n",
        "FEATURES_PATH = NB_DIR / \"modeling\" / \"property\" / \"feature_order.json\"\n",
        "\n",
        "if FEATURES_PATH.exists():\n",
        "    feat_spec = json.loads(FEATURES_PATH.read_text(encoding=\"utf-8\"))\n",
        "    # Supporta sia {\"feature_order\":[...], \"dtypes\": {...}} sia una semplice lista\n",
        "    official = feat_spec.get(\"feature_order\") if isinstance(feat_spec, dict) else feat_spec\n",
        "    official = list(official) if official is not None else []\n",
        "\n",
        "    # Colonne da portare subito dopo le required\n",
        "    official_tail = [c for c in official if c not in required and c in df.columns]\n",
        "\n",
        "    # Il resto (ordine stabile, escludendo giÃ  prese)\n",
        "    taken = set(required) | set(official_tail)\n",
        "    others_tail = [c for c in df.columns if c not in taken]\n",
        "\n",
        "    df = df[required + official_tail + others_tail]\n",
        "\n",
        "    # (Opzionale) cast dtypes se specificati nella feature spec\n",
        "    dtypes_map = feat_spec.get(\"dtypes\", {}) if isinstance(feat_spec, dict) else {}\n",
        "    for col, dtype in dtypes_map.items():\n",
        "        if col in df.columns:\n",
        "            try:\n",
        "                df[col] = df[col].astype(dtype)\n",
        "            except Exception as e:\n",
        "                logger.warning(\"Cast dtype failed for  %sâ†’%s: %s\", col, dtype, e)\n",
        "\n",
        "    logger.info(\n",
        "        \"âœ… Columns realigned to official feature order: %d required + %d official + %d others\",\n",
        "        len(required), len(official_tail), len(others_tail)\n",
        "    )\n",
        "else:\n",
        "    logger.info(\"â„¹ï¸ feature_order.json not found (%s); 'official'\", FEATURES_PATH)\n",
        "\n",
        "# 7) Diagnostics rapidi\n",
        "log_basic_diagnostics(df, logger)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DATASET GENERATION COMPLETED\")\n",
        "print(\"=\"*60)\n",
        "print(f\"âœ… Records: {len(df):,}\")\n",
        "print(f\"âœ… Features: {df.shape[1]}\")\n",
        "print(f\"âœ… Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "print(f\"âœ… Validation: {'PASSED' if df.attrs.get('validation_passed') else 'FAILED'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff389f8d-0113-4101-88fe-7689bb8ed720",
      "metadata": {},
      "source": [
        "### Profiling & optimizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "73f2519f-6d09-4193-bab9-9e29cea4edac",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-10-05 12:22:55,243] INFO dataset_generator: Profiling report saved to: C:\\Users\\anven\\OneDrive\\Documenti\\GitHub\\axiomatic_oracle\\logs\\profiling_report.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ” Profiling dataset performance...\n",
            "\n",
            "=== MEMORY PROFILE ===\n",
            "Total memory: 3.62 MB\n",
            "Memory per row: 0.247 KB\n",
            "\n",
            "Memory by dtype:\n",
            "  object: 1.92 MB (52.9%) - 2 cols\n",
            "  category: 0.13 MB (3.7%) - 9 cols\n",
            "  float32: 0.52 MB (14.2%) - 9 cols\n",
            "  datetime64[ns, UTC]: 0.11 MB (3.2%) - 1 cols\n",
            "  Int16: 0.94 MB (26.0%) - 22 cols\n",
            "\n",
            "=== PERFORMANCE BENCHMARKS ===\n",
            "GroupBy (location, energy_class): 1.16 ms\n",
            "  â†’ 12,969,047 rows/sec\n",
            "Sort by valuation_k: 2.05 ms\n",
            "\n",
            "=== DTYPE OPTIMIZATION SUGGESTIONS ===\n",
            "  asset_type: object â†’ category (low cardinality (0.0%))\n",
            "  listing_month: Int16 â†’ uint8 (values in [10, 10])\n",
            "    â†’ Mem saving: 50%\n",
            "  size_m2: Int16 â†’ uint8 (values in [40, 199])\n",
            "    â†’ Mem saving: 50%\n",
            "  rooms: Int16 â†’ uint8 (values in [2, 7])\n",
            "    â†’ Mem saving: 50%\n",
            "  bathrooms: Int16 â†’ uint8 (values in [1, 3])\n",
            "    â†’ Mem saving: 50%\n",
            "  year_built: Int16 â†’ uint16 (values in [1950, 2023])\n",
            "    â†’ Mem saving: 0%\n",
            "  age_years: Int16 â†’ uint8 (values in [2, 75])\n",
            "    â†’ Mem saving: 50%\n",
            "  floor: Int16 â†’ uint8 (values in [0, 5])\n",
            "    â†’ Mem saving: 50%\n",
            "  building_floors: Int16 â†’ uint8 (values in [3, 10])\n",
            "    â†’ Mem saving: 50%\n",
            "  is_top_floor: Int16 â†’ uint8 (values in [0, 1])\n",
            "    â†’ Mem saving: 50%\n",
            "\n",
            "=== INDEX SUGGESTIONS ===\n",
            "  âœ… asset_id: excellent (unique identifier)\n",
            "  âŒ location: poor (selectivity=0.1%)\n",
            "  âŒ energy_class: poor (selectivity=0.0%)\n",
            "  âŒ zone: poor (selectivity=0.0%)\n",
            "  âŒ region: poor (selectivity=0.0%)\n",
            "\n",
            "ðŸ”§ Applying dtype optimizations...\n",
            "Applied: 23, Skipped: 0, Failed: 0\n",
            "âœ… Memory saved: 1.53 MB (42.1%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-10-05 12:22:55,405] INFO dataset_generator: âœ… Sanity checks passed; report saved in outputs\\sanity_report.json\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "from shared.common.performance_utils import DatasetProfiler, DtypeOptimizer\n",
        "from shared.common.reports import run_sanity_checks\n",
        "from shared.common.utils import canonical_json_dumps\n",
        "\n",
        "# 1) Pre-profiling casts & parsing (best effort)\n",
        "for col, dtype in {\n",
        "    \"price_per_sqm_capped\": \"float32\",\n",
        "    \"listing_quarter\": \"category\",\n",
        "    \"location_premium\": \"float32\",\n",
        "}.items():\n",
        "    if col in df.columns:\n",
        "        try:\n",
        "            df[col] = df[col].astype(dtype)\n",
        "        except Exception:\n",
        "            pass  # best-effort\n",
        "\n",
        "if \"last_verified_ts\" in df.columns:\n",
        "    df[\"last_verified_ts\"] = pd.to_datetime(df[\"last_verified_ts\"], utc=True, errors=\"coerce\")\n",
        "\n",
        "# 2) Inizializza profiler (usa default se non configurato)\n",
        "profiler = DatasetProfiler(\n",
        "    float_downcast_atol=1e-6,\n",
        "    float_downcast_rtol=1e-3,\n",
        ")\n",
        "\n",
        "# 3) Esegui profiling completo\n",
        "print(\"ðŸ” Profiling dataset performance...\")\n",
        "profile_results = profiler.profile(\n",
        "    df,\n",
        "    groupby_observed=bool(GEN_CFG.get(\"groupby_observed\", True)),\n",
        ")\n",
        "\n",
        "# 4) Report memoria\n",
        "mem = profile_results.get(\"memory\", {}) or {}\n",
        "print(\"\\n=== MEMORY PROFILE ===\")\n",
        "print(f\"Total memory: {mem.get('total_mb', 0.0):.2f} MB\")\n",
        "print(f\"Memory per row: {mem.get('per_row_kb', 0.0):.3f} KB\")\n",
        "print(\"\\nMemory by dtype:\")\n",
        "for dtype, info in (mem.get(\"by_dtype\", {}) or {}).items():\n",
        "    print(f\"  {dtype}: {info.get('mb', 0.0):.2f} MB ({info.get('pct', 0.0):.1f}%) - {info.get('n_columns', 0)} cols\")\n",
        "\n",
        "# 5) Benchmark performance\n",
        "perf = profile_results.get(\"performance\", {}) or {}\n",
        "print(\"\\n=== PERFORMANCE BENCHMARKS ===\")\n",
        "if \"groupby\" in perf and \"time_ms\" in perf[\"groupby\"]:\n",
        "    gb = perf[\"groupby\"]\n",
        "    cols = gb.get(\"columns\") or []\n",
        "    print(f\"GroupBy ({', '.join(cols) if cols else '?' }): {gb['time_ms']:.2f} ms\")\n",
        "    if gb.get(\"rows_per_sec\") is not None:\n",
        "        print(f\"  â†’ {gb['rows_per_sec']:,} rows/sec\")\n",
        "if \"sort\" in perf and \"time_ms\" in perf[\"sort\"]:\n",
        "    srt = perf[\"sort\"]\n",
        "    print(f\"Sort by {srt.get('column','?')}: {srt['time_ms']:.2f} ms\")\n",
        "\n",
        "# 6) Suggerimenti dtype\n",
        "dtype_sugg = profile_results.get(\"dtype_optimization\") or {}\n",
        "print(\"\\n=== DTYPE OPTIMIZATION SUGGESTIONS ===\")\n",
        "if dtype_sugg:\n",
        "    for col, opt in list(dtype_sugg.items())[:10]:\n",
        "        print(f\"  {col}: {opt.get('current','?')} â†’ {opt.get('target','?')} ({opt.get('reason','')})\")\n",
        "        if \"memory_reduction_pct\" in opt:\n",
        "            print(f\"    â†’ Mem saving: {opt['memory_reduction_pct']}%\")\n",
        "else:\n",
        "    print(\"No optimizations suggested\")\n",
        "\n",
        "# 7) Suggerimenti indici\n",
        "print(\"\\n=== INDEX SUGGESTIONS ===\")\n",
        "for idx in profile_results.get(\"index_suggestions\", []) or []:\n",
        "    mark = \"âœ…\" if idx.get(\"utility\") in {\"excellent\", \"good\"} else \"âŒ\"\n",
        "    print(f\"  {mark} {idx.get('column','?')}: {idx.get('utility','?')} ({idx.get('reason','')})\")\n",
        "\n",
        "# 8) Applica ottimizzazioni automatiche (facoltativo)\n",
        "AUTO_APPLY_OPTIMIZATIONS = True\n",
        "if AUTO_APPLY_OPTIMIZATIONS and dtype_sugg:\n",
        "    print(\"\\nðŸ”§ Applying dtype optimizations...\")\n",
        "    optimizer = DtypeOptimizer()\n",
        "    try:\n",
        "        df, opt_report = optimizer.apply(\n",
        "            df,\n",
        "            dtype_sugg,\n",
        "            inplace=False,\n",
        "        )\n",
        "    except TypeError:\n",
        "        # Fallback per versioni piÃ¹ vecchie (positional only)\n",
        "        df, opt_report = optimizer.apply(df, dtype_sugg, False)\n",
        "\n",
        "    summary = opt_report.get(\"summary\", {}) or {}\n",
        "    print(\n",
        "        f\"Applied: {summary.get('applied', 0)}, \"\n",
        "        f\"Skipped: {summary.get('skipped', 0)}, \"\n",
        "        f\"Failed: {summary.get('failed', 0)}\"\n",
        "    )\n",
        "\n",
        "    # Delta memoria rispetto al profilo pre-ottimizzazione\n",
        "    total_before = float(mem.get(\"total_mb\", 0.0) or 0.0)\n",
        "    new_mem_mb = df.memory_usage(deep=True).sum() / 1024**2\n",
        "    saved = total_before - new_mem_mb\n",
        "    if saved > 0:\n",
        "        base = total_before if total_before > 0 else 1e-9\n",
        "        print(f\"âœ… Memory saved: {saved:.2f} MB ({(saved / base) * 100:.1f}%)\")\n",
        "\n",
        "# 9) Salva profiling report (bugfix: scrive effettivamente su file)\n",
        "SAVE_PROFILING_REPORT = True\n",
        "if SAVE_PROFILING_REPORT:\n",
        "    log_dir = GEN_CFG.get(\"paths\", {}).get(\"log_dir\", \"./logs\")\n",
        "    report_path = Path(log_dir) / \"profiling_report.json\"\n",
        "    report_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(canonical_json_dumps(profile_results))\n",
        "    logger.info(\"Profiling report saved to: %s\", report_path)\n",
        "\n",
        "# =========================\n",
        "# SANITY BENCHMARKS & DRIFT\n",
        "# =========================\n",
        "sanity_report, df = run_sanity_checks(df, GEN_CFG)\n",
        "\n",
        "logs_dir = Path(\"./outputs\")\n",
        "logs_dir.mkdir(parents=True, exist_ok=True)\n",
        "sanity_path = logs_dir / \"sanity_report.json\"\n",
        "\n",
        "with sanity_path.open(\"w\", encoding=\"utf-8\") as fp:\n",
        "    fp.write(canonical_json_dumps(sanity_report))\n",
        "\n",
        "if not sanity_report.get(\"all_passed\", True):\n",
        "    raise RuntimeError(f\"Sanity checks failed â€“ see details in {sanity_path}\")\n",
        "\n",
        "logger.info(\"âœ… Sanity checks passed; report saved in %s\", sanity_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba65bab5-f410-4aa1-9634-4fb5682f7966",
      "metadata": {},
      "source": [
        "### Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4ffaeeb-ded7-4bbd-9943-4fe9c53a9bad",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-10-05 12:22:55,717] INFO dataset_generator: âœ… Saved dataset to C:\\Users\\anven\\OneDrive\\Documenti\\GitHub\\axiomatic_oracle\\notebooks\\outputs\\dataset_generated.csv\n",
            "[2025-10-05 12:22:55,717] INFO dataset_generator: âœ… Saved quality report JSON to C:\\Users\\anven\\OneDrive\\Documenti\\GitHub\\axiomatic_oracle\\notebooks\\outputs\\logs\\quality_report.json\n",
            "[2025-10-05 12:22:55,717] WARNING dataset_generator: PyYAML not available or failed; skipping YAML report.\n",
            "[2025-10-05 12:22:55,726] INFO dataset_generator: âœ… Saved top 30 outliers to C:\\Users\\anven\\OneDrive\\Documenti\\GitHub\\axiomatic_oracle\\notebooks\\outputs\\logs\\top_outliers.csv\n",
            "[2025-10-05 12:22:55,879] INFO dataset_generator: âœ… Saved manifest to C:\\Users\\anven\\OneDrive\\Documenti\\GitHub\\axiomatic_oracle\\notebooks\\outputs\\snapshots\\manifest_20251005T102255Z.json (hash=79e88bbabaae852e3d560715fa736053000e1c17945cac2403e0271849c938d4)\n",
            "[2025-10-05 12:22:55,884] INFO dataset_generator: Export summary saved to C:\\Users\\anven\\OneDrive\\Documenti\\GitHub\\axiomatic_oracle\\notebooks\\outputs\\logs\\export_summary.json\n",
            "[2025-10-05 12:22:55,884] INFO dataset_generator: Artifact written: C:\\Users\\anven\\OneDrive\\Documenti\\GitHub\\axiomatic_oracle\\notebooks\\outputs\\dataset_generated.csv (3.34 MB) â€“ rows=15,000, cols=45\n",
            "[2025-10-05 12:22:55,916] INFO dataset_generator: Export completed with manifest:\n",
            "[2025-10-05 12:22:55,916] INFO dataset_generator: {\"paths\":{\"dataset\":\"C:\\\\Users\\\\anven\\\\OneDrive\\\\Documenti\\\\GitHub\\\\axiomatic_oracle\\\\notebooks\\\\outputs\\\\dataset_generated.csv\",\"quality_report_json\":\"C:\\\\Users\\\\anven\\\\OneDrive\\\\Documenti\\\\GitHub\\\\axiomatic_oracle\\\\notebooks\\\\outputs\\\\logs\\\\quality_report.json\",\"top_outliers_csv\":\"C:\\\\Users\\\\anven\\\\OneDrive\\\\Documenti\\\\GitHub\\\\axiomatic_oracle\\\\notebooks\\\\outputs\\\\logs\\\\top_outliers.csv\"},\"summary\":{\"feature_order_sha256\":null,\"generated_at\":\"2025-10-05T10:22:55.864108Z\",\"manifest_hash\":\"79e88bbabaae852e3d560715fa736053000e1c17945cac2403e0271849c938d4\",\"rows\":15000,\"seed\":42,\"sha256\":\"f9f19f892f3d6491e6df665c32646137748730e951248f3db6253c034c1160cf\",\"ts_utc\":\"2025-10-05T10:22:55Z\"}}\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import re\n",
        "import hashlib\n",
        "from datetime import datetime, timezone\n",
        "from pathlib import Path\n",
        "\n",
        "from shared.n01_generate_dataset.exporter import export_dataset\n",
        "from shared.common.utils import canonical_json_dumps\n",
        "\n",
        "# ---- Path helpers (non-breaking): still under notebooks/ if relative ----\n",
        "ROOT = Path(\".\").resolve()\n",
        "NB_DIR = ROOT if ROOT.name == \"notebooks\" else (ROOT / \"notebooks\")\n",
        "\n",
        "def _resolve_under_nbdir(p: str | Path, default: str = \"outputs\") -> Path:\n",
        "    p = Path(str(p or default)).expanduser()\n",
        "    return p if p.is_absolute() else (NB_DIR / p)\n",
        "\n",
        "# === Desired output locations ===\n",
        "OUT_DIR = _resolve_under_nbdir(GEN_CFG.get(\"paths\", {}).get(\"out_dir\", \"outputs\"))\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "FILE_FORMAT = str(GEN_CFG.get(\"export_format\", \"csv\")).lower()\n",
        "assert FILE_FORMAT in {\"csv\", \"parquet\"}, f\"Unsupported export_format: {FILE_FORMAT}\"\n",
        "COMPRESSION = GEN_CFG.get(\"compression\", None if FILE_FORMAT == \"csv\" else \"snappy\")\n",
        "NO_OVERWRITE = bool(GEN_CFG.get(\"no_overwrite\", False))\n",
        "\n",
        "# ---- sanitized filename base ----\n",
        "_raw_base = str(GEN_CFG.get(\"filename_prefix\") or GEN_CFG.get(\"name\") or \"dataset_generated\")\n",
        "BASE_NAME = re.sub(r\"[^A-Za-z0-9._-]+\", \"_\", _raw_base).strip(\"._-\") or \"dataset_generated\"\n",
        "ext = \"parquet\" if FILE_FORMAT == \"parquet\" else \"csv\"\n",
        "output_path = OUT_DIR / f\"{BASE_NAME}.{ext}\"\n",
        "\n",
        "snapshot_dir = OUT_DIR / \"snapshots\"\n",
        "log_dir = OUT_DIR / \"logs\"\n",
        "snapshot_dir.mkdir(parents=True, exist_ok=True)\n",
        "log_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Build config to feed the exporter (non-breaking)\n",
        "EXPORT_CFG = {\n",
        "    **GEN_CFG,\n",
        "    \"paths\": {\n",
        "        **GEN_CFG.get(\"paths\", {}),\n",
        "        \"output_path\": str(output_path),\n",
        "        \"snapshot_dir\": str(snapshot_dir),\n",
        "        \"log_dir\": str(log_dir),\n",
        "    },\n",
        "}\n",
        "\n",
        "# === Export dataset ===\n",
        "# Use the rich reports created earlier: quality_report (cell 02), validation clean_report (cell 02),\n",
        "# and sanity_report (cell 03). Fall back gracefully if any is missing.\n",
        "combined_report = {\n",
        "    \"quality_report\": locals().get(\"quality_report\", {}),\n",
        "    \"validation_report\": locals().get(\"clean_report\", {}),\n",
        "    \"sanity_report\": locals().get(\"sanity_report\", {}),\n",
        "}\n",
        "\n",
        "manifest = export_dataset(\n",
        "    df=df,\n",
        "    config=EXPORT_CFG,\n",
        "    report=combined_report,\n",
        "    logger=logger,\n",
        "    format=FILE_FORMAT,\n",
        "    compression=COMPRESSION,\n",
        "    index=False,\n",
        "    no_overwrite=NO_OVERWRITE,\n",
        ")\n",
        "\n",
        "# ---- Enrich in-memory manifest view with a few convenient extras (no overwrite if already present) ----\n",
        "def _sha256(p: Path) -> str:\n",
        "    h = hashlib.sha256()\n",
        "    with p.open(\"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(65536), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "# Resolve dataset path from manifest (new exporter structure)\n",
        "ds_path = None\n",
        "try:\n",
        "    ds_path_str = (manifest or {}).get(\"paths\", {}).get(\"dataset\")\n",
        "    if ds_path_str:\n",
        "        ds_path = Path(ds_path_str)\n",
        "except Exception:\n",
        "    ds_path = None\n",
        "if not ds_path or not ds_path.exists():\n",
        "    # Fallback candidates under OUT_DIR\n",
        "    for cand in [output_path, OUT_DIR / f\"{BASE_NAME}.csv\", OUT_DIR / f\"{BASE_NAME}.parquet\"]:\n",
        "        if cand.exists():\n",
        "            ds_path = cand\n",
        "            break\n",
        "\n",
        "extras = {}\n",
        "try:\n",
        "    if ds_path and ds_path.exists():\n",
        "        extras[\"sha256\"] = _sha256(ds_path)\n",
        "        extras[\"rows\"] = int(len(df))\n",
        "        extras[\"cols\"] = list(map(str, df.columns))\n",
        "    # feature_order signature (optional)\n",
        "    feat_json = NB_DIR / \"modeling\" / \"property\" / \"feature_order.json\"\n",
        "    if feat_json.exists():\n",
        "        extras[\"feature_order_sha256\"] = _sha256(feat_json)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# add seed + utc timestamp\n",
        "extras[\"seed\"] = int(SEED)\n",
        "extras[\"ts_utc\"] = datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
        "\n",
        "if isinstance(manifest, dict):\n",
        "    for k, v in extras.items():\n",
        "        manifest.setdefault(k, v)\n",
        "\n",
        "# Persist an export summary alongside logs for convenience\n",
        "summary_path = log_dir / \"export_summary.json\"\n",
        "summary_path.write_text(canonical_json_dumps(manifest), encoding=\"utf-8\")\n",
        "logger.info(\"Export summary saved to %s\", summary_path)\n",
        "\n",
        "# Artifact logging\n",
        "try:\n",
        "    size_mb = (ds_path.stat().st_size / 1024**2) if (ds_path and ds_path.exists()) else 0.0\n",
        "    logger.info(\n",
        "        \"Artifact written: %s (%.2f MB) â€“ rows=%s, cols=%s\",\n",
        "        ds_path, size_mb, f\"{len(df):,}\", df.shape[1]\n",
        "    )\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Optional: quick descriptive snapshot\n",
        "try:\n",
        "    (df.describe(include=\"all\")\n",
        "       .T.head(20)\n",
        "       .to_csv(OUT_DIR / \"describe_snapshot.csv\", encoding=\"utf-8\"))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ------ Location drift check (from sanity_report built in Cell 03) ------\n",
        "drift_info = (combined_report.get(\"sanity_report\", {}) or {}).get(\"sanity_benchmarks\", {}) or {}\n",
        "drift_info = drift_info.get(\"location_drift\", {}) if isinstance(drift_info, dict) else {}\n",
        "\n",
        "tolerance = float(GEN_CFG.get(\"drift_tolerance\", 0.15))\n",
        "\n",
        "ANALYSIS_DIR = OUT_DIR / \"analysis\"\n",
        "ANALYSIS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "(ANALYSIS_DIR / \"location_drift_generation.json\").write_text(\n",
        "    canonical_json_dumps(drift_info),\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "violating = []\n",
        "if isinstance(drift_info, dict) and \"by_location\" in drift_info:\n",
        "    for loc, info in (drift_info.get(\"by_location\") or {}).items():\n",
        "        try:\n",
        "            diff = float(info.get(\"difference\", 0.0))\n",
        "            if abs(diff) > tolerance:\n",
        "                violating.append(loc)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "if violating:\n",
        "    raise ValueError(f\"Excessive location drift for: {violating} (tolerance Â±{tolerance})\")\n",
        "\n",
        "# ------ Manifest recap in logs (new schema-aware) ------\n",
        "wanted = [\n",
        "    \"generated_at\", \"manifest_hash\", \"sha256\", \"feature_order_sha256\",\n",
        "    \"rows\", \"seed\", \"ts_utc\"\n",
        "]\n",
        "manifest_summary = {k: (manifest or {}).get(k) for k in wanted}\n",
        "paths_summary = (manifest or {}).get(\"paths\", {})\n",
        "logger.info(\"Export completed with manifest:\")\n",
        "logger.info(canonical_json_dumps({\"summary\": manifest_summary, \"paths\": paths_summary}))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
