{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b756d33a-e242-4a4c-b988-3c0bb1f0a764",
      "metadata": {},
      "source": [
        "### Base config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "67fe5d12-8cf5-4cee-9ac2-a0cdcc0b0ad0",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-08-21 08:40:21,065] INFO dataset_generator: ============================================================\n",
            "[2025-08-21 08:40:21,066] INFO dataset_generator: DATASET GENERATION CONFIGURATION\n",
            "[2025-08-21 08:40:21,067] INFO dataset_generator: ============================================================\n",
            "[2025-08-21 08:40:21,068] INFO dataset_generator: Config summary:\n",
            "{\"asset_type\":\"property\",\"cities_with_base_prices\":15,\"locations_count\":15,\"reference_time\":\"2025-08-21T06:40:21Z\",\"rows_to_generate\":15000,\"seed\":42}\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from notebooks.shared.common.config import configure_logger, load_settings, load_config\n",
        "from notebooks.shared.common.utils import (\n",
        "    canonical_json_dumps,\n",
        "    set_global_seed,\n",
        "    get_utc_now,\n",
        "    normalize_location_weights,\n",
        "    derive_city_mappings,\n",
        ")\n",
        "\n",
        "\n",
        "# 1) Logging setup\n",
        "logger = configure_logger(name=\"dataset_generator\", level=\"INFO\")\n",
        "\n",
        "\n",
        "# 2) Load central configuration (typed -> fallback dict)\n",
        "try:\n",
        "    SETTINGS = load_settings(\"./dataset_config.yaml\")     # preferred (typed)\n",
        "    CONFIG = SETTINGS.to_dict() if hasattr(SETTINGS, \"to_dict\") else dict(SETTINGS)\n",
        "except Exception:\n",
        "    CONFIG = load_config(\"./dataset_config.yaml\")         # fallback (dict)\n",
        "\n",
        "# Recupera blocco \"generation\" o fallback flat (retrocompatibilit√†)\n",
        "GEN_CFG = CONFIG.get(\"generation\", CONFIG) or {}\n",
        "\n",
        "# Timestamps canonici (UTC 'Z', senza microsecondi)\n",
        "REFERENCE_TIME = get_utc_now().replace(microsecond=0)\n",
        "REFERENCE_TIME_ISO = REFERENCE_TIME.isoformat().replace(\"+00:00\", \"Z\")\n",
        "\n",
        "# 3) Seeding deterministico\n",
        "SEED = int(GEN_CFG.get(\"seed\", CONFIG.get(\"seed\", 42)))\n",
        "rng = set_global_seed(SEED)\n",
        "\n",
        "# 4) Reference time\n",
        "REFERENCE_TIME = get_utc_now()\n",
        "\n",
        "# 5) Location weights ‚Äî normalizzazione e validazione\n",
        "location_weights_cfg = GEN_CFG.get(\"location_weights\", {}) or {}\n",
        "if not location_weights_cfg:\n",
        "    logger.error(\"location_weights mancante nella config.\")\n",
        "    raise ValueError(\"location_weights non definiti\")\n",
        "\n",
        "normalized_location_weights = normalize_location_weights(location_weights_cfg)\n",
        "\n",
        "\n",
        "# 6) City mappings\n",
        "LOCATIONS, URBAN_TYPE_BY_CITY, REGION_BY_CITY = derive_city_mappings(\n",
        "    GEN_CFG,\n",
        "    urban_override=GEN_CFG.get(\"urban_type_by_city\"),\n",
        "    region_override=GEN_CFG.get(\"region_by_city\"),\n",
        ")\n",
        "\n",
        "\n",
        "# 7) Altri derivati da config\n",
        "CITY_BASE_PRICES = GEN_CFG.get(\"city_base_prices\", {}) or {}\n",
        "SEASONALITY = GEN_CFG.get(\"seasonality\", {}) or {}\n",
        "ZONE_THRESHOLDS = GEN_CFG.get(\"zone_thresholds_km\", {\"center\": 1.5, \"semi_center\": 5.0}) or {}\n",
        "\n",
        "# Validazione thresholds (best-effort, extra rispetto ai modelli)\n",
        "if not {\"center\", \"semi_center\"} <= set(ZONE_THRESHOLDS.keys()):\n",
        "    raise ValueError(\"ZONE_THRESHOLDS incomplete: servono 'center' e 'semi_center'\")\n",
        "if float(ZONE_THRESHOLDS[\"center\"]) >= float(ZONE_THRESHOLDS[\"semi_center\"]):\n",
        "    raise ValueError(\"'center' threshold deve essere < 'semi_center'\")\n",
        "\n",
        "\n",
        "# 8) Flatten config per retrocompatibilit√†\n",
        "FLAT_CONFIG = {**CONFIG, **GEN_CFG}\n",
        "FLAT_CONFIG[\"location_weights\"] = dict(location_weights_cfg)\n",
        "\n",
        "obj = {\n",
        "            \"seed\": SEED,\n",
        "            \"reference_time\": REFERENCE_TIME_ISO,\n",
        "            \"locations_count\": len(LOCATIONS),\n",
        "            \"rows_to_generate\": int(FLAT_CONFIG.get(\"n_rows\", GEN_CFG.get(\"n_rows\", 0)) or 0),\n",
        "            \"asset_type\": str(FLAT_CONFIG.get(\"asset_type\", GEN_CFG.get(\"asset_type\", \"property\"))),\n",
        "            \"cities_with_base_prices\": len(CITY_BASE_PRICES),\n",
        "        }\n",
        "\n",
        "# 9) Log structured config summary\n",
        "logger.info(\"=\" * 60)\n",
        "logger.info(\"DATASET GENERATION CONFIGURATION\")\n",
        "logger.info(\"=\" * 60)\n",
        "logger.info(\n",
        "    \"Config summary:\\n%s\",\n",
        "    canonical_json_dumps(obj)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "486016bb-691b-4fba-990a-a5c11b6657d3",
      "metadata": {},
      "source": [
        "### Dataset generation and base enrichment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fb3c590-d077-4ba7-b2fb-6cd174165cd9",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-08-21 08:40:21,169] INFO dataset_generator: Starting dataset generation...\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15000/15000 [00:16<00:00, 909.01it/s]\n",
            "[2025-08-21 08:40:38,785] INFO dataset_generator: ‚úÖ Generated 15,000 records\n",
            "[2025-08-21 08:40:38,791] INFO dataset_generator: ‚úÖ Quality report saved to outputs/quality_report.json\n",
            "[2025-08-21 08:40:38,929] INFO dataset_generator: ‚úÖ Data types optimized: 6.70 MB ‚Üí 4.61 MB  (‚àí2.09 MB, 31.2%)\n",
            "[2025-08-21 08:40:38,930] INFO dataset_generator: üèÅ Top risparmio per colonna (MB): {'age_years': 0.072, 'garage': 0.072, 'year_built': 0.072, 'size_m2': 0.072, 'rooms': 0.072, 'public_transport_nearby': 0.072, 'parking_spot': 0.072, 'owner_occupied': 0.072}\n",
            "[2025-08-21 08:40:38,933] INFO dataset_generator: üî§ Dtype cambiati (31): {'age_years': 'int64‚ÜíInt16', 'air_quality_index': 'int64‚ÜíInt16', 'attic': 'int64‚ÜíInt16', 'bathrooms': 'int64‚ÜíInt16', 'building_floors': 'int64‚ÜíInt16', 'cellar': 'int64‚ÜíInt16', 'concierge': 'int64‚ÜíInt16', 'condition_score': 'float64‚Üífloat32'} (+23 altre)\n",
            "[2025-08-21 08:40:38,980] INFO dataset_generator: ‚úÖ Dataset validation passed\n",
            "[2025-08-21 08:40:38,984] INFO dataset_generator: ‚úÖ Validation report saved to outputs\\validation_report.json\n",
            "[2025-08-21 08:40:38,992] INFO dataset_generator: ‚úÖ Columns reordered: 43 required + 0 optional\n",
            "[2025-08-21 08:40:38,996] INFO dataset_generator: [UTILS] Distribuzione per location:\n",
            "location\n",
            "Milan       3017\n",
            "Rome        2700\n",
            "Turin       1214\n",
            "Naples      1190\n",
            "Bologna      886\n",
            "Genoa        773\n",
            "Florence     770\n",
            "Palermo      767\n",
            "Venice       596\n",
            "Bari         593\n",
            "Verona       591\n",
            "Padua        581\n",
            "Catania      444\n",
            "Cagliari     442\n",
            "Trieste      436\n",
            "[2025-08-21 08:40:38,997] INFO dataset_generator: [UTILS] Prezzo min: 47.39k‚Ç¨\n",
            "[2025-08-21 08:40:38,998] INFO dataset_generator: [UTILS] Prezzo max: 2304.21k‚Ç¨\n",
            "[2025-08-21 08:40:39,000] INFO dataset_generator: [UTILS] Prezzo medio: 472.44k‚Ç¨\n",
            "[2025-08-21 08:40:39,018] INFO dataset_generator: [UTILS] Corr size_m2 vs valuation_k: 0.630\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "VALIDATION SUMMARY\n",
            "============================================================\n",
            "‚úÖ All required fields present\n",
            "üìä Rows: 15,000\n",
            "üìä Cols: 43\n",
            "‚úÖ Validation: PASSED\n",
            "\n",
            "============================================================\n",
            "DATASET GENERATION COMPLETED\n",
            "============================================================\n",
            "‚úÖ Records: 15,000\n",
            "‚úÖ Features: 43\n",
            "‚úÖ Memory: 4.61 MB\n",
            "‚úÖ Validation: PASSED\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from notebooks.shared.n01_generate_dataset.dataset_builder import generate_dataset_df\n",
        "from notebooks.shared.common.sanity_checks import validate_dataset\n",
        "from notebooks.shared.common.schema import get_required_fields\n",
        "from notebooks.shared.common.utils import (\n",
        "    NumpyJSONEncoder,\n",
        "    optimize_dtypes,\n",
        "    log_basic_diagnostics,\n",
        ")\n",
        "\n",
        "# 0) Sanity preliminare\n",
        "GEN_CFG = CONFIG.get(\"generation\", CONFIG)\n",
        "assert isinstance(GEN_CFG, dict) and int(GEN_CFG.get(\"n_rows\", 0)) > 0, \"generation.n_rows mancante o non valido\"\n",
        "\n",
        "# 1) Generazione dataset + quality_report\n",
        "logger.info(\"Starting dataset generation...\")\n",
        "df, quality_report = generate_dataset_df(\n",
        "    config=GEN_CFG,                 # usa il blocco generation, NON CONFIG\n",
        "    locations=LOCATIONS,\n",
        "    urban_map=URBAN_TYPE_BY_CITY,\n",
        "    region_map=REGION_BY_CITY,\n",
        "    seasonality=SEASONALITY,\n",
        "    city_base_prices=CITY_BASE_PRICES,\n",
        "    rng=rng,\n",
        "    reference_time=REFERENCE_TIME,\n",
        "    batch_size=1000,\n",
        "    show_progress=True,\n",
        "    validate_each=True,\n",
        "    error_budget_pct=0.01,\n",
        ")\n",
        "logger.info(f\"‚úÖ Generated {len(df):,} records\")\n",
        "\n",
        "# Output dir\n",
        "Path(\"outputs\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Salva quality_report per analisi in Notebook 02\n",
        "with open(\"outputs/quality_report.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    canonical_json_dumps(quality_report)\n",
        "logger.info(\"‚úÖ Quality report saved to outputs/quality_report.json\")\n",
        "\n",
        "# 2) (Intenzionalmente niente enforce dei domini categoriali in nb01)\n",
        "\n",
        "# 3) Ottimizzazione tipi + log memoria risparmiata\n",
        "mem_before = df.memory_usage(deep=True).sum()\n",
        "mem_before_cols = df.memory_usage(deep=True, index=False)\n",
        "dtypes_before = df.dtypes.copy()\n",
        "\n",
        "df = optimize_dtypes(df)\n",
        "\n",
        "mem_after = df.memory_usage(deep=True).sum()\n",
        "mem_after_cols = df.memory_usage(deep=True, index=False)\n",
        "dtypes_after = df.dtypes\n",
        "\n",
        "saved_bytes = mem_before - mem_after\n",
        "saved_mb = saved_bytes / 1024**2\n",
        "pct_saved = (saved_bytes / mem_before * 100) if mem_before > 0 else 0.0\n",
        "\n",
        "logger.info(\n",
        "    \"‚úÖ Data types optimized: %.2f MB ‚Üí %.2f MB  (‚àí%.2f MB, %.1f%%)\",\n",
        "    mem_before / 1024**2, mem_after / 1024**2, saved_mb, pct_saved\n",
        ")\n",
        "\n",
        "# (Opzionale) Top colonne per risparmio memoria\n",
        "SHOW_TOP_SAVINGS = True\n",
        "TOP_N = 8\n",
        "if SHOW_TOP_SAVINGS:\n",
        "    diff = (mem_before_cols - mem_after_cols).sort_values(ascending=False)\n",
        "    top = {k: round(v / 1024**2, 3) for k, v in diff.head(TOP_N).items() if v > 0}\n",
        "    if top:\n",
        "        logger.info(\"üèÅ Top risparmio per colonna (MB): %s\", top)\n",
        "\n",
        "# (Opzionale) Log dei dtype cambiati\n",
        "changed = [c for c in df.columns if dtypes_before.get(c) is not None and dtypes_before[c] != dtypes_after[c]]\n",
        "if changed:\n",
        "    preview = {c: f\"{dtypes_before[c]}‚Üí{dtypes_after[c]}\" for c in changed[:TOP_N]}\n",
        "    more = f\" (+{len(changed)-TOP_N} altre)\" if len(changed) > TOP_N else \"\"\n",
        "    logger.info(\"üî§ Dtype cambiati (%d): %s%s\", len(changed), preview, more)\n",
        "\n",
        "# 4) Validazione schema & robust JSON dump\n",
        "try:\n",
        "    validation_report = validate_dataset(\n",
        "        df,\n",
        "        asset_type=GEN_CFG.get(\"asset_type\", \"property\"),  # usa GEN_CFG\n",
        "        raise_on_failure=True,\n",
        "    )\n",
        "    logger.info(\"‚úÖ Dataset validation passed\")\n",
        "except RuntimeError as e:\n",
        "    logger.error(f\"‚ùå Validation failed: {e}\")\n",
        "    validation_report = {\n",
        "        \"overall_passed\": False,\n",
        "        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
        "        \"n_rows\": len(df),\n",
        "        \"n_cols\": df.shape[1],\n",
        "        \"error\": str(e),\n",
        "    }\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"VALIDATION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "if \"schema\" in validation_report:\n",
        "    missing = validation_report[\"schema\"].get(\"missing\", [])\n",
        "    if missing:\n",
        "        print(f\"‚ö†Ô∏è Missing fields: {missing}\")\n",
        "    else:\n",
        "        print(\"‚úÖ All required fields present\")\n",
        "print(f\"üìä Rows: {validation_report.get('n_rows', len(df)):,}\")\n",
        "print(f\"üìä Cols: {validation_report.get('n_cols', df.shape[1])}\")\n",
        "print(f\"‚úÖ Validation: {'PASSED' if validation_report.get('overall_passed') else 'FAILED'}\")\n",
        "\n",
        "report_path = Path(\"outputs/validation_report.json\")\n",
        "report_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "clean_report = validation_report\n",
        "\n",
        "try:\n",
        "    # serializza e SCRIVE (prima mancava fp.write)\n",
        "    payload = canonical_json_dumps(clean_report)\n",
        "    with open(report_path, \"w\", encoding=\"utf-8\") as fp:\n",
        "        fp.write(payload)\n",
        "    logger.info(f\"‚úÖ Validation report saved to {report_path}\")\n",
        "except (TypeError, ValueError) as e:\n",
        "    logger.warning(f\"JSON encoder issue ({e}), applying fallback sanitization\")\n",
        "\n",
        "    def _convert_to_serializable(obj):\n",
        "        if isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        elif isinstance(obj, (np.integer, np.floating)):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, pd.Timestamp):\n",
        "            # ISO 8601; se vuoi, .tz_convert('UTC') e poi .isoformat().replace('+00:00','Z')\n",
        "            return obj.isoformat()\n",
        "        elif isinstance(obj, dict):\n",
        "            return {k: _convert_to_serializable(v) for k, v in obj.items()}\n",
        "        elif isinstance(obj, (list, tuple, set)):\n",
        "            return [_convert_to_serializable(item) for item in obj]\n",
        "        return obj\n",
        "\n",
        "    clean_report = _convert_to_serializable(validation_report)\n",
        "    with open(report_path, \"w\", encoding=\"utf-8\") as fp:\n",
        "        fp.write(canonical_json_dumps(clean_report))\n",
        "    logger.info(f\"‚úÖ Validation report (fallback) saved to {report_path}\")\n",
        "\n",
        "df.attrs[\"validation_timestamp\"] = validation_report.get(\"timestamp\")\n",
        "df.attrs[\"validation_passed\"] = bool(validation_report.get(\"overall_passed\", False))\n",
        "\n",
        "# 5) Riordino colonne secondo schema\n",
        "required = get_required_fields(GEN_CFG.get(\"asset_type\", \"property\"))  # usa GEN_CFG\n",
        "missing = [c for c in required if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Campi richiesti mancanti: {missing}\")\n",
        "\n",
        "others = [c for c in df.columns if c not in required]\n",
        "df = df[required + others]\n",
        "logger.info(\"‚úÖ Columns reordered: %d required + %d optional\", len(required), len(others))\n",
        "\n",
        "# 6)\n",
        "ROOT = Path(\".\").resolve()\n",
        "NB_DIR = ROOT if ROOT.name == \"notebooks\" else (ROOT / \"notebooks\")\n",
        "FEATURES_PATH = NB_DIR / \"modeling\" / \"property\" / \"feature_order.json\"\n",
        "\n",
        "if FEATURES_PATH.exists():\n",
        "    feat_spec = json.loads(FEATURES_PATH.read_text(encoding=\"utf-8\"))\n",
        "    # Supporta sia {\"feature_order\":[...], \"dtypes\": {...}} sia una semplice lista\n",
        "    official = feat_spec.get(\"feature_order\") if isinstance(feat_spec, dict) else feat_spec\n",
        "    official = list(official) if official is not None else []\n",
        "\n",
        "    # Colonne da portare subito dopo le required\n",
        "    official_tail = [c for c in official if c not in required and c in df.columns]\n",
        "\n",
        "    # Il resto (ordina stabilmente tenendo l'ordine corrente, escludendo gi√† presi)\n",
        "    taken = set(required) | set(official_tail)\n",
        "    others_tail = [c for c in df.columns if c not in taken]\n",
        "\n",
        "    df = df[required + official_tail + others_tail]\n",
        "\n",
        "    # (Opzionale) cast dtypes se specificati nella feature spec\n",
        "    dtypes_map = feat_spec.get(\"dtypes\", {}) if isinstance(feat_spec, dict) else {}\n",
        "    for col, dtype in dtypes_map.items():\n",
        "        if col in df.columns:\n",
        "            try:\n",
        "                df[col] = df[col].astype(dtype)\n",
        "            except Exception as e:\n",
        "                logger.warning(\"Cast dtype fallito per %s‚Üí%s: %s\", col, dtype, e)\n",
        "\n",
        "    logger.info(\"‚úÖ Columns realigned to official feature order: %d required + %d official + %d others\",\n",
        "                len(required), len(official_tail), len(others_tail))\n",
        "else:\n",
        "    logger.info(\"‚ÑπÔ∏è feature_order.json non trovato (%s): salto realineamento 'official'\", FEATURES_PATH)\n",
        "\n",
        "# 7) Diagnostics rapidi\n",
        "log_basic_diagnostics(df, logger)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DATASET GENERATION COMPLETED\")\n",
        "print(\"=\"*60)\n",
        "print(f\"‚úÖ Records: {len(df):,}\")\n",
        "print(f\"‚úÖ Features: {df.shape[1]}\")\n",
        "print(f\"‚úÖ Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "print(f\"‚úÖ Validation: {'PASSED' if df.attrs.get('validation_passed') else 'FAILED'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff389f8d-0113-4101-88fe-7689bb8ed720",
      "metadata": {},
      "source": [
        "### Profiling & optimizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "73f2519f-6d09-4193-bab9-9e29cea4edac",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-08-21 08:40:39,389] INFO dataset_generator: Profiling report saved to: C:\\Users\\Utente\\Desktop\\Projects\\ai_oracle_rwa\\logs\\profiling_report.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Profiling dataset performance...\n",
            "\n",
            "=== MEMORY PROFILE ===\n",
            "Total memory: 3.62 MB\n",
            "Memory per row: 0.247 KB\n",
            "\n",
            "Memory by dtype:\n",
            "  object: 1.92 MB (52.9%) - 2 cols\n",
            "  category: 0.13 MB (3.7%) - 9 cols\n",
            "  float32: 0.52 MB (14.2%) - 9 cols\n",
            "  datetime64[ns, UTC]: 0.11 MB (3.2%) - 1 cols\n",
            "  Int16: 0.94 MB (26.0%) - 22 cols\n",
            "\n",
            "=== PERFORMANCE BENCHMARKS ===\n",
            "GroupBy (location, energy_class): 42.22 ms\n",
            "  ‚Üí 355,256 rows/sec\n",
            "Sort by valuation_k: 8.47 ms\n",
            "\n",
            "=== DTYPE OPTIMIZATION SUGGESTIONS ===\n",
            "  asset_type: object ‚Üí category (low cardinality (0.0%))\n",
            "  listing_month: Int16 ‚Üí uint8 (values in [8, 8])\n",
            "    ‚Üí Mem saving: 50%\n",
            "  size_m2: Int16 ‚Üí uint8 (values in [40, 199])\n",
            "    ‚Üí Mem saving: 50%\n",
            "  rooms: Int16 ‚Üí uint8 (values in [2, 6])\n",
            "    ‚Üí Mem saving: 50%\n",
            "  bathrooms: Int16 ‚Üí uint8 (values in [1, 3])\n",
            "    ‚Üí Mem saving: 50%\n",
            "  year_built: Int16 ‚Üí uint16 (values in [1950, 2023])\n",
            "    ‚Üí Mem saving: 0%\n",
            "  age_years: Int16 ‚Üí uint8 (values in [2, 75])\n",
            "    ‚Üí Mem saving: 50%\n",
            "  floor: Int16 ‚Üí uint8 (values in [0, 4])\n",
            "    ‚Üí Mem saving: 50%\n",
            "  building_floors: Int16 ‚Üí uint8 (values in [1, 9])\n",
            "    ‚Üí Mem saving: 50%\n",
            "  is_top_floor: Int16 ‚Üí uint8 (values in [0, 1])\n",
            "    ‚Üí Mem saving: 50%\n",
            "\n",
            "=== INDEX SUGGESTIONS ===\n",
            "  ‚úÖ asset_id: excellent (unique identifier)\n",
            "  ‚ùå location: poor (selectivity=0.1%)\n",
            "  ‚ùå energy_class: poor (selectivity=0.0%)\n",
            "  ‚ùå zone: poor (selectivity=0.0%)\n",
            "  ‚ùå region: poor (selectivity=0.0%)\n",
            "\n",
            "üîß Applying dtype optimizations...\n",
            "Applied: 23, Skipped: 0, Failed: 0\n",
            "‚úÖ Memory saved: 1.53 MB (42.1%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-08-21 08:40:40,030] INFO dataset_generator: ‚úÖ Sanity checks passed; report salvato in outputs\\sanity_report.json\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from notebooks.shared.common.performance_utils import DatasetProfiler, DtypeOptimizer\n",
        "from notebooks.shared.common.reports import run_sanity_checks\n",
        "from notebooks.shared.common.utils import NumpyJSONEncoder\n",
        "\n",
        "# 1. Pre-profiling casts & parsing (best effort)\n",
        "for col, dtype in {\n",
        "    \"price_per_sqm_capped\": \"float32\",\n",
        "    \"listing_quarter\": \"category\",\n",
        "    \"location_premium\": \"float32\",\n",
        "}.items():\n",
        "    if col in df.columns:\n",
        "        try:\n",
        "            df[col] = df[col].astype(dtype)\n",
        "        except Exception:\n",
        "            pass  # best-effort\n",
        "\n",
        "if \"last_verified_ts\" in df.columns:\n",
        "    df[\"last_verified_ts\"] = pd.to_datetime(df[\"last_verified_ts\"], utc=True, errors=\"coerce\")\n",
        "\n",
        "# 2. Inizializza profiler (usa default se non configurato)\n",
        "profiler = DatasetProfiler(\n",
        "    float_downcast_atol=1e-6,\n",
        "    float_downcast_rtol=1e-3,\n",
        ")\n",
        "\n",
        "# 3. Esegui profiling completo\n",
        "print(\"üîç Profiling dataset performance...\")\n",
        "profile_results = profiler.profile(\n",
        "    df,\n",
        "    groupby_observed=GEN_CFG.get(\"groupby_observed\", True),\n",
        ")\n",
        "\n",
        "# 4. Report memoria\n",
        "mem = profile_results.get(\"memory\", {}) or {}\n",
        "print(\"\\n=== MEMORY PROFILE ===\")\n",
        "print(f\"Total memory: {mem.get('total_mb', 0.0):.2f} MB\")\n",
        "print(f\"Memory per row: {mem.get('per_row_kb', 0.0):.3f} KB\")\n",
        "print(\"\\nMemory by dtype:\")\n",
        "for dtype, info in (mem.get(\"by_dtype\", {}) or {}).items():\n",
        "    print(f\"  {dtype}: {info.get('mb', 0.0):.2f} MB ({info.get('pct', 0.0):.1f}%) - {info.get('n_columns', 0)} cols\")\n",
        "\n",
        "# 5. Benchmark performance\n",
        "perf = profile_results.get(\"performance\", {}) or {}\n",
        "print(\"\\n=== PERFORMANCE BENCHMARKS ===\")\n",
        "if \"groupby\" in perf and \"time_ms\" in perf[\"groupby\"]:\n",
        "    gb = perf[\"groupby\"]\n",
        "    cols = gb.get(\"columns\") or []\n",
        "    print(f\"GroupBy ({', '.join(cols) if cols else '?' }): {gb['time_ms']:.2f} ms\")\n",
        "    if gb.get(\"rows_per_sec\") is not None:\n",
        "        print(f\"  ‚Üí {gb['rows_per_sec']:,} rows/sec\")\n",
        "if \"sort\" in perf and \"time_ms\" in perf[\"sort\"]:\n",
        "    srt = perf[\"sort\"]\n",
        "    print(f\"Sort by {srt.get('column','?')}: {srt['time_ms']:.2f} ms\")\n",
        "\n",
        "# 6. Suggerimenti dtype\n",
        "dtype_sugg = profile_results.get(\"dtype_optimization\") or {}\n",
        "print(\"\\n=== DTYPE OPTIMIZATION SUGGESTIONS ===\")\n",
        "if dtype_sugg:\n",
        "    for col, opt in list(dtype_sugg.items())[:10]:\n",
        "        print(f\"  {col}: {opt.get('current','?')} ‚Üí {opt.get('target','?')} ({opt.get('reason','')})\")\n",
        "        if \"memory_reduction_pct\" in opt:\n",
        "            print(f\"    ‚Üí Mem saving: {opt['memory_reduction_pct']}%\")\n",
        "else:\n",
        "    print(\"  No optimizations suggested\")\n",
        "\n",
        "# 7. Suggerimenti indici\n",
        "print(\"\\n=== INDEX SUGGESTIONS ===\")\n",
        "for idx in profile_results.get(\"index_suggestions\", []) or []:\n",
        "    mark = \"‚úÖ\" if idx.get(\"utility\") in {\"excellent\", \"good\"} else \"‚ùå\"\n",
        "    print(f\"  {mark} {idx.get('column','?')}: {idx.get('utility','?')} ({idx.get('reason','')})\")\n",
        "\n",
        "# 8. Applica ottimizzazioni automatiche (facoltativo)\n",
        "AUTO_APPLY_OPTIMIZATIONS = True\n",
        "if AUTO_APPLY_OPTIMIZATIONS and dtype_sugg:\n",
        "    print(\"\\nüîß Applying dtype optimizations...\")\n",
        "    optimizer = DtypeOptimizer()\n",
        "    try:\n",
        "        # API corretta: 'inplace'\n",
        "        df, opt_report = optimizer.apply(\n",
        "            df,\n",
        "            dtype_sugg,\n",
        "            inplace=False,  # <-- fix qui\n",
        "        )\n",
        "    except TypeError:\n",
        "        # Fallback per versioni pi√π vecchie (positional only)\n",
        "        df, opt_report = optimizer.apply(df, dtype_sugg, False)\n",
        "\n",
        "    summary = opt_report.get(\"summary\", {}) or {}\n",
        "    print(\n",
        "        f\"Applied: {summary.get('applied', 0)}, \"\n",
        "        f\"Skipped: {summary.get('skipped', 0)}, \"\n",
        "        f\"Failed: {summary.get('failed', 0)}\"\n",
        "    )\n",
        "\n",
        "    # Delta memoria rispetto al profilo pre-ottimizzazione\n",
        "    total_before = float(mem.get(\"total_mb\", 0.0) or 0.0)\n",
        "    new_mem_mb = df.memory_usage(deep=True).sum() / 1024**2\n",
        "    saved = total_before - new_mem_mb\n",
        "    if saved > 0:\n",
        "        base = total_before if total_before > 0 else 1e-9\n",
        "        print(f\"‚úÖ Memory saved: {saved:.2f} MB ({(saved / base) * 100:.1f}%)\")\n",
        "\n",
        "# 9. Salva profiling report\n",
        "SAVE_PROFILING_REPORT = True\n",
        "if SAVE_PROFILING_REPORT:\n",
        "    log_dir = GEN_CFG.get(\"paths\", {}).get(\"log_dir\", \"./logs\")\n",
        "    report_path = Path(log_dir) / \"profiling_report.json\"\n",
        "    report_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        canonical_json_dumps(profile_results)\n",
        "    logger.info(\"Profiling report saved to: %s\", report_path)\n",
        "\n",
        "# =========================\n",
        "# SANITY BENCHMARKS & DRIFT\n",
        "# =========================\n",
        "sanity_report, df = run_sanity_checks(df, GEN_CFG)\n",
        "\n",
        "logs_dir = Path(\"./outputs\")\n",
        "logs_dir.mkdir(parents=True, exist_ok=True)\n",
        "sanity_path = logs_dir / \"sanity_report.json\"\n",
        "\n",
        "with sanity_path.open(\"w\", encoding=\"utf-8\") as fp:\n",
        "    canonical_json_dumps(sanity_report)\n",
        "\n",
        "if not sanity_report.get(\"all_passed\", True):\n",
        "    raise RuntimeError(f\"Sanity checks failed ‚Äì vedi dettagli in {sanity_path}\")\n",
        "\n",
        "logger.info(\"‚úÖ Sanity checks passed; report salvato in %s\", sanity_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba65bab5-f410-4aa1-9634-4fb5682f7966",
      "metadata": {},
      "source": [
        "### Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4ffaeeb-ded7-4bbd-9943-4fe9c53a9bad",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2025-08-21 08:40:41,104] INFO dataset_generator: ‚úÖ Saved dataset to outputs\\dataset_generated.csv\n",
            "[2025-08-21 08:40:41,109] INFO dataset_generator: ‚úÖ Saved quality report JSON to outputs\\logs\\quality_report.json\n",
            "[2025-08-21 08:40:41,115] INFO dataset_generator: ‚úÖ Saved quality report YAML to outputs\\logs\\quality_report.yaml\n",
            "[2025-08-21 08:40:41,131] INFO dataset_generator: ‚úÖ Saved top 30 outliers to outputs\\logs\\top_outliers.csv\n",
            "[2025-08-21 08:40:41,733] INFO dataset_generator: ‚úÖ Saved manifest to outputs\\snapshots\\manifest_20250821T064041Z.json (hash=035c4a0e1d9215cc2fd6a476763e65af129a98256fd5b9c6cf1f916fce558e2f)\n",
            "[2025-08-21 08:40:41,736] INFO dataset_generator: Artefatto scritto: outputs\\dataset_generated.csv (3.32 MB) ‚Äì rows=15,000, cols=45\n",
            "[2025-08-21 08:40:41,871] INFO dataset_generator: Export completato con manifest:\n",
            "[2025-08-21 08:40:41,872] INFO dataset_generator: {\"asset_type\":\"property\",\"checksums\":{\"dataset_sha256\":\"8191d4050966e18ac701f07bcaae041f54d164e78d9ffeb4cc5824232b64565e\"},\"config_snapshot\":{\"asset_type\":\"property\",\"city_base_prices\":{\"Bari\":{\"center\":3800.0,\"periphery\":1800.0,\"semi_center\":2700.0},\"Bologna\":{\"center\":4700.0,\"periphery\":2300.0,\"semi_center\":3400.0},\"Cagliari\":{\"center\":3700.0,\"periphery\":1700.0,\"semi_center\":2600.0},\"Catania\":{\"center\":3300.0,\"periphery\":1600.0,\"semi_center\":2400.0},\"Florence\":{\"center\":5500.0,\"periphery\":2700.0,\"semi_center\":3800.0},\"Genoa\":{\"center\":4200.0,\"periphery\":2000.0,\"semi_center\":3000.0},\"Milan\":{\"center\":7500.0,\"periphery\":3500.0,\"semi_center\":5000.0},\"Naples\":{\"center\":4000.0,\"periphery\":1900.0,\"semi_center\":2800.0},\"Padua\":{\"center\":4400.0,\"periphery\":2100.0,\"semi_center\":3100.0},\"Palermo\":{\"center\":3500.0,\"periphery\":1700.0,\"semi_center\":2500.0},\"Rome\":{\"center\":6000.0,\"periphery\":2800.0,\"semi_center\":4000.0},\"Trieste\":{\"center\":4100.0,\"periphery\":1900.0,\"semi_center\":2900.0},\"Turin\":{\"center\":4500.0,\"periphery\":2100.0,\"semi_center\":3200.0},\"Venice\":{\"center\":6000.0,\"periphery\":3000.0,\"semi_center\":4200.0},\"Verona\":{\"center\":4600.0,\"periphery\":2200.0,\"semi_center\":3200.0}},\"expected_profile\":{\"location_distribution_tolerance\":0.05},\"generation_version\":\"v1.0\",\"groupby_observed\":true,\"incoherence\":{\"confidence_thresh\":0.6,\"val_threshold_quantile\":0.95,\"weights\":{\"condition\":0.5,\"env\":0.2,\"luxury\":0.3}},\"location_weights\":{\"Bari\":0.04,\"Bologna\":0.06,\"Cagliari\":0.03,\"Catania\":0.03,\"Florence\":0.05,\"Genoa\":0.05,\"Milan\":0.2,\"Naples\":0.08,\"Padua\":0.04,\"Palermo\":0.05,\"Rome\":0.18,\"Trieste\":0.03,\"Turin\":0.08,\"Venice\":0.04,\"Verona\":0.04},\"n_rows\":15000,\"paths\":{\"log_dir\":\"outputs\\\\logs\",\"output_path\":\"outputs\\\\dataset_generated.csv\",\"snapshot_dir\":\"outputs\\\\snapshots\"},\"price_caps\":{\"max_multiplier\":3.0},\"pricing\":{\"build_age\":{\"new\":0.2,\"old\":-0.15,\"recent\":0.05},\"energy_class_multipliers\":{\"A\":1.15,\"B\":1.08,\"C\":1.0,\"D\":0.95,\"E\":0.9,\"F\":0.85,\"G\":0.8},\"extras\":{\"has_balcony\":0.04,\"has_garage\":0.06,\"has_garden\":0.05},\"floor_modifiers\":{\"is_ground_floor\":-0.08,\"is_top_floor\":0.08},\"state_modifiers\":{\"good\":1.0,\"needs_renovation\":0.85,\"new\":1.15,\"renovated\":1.08},\"view_multipliers\":{\"landmarks\":1.1,\"sea\":1.25}},\"seasonality\":{\"1\":0.98,\"2\":0.98,\"3\":1.02,\"4\":1.05,\"5\":1.05,\"6\":1.03,\"7\":1.0,\"8\":0.97,\"9\":1.02,\"10\":1.01,\"11\":0.99,\"12\":0.97},\"source_tag\":\"synthetic_with_priors\",\"urban_type_by_city\":{\"Bari\":\"urban\",\"Cagliari\":\"urban\",\"Catania\":\"urban\",\"Florence\":\"urban\",\"Genoa\":\"urban\",\"Milan\":\"urban\",\"Naples\":\"urban\",\"Padua\":\"urban\",\"Palermo\":\"urban\",\"Rome\":\"urban\",\"Trieste\":\"urban\",\"Venice\":\"urban\",\"Verona\":\"urban\"},\"zone_thresholds_km\":{\"center\":1.5,\"semi_center\":5.0}},\"generated_at\":\"2025-08-21T06:40:41.731894Z\",\"hashes\":{\"data_digest_sha256\":\"1c5e2f60386cb97f07dfaae408b27495891424b96dc37b5f5f42502a32413874\",\"dataset_file_sha256\":\"8191d4050966e18ac701f07bcaae041f54d164e78d9ffeb4cc5824232b64565e\",\"quality_report_sha256\":\"29d3c3d47f25f6bb5a0be8c6b9af3aa26ca02923ab952fc16caeb5941306c17e\"},\"manifest_hash\":\"035c4a0e1d9215cc2fd6a476763e65af129a98256fd5b9c6cf1f916fce558e2f\",\"paths\":{\"dataset\":\"outputs\\\\dataset_generated.csv\",\"quality_report_json\":\"outputs\\\\logs\\\\quality_report.json\",\"quality_report_yaml\":\"outputs\\\\logs\\\\quality_report.yaml\"},\"report_summary\":{\"keys\":[\"asset_type\",\"consistency\",\"domains\",\"logic\",\"n_cols\",\"n_rows\",\"overall_passed\",\"ranges\",\"schema\",\"temporal\",\"timestamp\"]},\"schema_version\":\"v2\",\"shape\":{\"columns\":[\"asset_id\",\"asset_type\",\"location\",\"valuation_k\",\"price_per_sqm\",\"last_verified_ts\",\"listing_month\",\"region\",\"urban_type\",\"zone\",\"size_m2\",\"rooms\",\"bathrooms\",\"year_built\",\"age_years\",\"floor\",\"building_floors\",\"is_top_floor\",\"is_ground_floor\",\"has_elevator\",\"has_garden\",\"has_balcony\",\"garage\",\"owner_occupied\",\"public_transport_nearby\",\"distance_to_center_km\",\"parking_spot\",\"cellar\",\"attic\",\"concierge\",\"energy_class\",\"humidity_level\",\"temperature_avg\",\"noise_level\",\"air_quality_index\",\"condition_score\",\"risk_score\",\"luxury_score\",\"env_score\",\"orientation\",\"view\",\"condition\",\"heating\",\"confidence_score\",\"strongly_incoherent\"],\"rows\":15000},\"versions\":{\"dataset\":\"v2\",\"feature_set\":\"v1\",\"model_family\":\"axiomatic_rwa_property\"}}\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json, re, hashlib, time\n",
        "from datetime import datetime, timezone\n",
        "from pathlib import Path\n",
        "\n",
        "from notebooks.shared.n01_generate_dataset.exporter import export_dataset\n",
        "from notebooks.shared.common.utils import NumpyJSONEncoder, canonical_json_dumps\n",
        "\n",
        "# ---- Path helpers (non-breaking): ancora a notebooks/ se relativo ----\n",
        "ROOT = Path(\".\").resolve()\n",
        "NB_DIR = ROOT if ROOT.name == \"notebooks\" else (ROOT / \"notebooks\")\n",
        "\n",
        "def _resolve_under_nbdir(p: str | Path, default: str = \"outputs\") -> Path:\n",
        "    p = Path(str(p or default)).expanduser()\n",
        "    return p if p.is_absolute() else (NB_DIR / p)\n",
        "\n",
        "# === Setup percorsi desiderati ===\n",
        "OUT_DIR = _resolve_under_nbdir(GEN_CFG.get(\"paths\", {}).get(\"out_dir\", \"outputs\"))\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "FILE_FORMAT = str(GEN_CFG.get(\"export_format\", \"csv\")).lower()\n",
        "assert FILE_FORMAT in {\"csv\", \"parquet\"}, f\"export_format non supportato: {FILE_FORMAT}\"\n",
        "COMPRESSION = GEN_CFG.get(\"compression\", None if FILE_FORMAT == \"csv\" else \"snappy\")\n",
        "NO_OVERWRITE = bool(GEN_CFG.get(\"no_overwrite\", False))\n",
        "\n",
        "# ---- filename base sanificato ----\n",
        "_raw_base = str(GEN_CFG.get(\"filename_prefix\") or GEN_CFG.get(\"name\") or \"dataset_generated\")\n",
        "BASE_NAME = re.sub(r\"[^A-Za-z0-9._-]+\", \"_\", _raw_base).strip(\"._-\") or \"dataset_generated\"\n",
        "ext = \"parquet\" if FILE_FORMAT == \"parquet\" else \"csv\"\n",
        "output_path = OUT_DIR / f\"{BASE_NAME}.{ext}\"\n",
        "\n",
        "snapshot_dir = OUT_DIR / \"snapshots\"\n",
        "log_dir = OUT_DIR / \"logs\"\n",
        "snapshot_dir.mkdir(parents=True, exist_ok=True)\n",
        "log_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Costruisci config da passare all'exporter (non-breaking)\n",
        "EXPORT_CFG = {\n",
        "    **GEN_CFG,\n",
        "    \"paths\": {\n",
        "        **GEN_CFG.get(\"paths\", {}),\n",
        "        \"output_path\": str(output_path),\n",
        "        \"snapshot_dir\": str(snapshot_dir),\n",
        "        \"log_dir\": str(log_dir),\n",
        "    },\n",
        "}\n",
        "\n",
        "# === Export dataset ===\n",
        "manifest = export_dataset(\n",
        "    df=df,\n",
        "    config=EXPORT_CFG,\n",
        "    report=clean_report,\n",
        "    logger=logger,\n",
        "    format=FILE_FORMAT,\n",
        "    compression=COMPRESSION,\n",
        "    index=False,\n",
        "    no_overwrite=NO_OVERWRITE,\n",
        ")\n",
        "\n",
        "# ---- Arricchisci manifest se mancano campi utili (non rompe se gi√† presenti) ----\n",
        "def _sha256(p: Path) -> str:\n",
        "    h = hashlib.sha256()\n",
        "    with p.open(\"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(65536), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "# dataset path: usa quello restituito o fallback all'output atteso\n",
        "ds_path = Path((manifest or {}).get(\"dataset_path\") or output_path)\n",
        "if not ds_path.exists():\n",
        "    # piccoli fallback (csv/parquet) sotto OUT_DIR\n",
        "    for cand in [output_path, OUT_DIR / f\"{BASE_NAME}.csv\", OUT_DIR / f\"{BASE_NAME}.parquet\"]:\n",
        "        if cand.exists():\n",
        "            ds_path = cand; break\n",
        "\n",
        "# compila extra fields\n",
        "extra = {}\n",
        "try:\n",
        "    if ds_path.exists():\n",
        "        extra[\"sha256\"] = _sha256(ds_path)\n",
        "        extra[\"rows\"] = int(len(df))\n",
        "        extra[\"cols\"] = list(df.columns)\n",
        "    # feature_order sha\n",
        "    feat_json = NB_DIR / \"modeling\" / \"property\" / \"feature_order.json\"\n",
        "    if feat_json.exists():\n",
        "        extra[\"feature_order_sha256\"] = _sha256(feat_json)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# seed + ts_utc sempre utili\n",
        "extra[\"seed\"] = int(SEED)\n",
        "extra[\"ts_utc\"] = datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
        "\n",
        "# aggiorna struttura manifest in memoria\n",
        "if isinstance(manifest, dict):\n",
        "    for k, v in extra.items():\n",
        "        manifest.setdefault(k, v)\n",
        "\n",
        "# prova a riversare su file manifest (se disponibile)\n",
        "try:\n",
        "    man_path = Path((manifest or {}).get(\"manifest_path\", \"\"))\n",
        "    if man_path and man_path.exists():\n",
        "        m = json.loads(man_path.read_text(encoding=\"utf-8\"))\n",
        "        for k, v in extra.items():\n",
        "            m.setdefault(k, v)\n",
        "        man_path.write_text(canonical_json_dumps(m), encoding=\"utf-8\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Log artefatto\n",
        "try:\n",
        "    size_mb = (ds_path.stat().st_size / 1024**2) if ds_path.exists() else 0.0\n",
        "    logger.info(\"Artefatto scritto: %s (%.2f MB) ‚Äì rows=%s, cols=%s\",\n",
        "                ds_path, size_mb, f\"{len(df):,}\", df.shape[1])\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Snapshot descrittivo (opzionale)\n",
        "try:\n",
        "    (df.describe(include=\"all\")\n",
        "       .T.head(20)\n",
        "       .to_csv(OUT_DIR / \"describe_snapshot.csv\", encoding=\"utf-8\"))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ------ 3) Drift check ------\n",
        "drift_info = (clean_report.get(\"sanity_benchmarks\", {}) or {}).get(\"location_drift\") or {}\n",
        "tolerance = float(GEN_CFG.get(\"drift_tolerance\", 0.15))\n",
        "\n",
        "ANALYSIS_DIR = OUT_DIR / \"analysis\"\n",
        "ANALYSIS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "(ANALYSIS_DIR / \"location_drift_generation.json\").write_text(\n",
        "    canonical_json_dumps(drift_info),\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "violating = []\n",
        "if isinstance(drift_info, dict) and \"by_location\" in drift_info:\n",
        "    for loc, info in (drift_info.get(\"by_location\") or {}).items():\n",
        "        try:\n",
        "            diff = float(info.get(\"difference\", 0.0))\n",
        "            if abs(diff) > tolerance:\n",
        "                violating.append(loc)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "if violating:\n",
        "    raise ValueError(f\"Location drift eccessivo su: {violating} (tolleranza ¬±{tolerance})\")\n",
        "\n",
        "# ------ 4) Log riassunto manifest ------\n",
        "wanted = [\"generated_at\", \"dataset_path\", \"quality_report_path\", \"manifest_path\", \"manifest_hash\",\n",
        "          \"sha256\", \"feature_order_sha256\", \"rows\", \"seed\", \"ts_utc\"]\n",
        "manifest_summary = {k: (manifest or {}).get(k) for k in wanted}\n",
        "logger.info(\"Export completato con manifest:\")\n",
        "logger.info(canonical_json_dumps(manifest if isinstance(manifest, dict) and manifest else manifest_summary))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ai-oracle",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
