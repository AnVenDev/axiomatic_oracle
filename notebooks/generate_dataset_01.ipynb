{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b756d33a-e242-4a4c-b988-3c0bb1f0a764",
      "metadata": {},
      "source": [
        "### Base config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67fe5d12-8cf5-4cee-9ac2-a0cdcc0b0ad0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import json\n",
        "\n",
        "from notebooks.shared.common.config import configure_logger, load_settings, load_config\n",
        "from notebooks.shared.common.utils import (\n",
        "    set_global_seed,\n",
        "    get_utc_now,\n",
        "    normalize_location_weights,\n",
        "    derive_city_mappings,\n",
        ")\n",
        "\n",
        "\n",
        "# 1) Logging setup\n",
        "logger = configure_logger(name=\"dataset_generator\", level=\"INFO\")\n",
        "\n",
        "\n",
        "# 2) Load central configuration (typed -> fallback dict)\n",
        "try:\n",
        "    SETTINGS = load_settings(\"./dataset_config.yaml\")     # preferred (typed)\n",
        "    CONFIG = SETTINGS.to_dict() if hasattr(SETTINGS, \"to_dict\") else dict(SETTINGS)\n",
        "except Exception:\n",
        "    CONFIG = load_config(\"./dataset_config.yaml\")         # fallback (dict)\n",
        "\n",
        "# Recupera blocco \"generation\" o fallback flat (retrocompatibilit√†)\n",
        "GEN_CFG = CONFIG.get(\"generation\", CONFIG) or {}\n",
        "\n",
        "# 3) Seeding deterministico\n",
        "SEED = int(GEN_CFG.get(\"seed\", CONFIG.get(\"seed\", 42)))\n",
        "rng = set_global_seed(SEED)\n",
        "\n",
        "\n",
        "# 4) Reference time\n",
        "REFERENCE_TIME = get_utc_now()\n",
        "\n",
        "\n",
        "# 5) Location weights ‚Äî normalizzazione e validazione\n",
        "location_weights_cfg = GEN_CFG.get(\"location_weights\", {}) or {}\n",
        "if not location_weights_cfg:\n",
        "    logger.error(\"location_weights mancante nella config.\")\n",
        "    raise ValueError(\"location_weights non definiti\")\n",
        "\n",
        "normalized_location_weights = normalize_location_weights(location_weights_cfg)\n",
        "\n",
        "\n",
        "# 6) City mappings\n",
        "LOCATIONS, URBAN_TYPE_BY_CITY, REGION_BY_CITY = derive_city_mappings(\n",
        "    GEN_CFG,\n",
        "    urban_override=GEN_CFG.get(\"urban_type_by_city\"),\n",
        "    region_override=GEN_CFG.get(\"region_by_city\"),\n",
        ")\n",
        "\n",
        "\n",
        "# 7) Altri derivati da config\n",
        "CITY_BASE_PRICES = GEN_CFG.get(\"city_base_prices\", {}) or {}\n",
        "SEASONALITY = GEN_CFG.get(\"seasonality\", {}) or {}\n",
        "ZONE_THRESHOLDS = GEN_CFG.get(\"zone_thresholds_km\", {\"center\": 1.5, \"semi_center\": 5.0}) or {}\n",
        "\n",
        "# Validazione thresholds (best-effort, extra rispetto ai modelli)\n",
        "if not {\"center\", \"semi_center\"} <= set(ZONE_THRESHOLDS.keys()):\n",
        "    raise ValueError(\"ZONE_THRESHOLDS incomplete: servono 'center' e 'semi_center'\")\n",
        "if float(ZONE_THRESHOLDS[\"center\"]) >= float(ZONE_THRESHOLDS[\"semi_center\"]):\n",
        "    raise ValueError(\"'center' threshold deve essere < 'semi_center'\")\n",
        "\n",
        "\n",
        "# 8) Flatten config per retrocompatibilit√†\n",
        "FLAT_CONFIG = {**CONFIG, **GEN_CFG}\n",
        "FLAT_CONFIG[\"location_weights\"] = dict(location_weights_cfg)\n",
        "\n",
        "\n",
        "# 9) Log structured config summary\n",
        "logger.info(\"=\" * 60)\n",
        "logger.info(\"DATASET GENERATION CONFIGURATION\")\n",
        "logger.info(\"=\" * 60)\n",
        "logger.info(\n",
        "    \"Config summary:\\n%s\",\n",
        "    json.dumps(\n",
        "        {\n",
        "            \"seed\": SEED,\n",
        "            \"reference_time\": REFERENCE_TIME.isoformat(),\n",
        "            \"locations_count\": len(LOCATIONS),\n",
        "            \"rows_to_generate\": int(FLAT_CONFIG.get(\"n_rows\", GEN_CFG.get(\"n_rows\", 0)) or 0),\n",
        "            \"asset_type\": str(FLAT_CONFIG.get(\"asset_type\", GEN_CFG.get(\"asset_type\", \"property\"))),\n",
        "            \"cities_with_base_prices\": len(CITY_BASE_PRICES),\n",
        "        },\n",
        "        indent=2,\n",
        "        ensure_ascii=False,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "486016bb-691b-4fba-990a-a5c11b6657d3",
      "metadata": {},
      "source": [
        "### Dataset generation and base enrichment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fb3c590-d077-4ba7-b2fb-6cd174165cd9",
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from notebooks.shared.n01_generate_dataset.dataset_builder import generate_dataset_df\n",
        "from notebooks.shared.common.sanity_checks import validate_dataset\n",
        "from notebooks.shared.common.schema import get_required_fields\n",
        "from notebooks.shared.common.utils import (\n",
        "    NumpyJSONEncoder,\n",
        "    optimize_dtypes,\n",
        "    log_basic_diagnostics,\n",
        ")\n",
        "\n",
        "# 0) Sanity preliminare\n",
        "GEN_CFG = CONFIG.get(\"generation\", CONFIG)\n",
        "assert isinstance(GEN_CFG, dict) and int(GEN_CFG.get(\"n_rows\", 0)) > 0, \"generation.n_rows mancante o non valido\"\n",
        "\n",
        "# 1) Generazione dataset + quality_report\n",
        "logger.info(\"Starting dataset generation...\")\n",
        "df, quality_report = generate_dataset_df(\n",
        "    config=GEN_CFG,                 # usa il blocco generation, NON CONFIG\n",
        "    locations=LOCATIONS,\n",
        "    urban_map=URBAN_TYPE_BY_CITY,\n",
        "    region_map=REGION_BY_CITY,\n",
        "    seasonality=SEASONALITY,\n",
        "    city_base_prices=CITY_BASE_PRICES,\n",
        "    rng=rng,\n",
        "    reference_time=REFERENCE_TIME,\n",
        "    batch_size=1000,\n",
        "    show_progress=True,\n",
        "    validate_each=True,\n",
        "    error_budget_pct=0.01,\n",
        ")\n",
        "logger.info(f\"‚úÖ Generated {len(df):,} records\")\n",
        "\n",
        "# Output dir\n",
        "Path(\"outputs\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Salva quality_report per analisi in Notebook 02\n",
        "with open(\"outputs/quality_report.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(quality_report, f, cls=NumpyJSONEncoder, indent=2, ensure_ascii=False)\n",
        "logger.info(\"‚úÖ Quality report saved to outputs/quality_report.json\")\n",
        "\n",
        "# 2) (Intenzionalmente niente enforce dei domini categoriali in nb01)\n",
        "\n",
        "# 3) Ottimizzazione tipi + log memoria risparmiata\n",
        "mem_before = df.memory_usage(deep=True).sum()\n",
        "mem_before_cols = df.memory_usage(deep=True, index=False)\n",
        "dtypes_before = df.dtypes.copy()\n",
        "\n",
        "df = optimize_dtypes(df)\n",
        "\n",
        "mem_after = df.memory_usage(deep=True).sum()\n",
        "mem_after_cols = df.memory_usage(deep=True, index=False)\n",
        "dtypes_after = df.dtypes\n",
        "\n",
        "saved_bytes = mem_before - mem_after\n",
        "saved_mb = saved_bytes / 1024**2\n",
        "pct_saved = (saved_bytes / mem_before * 100) if mem_before > 0 else 0.0\n",
        "\n",
        "logger.info(\n",
        "    \"‚úÖ Data types optimized: %.2f MB ‚Üí %.2f MB  (‚àí%.2f MB, %.1f%%)\",\n",
        "    mem_before / 1024**2, mem_after / 1024**2, saved_mb, pct_saved\n",
        ")\n",
        "\n",
        "# (Opzionale) Top colonne per risparmio memoria\n",
        "SHOW_TOP_SAVINGS = True\n",
        "TOP_N = 8\n",
        "if SHOW_TOP_SAVINGS:\n",
        "    diff = (mem_before_cols - mem_after_cols).sort_values(ascending=False)\n",
        "    top = {k: round(v / 1024**2, 3) for k, v in diff.head(TOP_N).items() if v > 0}\n",
        "    if top:\n",
        "        logger.info(\"üèÅ Top risparmio per colonna (MB): %s\", top)\n",
        "\n",
        "# (Opzionale) Log dei dtype cambiati\n",
        "changed = [c for c in df.columns if dtypes_before.get(c) is not None and dtypes_before[c] != dtypes_after[c]]\n",
        "if changed:\n",
        "    preview = {c: f\"{dtypes_before[c]}‚Üí{dtypes_after[c]}\" for c in changed[:TOP_N]}\n",
        "    more = f\" (+{len(changed)-TOP_N} altre)\" if len(changed) > TOP_N else \"\"\n",
        "    logger.info(\"üî§ Dtype cambiati (%d): %s%s\", len(changed), preview, more)\n",
        "\n",
        "# 4) Validazione schema & robust JSON dump\n",
        "try:\n",
        "    validation_report = validate_dataset(\n",
        "        df,\n",
        "        asset_type=GEN_CFG.get(\"asset_type\", \"property\"),  # usa GEN_CFG\n",
        "        raise_on_failure=True,\n",
        "    )\n",
        "    logger.info(\"‚úÖ Dataset validation passed\")\n",
        "except RuntimeError as e:\n",
        "    logger.error(f\"‚ùå Validation failed: {e}\")\n",
        "    validation_report = {\n",
        "        \"overall_passed\": False,\n",
        "        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
        "        \"n_rows\": len(df),\n",
        "        \"n_cols\": df.shape[1],\n",
        "        \"error\": str(e),\n",
        "    }\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"VALIDATION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "if \"schema\" in validation_report:\n",
        "    missing = validation_report[\"schema\"].get(\"missing\", [])\n",
        "    if missing:\n",
        "        print(f\"‚ö†Ô∏è Missing fields: {missing}\")\n",
        "    else:\n",
        "        print(\"‚úÖ All required fields present\")\n",
        "print(f\"üìä Rows: {validation_report.get('n_rows', len(df)):,}\")\n",
        "print(f\"üìä Cols: {validation_report.get('n_cols', df.shape[1])}\")\n",
        "print(f\"‚úÖ Validation: {'PASSED' if validation_report.get('overall_passed') else 'FAILED'}\")\n",
        "\n",
        "report_path = Path(\"outputs/validation_report.json\")\n",
        "report_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "try:\n",
        "    with open(report_path, \"w\") as fp:\n",
        "        json.dump(validation_report, fp, indent=2, cls=NumpyJSONEncoder)\n",
        "    logger.info(f\"‚úÖ Validation report saved to {report_path}\")\n",
        "except (TypeError, ValueError) as e:\n",
        "    logger.warning(f\"JSON encoder issue ({e}), trying without custom encoder)\")\n",
        "\n",
        "    def convert_to_serializable(obj):\n",
        "        if isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        elif isinstance(obj, (np.integer, np.floating)):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, pd.Timestamp):\n",
        "            return obj.isoformat()\n",
        "        elif isinstance(obj, dict):\n",
        "            return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
        "        elif isinstance(obj, list):\n",
        "            return [convert_to_serializable(item) for item in obj]\n",
        "        return obj\n",
        "\n",
        "    clean_report = convert_to_serializable(validation_report)\n",
        "    with open(report_path, \"w\") as fp:\n",
        "        json.dump(clean_report, fp, indent=2)\n",
        "    logger.info(f\"‚úÖ Validation report (fallback) saved to {report_path}\")\n",
        "\n",
        "df.attrs[\"validation_timestamp\"] = validation_report.get(\"timestamp\")\n",
        "df.attrs[\"validation_passed\"] = bool(validation_report.get(\"overall_passed\", False))\n",
        "\n",
        "# 5) Riordino colonne secondo schema\n",
        "required = get_required_fields(GEN_CFG.get(\"asset_type\", \"property\"))  # usa GEN_CFG\n",
        "missing = [c for c in required if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Campi richiesti mancanti: {missing}\")\n",
        "\n",
        "others = [c for c in df.columns if c not in required]\n",
        "df = df[required + others]\n",
        "logger.info(\"‚úÖ Columns reordered: %d required + %d optional\", len(required), len(others))\n",
        "\n",
        "# 6) Diagnostics rapidi\n",
        "log_basic_diagnostics(df, logger)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DATASET GENERATION COMPLETED\")\n",
        "print(\"=\"*60)\n",
        "print(f\"‚úÖ Records: {len(df):,}\")\n",
        "print(f\"‚úÖ Features: {df.shape[1]}\")\n",
        "print(f\"‚úÖ Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "print(f\"‚úÖ Validation: {'PASSED' if df.attrs.get('validation_passed') else 'FAILED'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff389f8d-0113-4101-88fe-7689bb8ed720",
      "metadata": {},
      "source": [
        "### Profiling & optimizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73f2519f-6d09-4193-bab9-9e29cea4edac",
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from notebooks.shared.common.performance_utils import DatasetProfiler, DtypeOptimizer\n",
        "from notebooks.shared.common.reports import run_sanity_checks\n",
        "from notebooks.shared.common.utils import NumpyJSONEncoder\n",
        "\n",
        "# 1. Pre-profiling casts & parsing (best effort)\n",
        "for col, dtype in {\n",
        "    \"price_per_sqm_capped\": \"float32\",\n",
        "    \"listing_quarter\": \"category\",\n",
        "    \"location_premium\": \"float32\",\n",
        "}.items():\n",
        "    if col in df.columns:\n",
        "        try:\n",
        "            df[col] = df[col].astype(dtype)\n",
        "        except Exception:\n",
        "            pass  # best-effort\n",
        "\n",
        "if \"last_verified_ts\" in df.columns:\n",
        "    df[\"last_verified_ts\"] = pd.to_datetime(df[\"last_verified_ts\"], utc=True, errors=\"coerce\")\n",
        "\n",
        "# 2. Inizializza profiler (usa default se non configurato)\n",
        "profiler = DatasetProfiler(\n",
        "    float_downcast_atol=1e-6,\n",
        "    float_downcast_rtol=1e-3,\n",
        ")\n",
        "\n",
        "# 3. Esegui profiling completo\n",
        "print(\"üîç Profiling dataset performance...\")\n",
        "profile_results = profiler.profile(\n",
        "    df,\n",
        "    groupby_observed=GEN_CFG.get(\"groupby_observed\", True),\n",
        ")\n",
        "\n",
        "# 4. Report memoria\n",
        "mem = profile_results.get(\"memory\", {}) or {}\n",
        "print(\"\\n=== MEMORY PROFILE ===\")\n",
        "print(f\"Total memory: {mem.get('total_mb', 0.0):.2f} MB\")\n",
        "print(f\"Memory per row: {mem.get('per_row_kb', 0.0):.3f} KB\")\n",
        "print(\"\\nMemory by dtype:\")\n",
        "for dtype, info in (mem.get(\"by_dtype\", {}) or {}).items():\n",
        "    print(f\"  {dtype}: {info.get('mb', 0.0):.2f} MB ({info.get('pct', 0.0):.1f}%) - {info.get('n_columns', 0)} cols\")\n",
        "\n",
        "# 5. Benchmark performance\n",
        "perf = profile_results.get(\"performance\", {}) or {}\n",
        "print(\"\\n=== PERFORMANCE BENCHMARKS ===\")\n",
        "if \"groupby\" in perf and \"time_ms\" in perf[\"groupby\"]:\n",
        "    gb = perf[\"groupby\"]\n",
        "    cols = gb.get(\"columns\") or []\n",
        "    print(f\"GroupBy ({', '.join(cols) if cols else '?' }): {gb['time_ms']:.2f} ms\")\n",
        "    if gb.get(\"rows_per_sec\") is not None:\n",
        "        print(f\"  ‚Üí {gb['rows_per_sec']:,} rows/sec\")\n",
        "if \"sort\" in perf and \"time_ms\" in perf[\"sort\"]:\n",
        "    srt = perf[\"sort\"]\n",
        "    print(f\"Sort by {srt.get('column','?')}: {srt['time_ms']:.2f} ms\")\n",
        "\n",
        "# 6. Suggerimenti dtype\n",
        "dtype_sugg = profile_results.get(\"dtype_optimization\") or {}\n",
        "print(\"\\n=== DTYPE OPTIMIZATION SUGGESTIONS ===\")\n",
        "if dtype_sugg:\n",
        "    for col, opt in list(dtype_sugg.items())[:10]:\n",
        "        print(f\"  {col}: {opt.get('current','?')} ‚Üí {opt.get('target','?')} ({opt.get('reason','')})\")\n",
        "        if \"memory_reduction_pct\" in opt:\n",
        "            print(f\"    ‚Üí Mem saving: {opt['memory_reduction_pct']}%\")\n",
        "else:\n",
        "    print(\"  No optimizations suggested\")\n",
        "\n",
        "# 7. Suggerimenti indici\n",
        "print(\"\\n=== INDEX SUGGESTIONS ===\")\n",
        "for idx in profile_results.get(\"index_suggestions\", []) or []:\n",
        "    mark = \"‚úÖ\" if idx.get(\"utility\") in {\"excellent\", \"good\"} else \"‚ùå\"\n",
        "    print(f\"  {mark} {idx.get('column','?')}: {idx.get('utility','?')} ({idx.get('reason','')})\")\n",
        "\n",
        "# 8. Applica ottimizzazioni automatiche (facoltativo)\n",
        "AUTO_APPLY_OPTIMIZATIONS = True\n",
        "if AUTO_APPLY_OPTIMIZATIONS and dtype_sugg:\n",
        "    print(\"\\nüîß Applying dtype optimizations...\")\n",
        "    optimizer = DtypeOptimizer()\n",
        "    try:\n",
        "        # API corretta: 'inplace'\n",
        "        df, opt_report = optimizer.apply(\n",
        "            df,\n",
        "            dtype_sugg,\n",
        "            inplace=False,  # <-- fix qui\n",
        "        )\n",
        "    except TypeError:\n",
        "        # Fallback per versioni pi√π vecchie (positional only)\n",
        "        df, opt_report = optimizer.apply(df, dtype_sugg, False)\n",
        "\n",
        "    summary = opt_report.get(\"summary\", {}) or {}\n",
        "    print(\n",
        "        f\"Applied: {summary.get('applied', 0)}, \"\n",
        "        f\"Skipped: {summary.get('skipped', 0)}, \"\n",
        "        f\"Failed: {summary.get('failed', 0)}\"\n",
        "    )\n",
        "\n",
        "    # Delta memoria rispetto al profilo pre-ottimizzazione\n",
        "    total_before = float(mem.get(\"total_mb\", 0.0) or 0.0)\n",
        "    new_mem_mb = df.memory_usage(deep=True).sum() / 1024**2\n",
        "    saved = total_before - new_mem_mb\n",
        "    if saved > 0:\n",
        "        base = total_before if total_before > 0 else 1e-9\n",
        "        print(f\"‚úÖ Memory saved: {saved:.2f} MB ({(saved / base) * 100:.1f}%)\")\n",
        "\n",
        "# 9. Salva profiling report\n",
        "SAVE_PROFILING_REPORT = True\n",
        "if SAVE_PROFILING_REPORT:\n",
        "    log_dir = GEN_CFG.get(\"paths\", {}).get(\"log_dir\", \"./logs\")\n",
        "    report_path = Path(log_dir) / \"profiling_report.json\"\n",
        "    report_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(profile_results, f, indent=2, cls=NumpyJSONEncoder)\n",
        "    logger.info(\"Profiling report saved to: %s\", report_path)\n",
        "\n",
        "# =========================\n",
        "# SANITY BENCHMARKS & DRIFT\n",
        "# =========================\n",
        "sanity_report, df = run_sanity_checks(df, GEN_CFG)\n",
        "\n",
        "logs_dir = Path(\"./outputs\")\n",
        "logs_dir.mkdir(parents=True, exist_ok=True)\n",
        "sanity_path = logs_dir / \"sanity_report.json\"\n",
        "\n",
        "with sanity_path.open(\"w\", encoding=\"utf-8\") as fp:\n",
        "    json.dump(sanity_report, fp, indent=2, ensure_ascii=False, cls=NumpyJSONEncoder)\n",
        "\n",
        "if not sanity_report.get(\"all_passed\", True):\n",
        "    raise RuntimeError(f\"Sanity checks failed ‚Äì vedi dettagli in {sanity_path}\")\n",
        "\n",
        "logger.info(\"‚úÖ Sanity checks passed; report salvato in %s\", sanity_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba65bab5-f410-4aa1-9634-4fb5682f7966",
      "metadata": {},
      "source": [
        "### Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4ffaeeb-ded7-4bbd-9943-4fe9c53a9bad",
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "from notebooks.shared.n01_generate_dataset.exporter import export_dataset\n",
        "from notebooks.shared.common.utils import NumpyJSONEncoder\n",
        "\n",
        "# === Setup percorsi desiderati ===\n",
        "OUT_DIR = Path(GEN_CFG.get(\"paths\", {}).get(\"out_dir\", \"./outputs\"))\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "FILE_FORMAT = str(GEN_CFG.get(\"export_format\", \"csv\")).lower()\n",
        "assert FILE_FORMAT in {\"csv\", \"parquet\"}, f\"export_format non supportato: {FILE_FORMAT}\"\n",
        "COMPRESSION = GEN_CFG.get(\"compression\", None if FILE_FORMAT == \"csv\" else \"snappy\")\n",
        "NO_OVERWRITE = bool(GEN_CFG.get(\"no_overwrite\", False))\n",
        "\n",
        "ext = \"parquet\" if FILE_FORMAT.lower() == \"parquet\" else \"csv\"\n",
        "output_path = OUT_DIR / f\"{FILENAME_PREFIX}.{ext}\"\n",
        "snapshot_dir = OUT_DIR / \"snapshots\"\n",
        "log_dir = OUT_DIR / \"logs\"\n",
        "\n",
        "# Costruisci config da passare all'exporter\n",
        "EXPORT_CFG = {\n",
        "    **GEN_CFG,\n",
        "    \"paths\": {\n",
        "        **GEN_CFG.get(\"paths\", {}),\n",
        "        \"output_path\": str(output_path),\n",
        "        \"snapshot_dir\": str(snapshot_dir),\n",
        "        \"log_dir\": str(log_dir),\n",
        "    },\n",
        "}\n",
        "\n",
        "# === Export dataset ===\n",
        "manifest = export_dataset(\n",
        "    df=df,\n",
        "    config=EXPORT_CFG,\n",
        "    report=clean_report,\n",
        "    logger=logger,\n",
        "    format=FILE_FORMAT,\n",
        "    compression=COMPRESSION,\n",
        "    index=False,\n",
        "    no_overwrite=NO_OVERWRITE,\n",
        ")\n",
        "\n",
        "try:\n",
        "    ds_path = Path(manifest.get(\"dataset_path\", output_path))\n",
        "    size_mb = ds_path.stat().st_size / 1024**2\n",
        "    logger.info(\"Artefatto scritto: %s (%.2f MB) ‚Äì rows=%s, cols=%s\",\n",
        "                ds_path, size_mb, f\"{len(df):,}\", df.shape[1])\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Snapshot descrittivo (opzionale)\n",
        "df.describe(include=\"all\").T.head(20).to_csv(OUT_DIR / \"describe_snapshot.csv\", encoding=\"utf-8\")\n",
        "\n",
        "# ------ 3) Drift check ------\n",
        "drift_info = (\n",
        "    clean_report.get(\"sanity_benchmarks\", {}).get(\"location_drift\")\n",
        "    or df.attrs.get(\"location_drift_report\", {})\n",
        "    or {}\n",
        ")\n",
        "\n",
        "with (OUT_DIR / \"location_drift.json\").open(\"w\", encoding=\"utf-8\") as fp:\n",
        "    json.dump(drift_info, fp, cls=NumpyJSONEncoder, indent=2, ensure_ascii=False)\n",
        "\n",
        "tolerance = (\n",
        "    GEN_CFG.get(\"expected_profile\", {}).get(\"location_distribution_tolerance\", 0.05)\n",
        "    if isinstance(GEN_CFG.get(\"expected_profile\", {}), dict) else 0.05\n",
        ")\n",
        "\n",
        "violating = []\n",
        "if isinstance(drift_info, dict):\n",
        "    for loc, info in drift_info.items():\n",
        "        try:\n",
        "            diff = float(info.get(\"difference\", 0.0))\n",
        "            if abs(diff) > tolerance:\n",
        "                violating.append(loc)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "if violating:\n",
        "    raise ValueError(f\"Location drift eccessivo su: {violating} (tolleranza ¬±{tolerance})\")\n",
        "\n",
        "# ------ 4) Log riassunto manifest ------\n",
        "wanted = [\"generated_at\", \"dataset_path\", \"quality_report_path\", \"manifest_path\", \"manifest_hash\"]\n",
        "manifest_summary = {k: manifest.get(k) for k in wanted if isinstance(manifest, dict) and k in manifest}\n",
        "logger.info(\"Export completato con manifest:\")\n",
        "logger.info(json.dumps(manifest_summary or manifest, indent=2, ensure_ascii=False))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ai-oracle",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
