{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "10c75f60-9ee0-42f9-9b6b-21c018a83790",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Imports & Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2d6afe71-a30b-4efa-b790-baab0312540d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Loaded FITTED model v2 from C:\\Users\\anven\\OneDrive\\Documenti\\GitHub\\axiomatic_oracle\\notebooks\\outputs\\modeling\\property\n",
            "   Features: 53 (cat=23, num=41)\n",
            "   Inference dir: C:/Users/anven/OneDrive/Documenti/GitHub/axiomatic_oracle/notebooks/outputs/inference\n",
            "   API compare: True → http://127.0.0.1:8000\n",
            "constants module: C:\\Users\\anven\\OneDrive\\Documenti\\GitHub\\axiomatic_oracle\\notebooks\\shared\\common\\constants.py\n",
            "SCHEMA_VERSION = v2\n",
            "Legacy alias OK → notebooks.shared.common.constants → shared.common.constants\n",
            "=== PIPELINE LAYOUT ===\n",
            "Pipeline(steps=4)\n",
            "  [0] canon_geo: GeoCanonizer\n",
            "  [1] priors_guard: PriorsGuard\n",
            "  [2] derive: EnsureDerivedFeatures\n",
            "  [3] core: TransformedTargetRegressor\n"
          ]
        }
      ],
      "source": [
        "# 01) Imports & Config — hardened (path bootstrap, legacy shims, safe predict)\n",
        "from __future__ import annotations\n",
        "\n",
        "import os, re, json, logging, warnings, hashlib, sys, types, importlib\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, Optional, List, Tuple\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Path bootstrap\n",
        "# ---------------------------------------------------------------------\n",
        "NB_DIR = Path.cwd().resolve()\n",
        "if NB_DIR.name.lower() == \"notebooks\" and (NB_DIR.parent / \"shared\").exists():\n",
        "    PROJECT_ROOT = NB_DIR.parent.resolve()\n",
        "else:\n",
        "    PROJECT_ROOT = NB_DIR if (NB_DIR / \"shared\").exists() else NB_DIR.parent\n",
        "\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Env: limita i thread nativi (evita crash con MKL/OpenBLAS)\n",
        "# ---------------------------------------------------------------------\n",
        "for _k in (\"OMP_NUM_THREADS\", \"OPENBLAS_NUM_THREADS\", \"MKL_NUM_THREADS\", \"NUMEXPR_NUM_THREADS\"):\n",
        "    os.environ.setdefault(_k, \"1\")\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Legacy shims (installali SUBITO, prima di qualsiasi joblib.load)\n",
        "# ---------------------------------------------------------------------\n",
        "def _install_legacy_aliases():\n",
        "    # crea gli scheletri dei parent package\n",
        "    for name in (\"notebooks\", \"notebooks.shared\", \"notebooks.shared.common\"):\n",
        "        if name not in sys.modules:\n",
        "            sys.modules[name] = types.ModuleType(name)\n",
        "\n",
        "    # carica i moduli reali\n",
        "    new_const = importlib.import_module(\"shared.common.constants\")\n",
        "    sys.modules[\"notebooks.shared.common.constants\"] = new_const\n",
        "\n",
        "    # garantisci simboli legacy eventualmente cercati dagli artifact\n",
        "    if not hasattr(new_const, \"EXPECTED_PRICE_PER_SQM_EUR_RANGE\"):\n",
        "        setattr(\n",
        "            new_const, \"EXPECTED_PRICE_PER_SQM_EUR_RANGE\",\n",
        "            getattr(new_const, \"EXPECTED_PRED_RANGE\", (20.0, 20000.0))\n",
        "        )\n",
        "\n",
        "    # alias serving_transformers → quello moderno\n",
        "    new_st = importlib.import_module(\"shared.common.serving_transformers\")\n",
        "    sys.modules[\"notebooks.shared.common.serving_transformers\"] = new_st\n",
        "\n",
        "    # rendi navigabile \"notebooks.shared.common\"\n",
        "    sys.modules[\"notebooks\"].shared = sys.modules[\"notebooks.shared\"]\n",
        "    sys.modules[\"notebooks.shared\"].common = sys.modules[\"notebooks.shared.common\"]\n",
        "\n",
        "# Installa SUBITO gli alias (prima di qualunque import che possa toccare i pickle)\n",
        "_install_legacy_aliases()\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Third-party\n",
        "# ---------------------------------------------------------------------\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Shared modern modules\n",
        "# ---------------------------------------------------------------------\n",
        "from shared.common.config import configure_logger\n",
        "from shared.common.utils import canonical_json_dumps\n",
        "from shared.common.sanity_checks import leakage_gate, scale_gate\n",
        "from shared.common.constants import SCHEMA_VERSION, NOTE_MAX_BYTES\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Logger & dirs\n",
        "# ---------------------------------------------------------------------\n",
        "ASSET_TYPE = \"property\"\n",
        "PREFERRED_MODEL_VERSION = os.getenv(\"MODEL_VERSION\", \"v2\")\n",
        "\n",
        "MODELS_ROOT_CANDIDATES: List[Path] = [\n",
        "    PROJECT_ROOT / \"notebooks\" / \"outputs\" / \"modeling\",\n",
        "    PROJECT_ROOT / \"outputs\" / \"modeling\",\n",
        "]\n",
        "env_root = os.getenv(\"MODELS_ROOT\")\n",
        "if env_root and env_root.strip():\n",
        "    MODELS_ROOT_CANDIDATES.insert(0, Path(env_root))\n",
        "\n",
        "MODELS_ROOT = next((c for c in MODELS_ROOT_CANDIDATES if c.exists()), PROJECT_ROOT / \"outputs\" / \"modeling\")\n",
        "MODEL_DIR = MODELS_ROOT / ASSET_TYPE\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "INFER_DIR = PROJECT_ROOT / \"outputs\" / \"inference\"\n",
        "INFER_DIR.mkdir(parents=True, exist_ok=True)\n",
        "LOG_PATH = PROJECT_ROOT / \"outputs\" / \"logs\" / \"predictions_log.jsonl\"\n",
        "LOG_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "API_BASE = os.getenv(\"API_BASE\", \"http://127.0.0.1:8000\")\n",
        "COMPARE_WITH_API = os.getenv(\"COMPARE_WITH_API\", \"true\").lower() in {\"1\", \"true\", \"yes\", \"y\"}\n",
        "\n",
        "LOG_LEVEL = os.getenv(\"LOG_LEVEL\", \"INFO\")\n",
        "LOG_JSON = os.getenv(\"LOG_JSON\", \"false\").lower() in {\"1\", \"true\", \"yes\", \"y\"}\n",
        "logger = configure_logger(level=LOG_LEVEL, name=\"nb04_infer\", json_format=LOG_JSON)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Utils\n",
        "# ---------------------------------------------------------------------\n",
        "_version_re = re.compile(r\"value_regressor_(v\\d+)\\.joblib$\")\n",
        "\n",
        "def _list_versions(dirpath: Path) -> List[str]:\n",
        "    out: List[Tuple[int, str]] = []\n",
        "    for p in dirpath.glob(\"value_regressor_v*.joblib\"):\n",
        "        m = _version_re.search(p.name)\n",
        "        if not m:\n",
        "            continue\n",
        "        v = m.group(1)\n",
        "        try:\n",
        "            n = int(v[1:])\n",
        "        except Exception:\n",
        "            n = -1\n",
        "        out.append((n, v))\n",
        "    out.sort(reverse=True)\n",
        "    return [v for _, v in out]\n",
        "\n",
        "def _is_fitted(obj) -> bool:\n",
        "    try:\n",
        "        if isinstance(obj, TransformedTargetRegressor):\n",
        "            est = getattr(obj, \"regressor_\", None) or getattr(obj, \"regressor\", None)\n",
        "            if est is not None:\n",
        "                return _is_fitted(est)\n",
        "            check_is_fitted(obj)\n",
        "            return True\n",
        "        if isinstance(obj, Pipeline):\n",
        "            last = obj.steps[-1][1]\n",
        "            return _is_fitted(last)\n",
        "        check_is_fitted(obj)\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def _read_json(path: Path) -> Dict[str, Any]:\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def _sha256_file(path: Path) -> str:\n",
        "    h = hashlib.sha256()\n",
        "    with path.open(\"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(1<<20), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def _dedup_preserve(seq: List[str]) -> List[str]:\n",
        "    seen, out = set(), []\n",
        "    for s in seq:\n",
        "        if s not in seen:\n",
        "            seen.add(s); out.append(s)\n",
        "    return out\n",
        "\n",
        "def _safe_joblib_load(path: Path):\n",
        "    try:\n",
        "        return joblib.load(path)\n",
        "    except ModuleNotFoundError:\n",
        "        _install_legacy_aliases()\n",
        "        return joblib.load(path)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Carica modello FITTED (preferito → fallback più recente)\n",
        "# ---------------------------------------------------------------------\n",
        "def resolve_fitted_model(base_dir: Path, preferred: Optional[str]) -> Dict[str, Any]:\n",
        "    def _try(ver: str):\n",
        "        p = base_dir / f\"value_regressor_{ver}.joblib\"\n",
        "        m = base_dir / f\"value_regressor_{ver}_meta.json\"\n",
        "        if p.exists() and m.exists():\n",
        "            pl = _safe_joblib_load(p)\n",
        "            if _is_fitted(pl):\n",
        "                return {\"version\": ver, \"pipeline\": p, \"meta\": m, \"manifest\": base_dir / \"training_manifest.json\", \"obj\": pl}\n",
        "        return None\n",
        "\n",
        "    if preferred:\n",
        "        got = _try(preferred)\n",
        "        if got:\n",
        "            return got\n",
        "        logger.warning(\"Model %s presente ma non fitted/leggibile; cerco fallback…\", preferred)\n",
        "\n",
        "    for ver in _list_versions(base_dir):\n",
        "        got = _try(ver)\n",
        "        if got:\n",
        "            return got\n",
        "\n",
        "    raise FileNotFoundError(f\"Nessun modello fitted trovato in {base_dir}\")\n",
        "\n",
        "# ---- GO ----\n",
        "resolved = resolve_fitted_model(MODEL_DIR, PREFERRED_MODEL_VERSION)\n",
        "MODEL_VERSION: str = resolved[\"version\"]\n",
        "PIPELINE_PATH: Path = resolved[\"pipeline\"]\n",
        "META_PATH: Path = resolved[\"meta\"]\n",
        "MANIFEST_PATH: Path = resolved[\"manifest\"]\n",
        "pipeline = resolved[\"obj\"]\n",
        "\n",
        "# integrità bundle\n",
        "model_meta: Dict[str, Any] = _read_json(META_PATH)\n",
        "expected_hash = model_meta.get(\"model_hash\") or model_meta.get(\"pipeline_sha256\")\n",
        "actual_hash = _sha256_file(PIPELINE_PATH)\n",
        "if expected_hash and expected_hash != actual_hash:\n",
        "    raise ValueError(f\"Bundle manomesso: meta={expected_hash[:8]}… != actual={actual_hash[:8]}…\")\n",
        "\n",
        "# expected features\n",
        "feature_order_candidates: List[Path] = []\n",
        "manifest: Dict[str, Any] = {}\n",
        "if MANIFEST_PATH.exists():\n",
        "    try:\n",
        "        manifest = _read_json(MANIFEST_PATH)\n",
        "        p_from_manifest = (manifest.get(\"paths\", {}) or {}).get(\"feature_order\") or manifest.get(\"feature_order_path\")\n",
        "        if p_from_manifest:\n",
        "            feature_order_candidates.append(Path(p_from_manifest))\n",
        "    except Exception as e:\n",
        "        logger.warning(\"Manifest presente ma non leggibile; fallback a meta.json\", extra={\"error\": str(e)})\n",
        "\n",
        "feature_order_candidates.append(PIPELINE_PATH.parent / \"feature_order.json\")\n",
        "FEATURE_ORDER_PATH: Optional[Path] = next((p for p in feature_order_candidates if p and p.exists()), None)\n",
        "\n",
        "categorical_expected: List[str] = list(model_meta.get(\"features_categorical\", []) or [])\n",
        "numeric_expected: List[str] = list(model_meta.get(\"features_numeric\", []) or [])\n",
        "\n",
        "if FEATURE_ORDER_PATH:\n",
        "    try:\n",
        "        feature_order: List[str] = _read_json(FEATURE_ORDER_PATH)\n",
        "        ALL_EXPECTED: List[str] = list(map(str, feature_order))\n",
        "    except Exception as e:\n",
        "        logger.warning(\"feature_order.json non leggibile; uso meta/manifest\", extra={\"error\": str(e)})\n",
        "        ALL_EXPECTED = _dedup_preserve(categorical_expected + [c for c in numeric_expected if c not in categorical_expected])\n",
        "else:\n",
        "    try:\n",
        "        feats_from_manifest = (manifest.get(\"feature_order\")\n",
        "                               or manifest.get(\"expected_features\")\n",
        "                               or manifest.get(\"model\", {}).get(\"feature_list\")\n",
        "                               or manifest.get(\"model\", {}).get(\"features\"))\n",
        "        if isinstance(feats_from_manifest, dict):\n",
        "            categorical_expected = feats_from_manifest.get(\"categorical\", categorical_expected) or categorical_expected\n",
        "            numeric_expected = feats_from_manifest.get(\"numeric\", numeric_expected) or numeric_expected\n",
        "    except Exception:\n",
        "        pass\n",
        "    ALL_EXPECTED = _dedup_preserve(categorical_expected + [c for c in numeric_expected if c not in categorical_expected])\n",
        "\n",
        "print(f\"✅ Loaded FITTED model {MODEL_VERSION} from {PIPELINE_PATH.parent}\")\n",
        "print(f\"   Features: {len(ALL_EXPECTED)} (cat={len(categorical_expected)}, num={len(numeric_expected)})\")\n",
        "print(f\"   Inference dir: {INFER_DIR.as_posix()}\")\n",
        "print(f\"   API compare: {COMPARE_WITH_API} → {API_BASE}\")\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# MONKEY PATCH: PriorsGuard → pulizia missing/nullable (come API)\n",
        "# ---------------------------------------------------------------------\n",
        "def _to_numpy_nan(x):\n",
        "    try:\n",
        "        import pandas as _pd\n",
        "        if x is _pd.NA:\n",
        "            return np.nan\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        if isinstance(x, float) and (x != x):\n",
        "            return np.nan\n",
        "    except Exception:\n",
        "        pass\n",
        "    return x\n",
        "\n",
        "def _is_nullable_int_or_bool_dtype(dtype) -> bool:\n",
        "    s = str(dtype)\n",
        "    return s.startswith(\"Int\") or s == \"boolean\"\n",
        "\n",
        "def _df_map(df: pd.DataFrame, func):\n",
        "    if hasattr(pd.DataFrame, \"map\"):\n",
        "        return df.map(func)\n",
        "    return df.applymap(func)\n",
        "\n",
        "def _to_numpy_nan(x):\n",
        "    try:\n",
        "        import pandas as _pd\n",
        "        if x is _pd.NA:\n",
        "            return np.nan\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        if isinstance(x, float) and (x != x):  # NaN\n",
        "            return np.nan\n",
        "    except Exception:\n",
        "        pass\n",
        "    return x\n",
        "\n",
        "def _is_nullable_int_or_bool_dtype(dtype) -> bool:\n",
        "    s = str(dtype)\n",
        "    return s.startswith(\"Int\") or s == \"boolean\"\n",
        "\n",
        "def _clean_missing_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        df = pd.DataFrame(df)\n",
        "    # usa compat wrapper (niente più FutureWarning)\n",
        "    df = _df_map(df, _to_numpy_nan)\n",
        "    for c in df.columns:\n",
        "        dt = df[c].dtype\n",
        "        if _is_nullable_int_or_bool_dtype(dt):\n",
        "            df[c] = df[c].astype(\"float64\")\n",
        "    return df\n",
        "\n",
        "def _patch_priorsguard_cleaner(p):\n",
        "    \"\"\"Se nella pipeline c'è PriorsGuard, wrappa .transform per ripulire il DF (evita crash).\"\"\"\n",
        "    try:\n",
        "        from sklearn.pipeline import Pipeline as _SkPipeline\n",
        "        if isinstance(p, _SkPipeline):\n",
        "            new_steps = []\n",
        "            for name, step in p.steps:\n",
        "                clsname = getattr(step, \"__class__\", type(None)).__name__\n",
        "                if clsname == \"PriorsGuard\" and hasattr(step, \"transform\"):\n",
        "                    orig_transform = step.transform\n",
        "                    def _wrapped_transform(X, _orig=orig_transform):\n",
        "                        Y = _orig(X)\n",
        "                        try:\n",
        "                            if isinstance(Y, pd.DataFrame):\n",
        "                                Y = _clean_missing_df(Y)\n",
        "                        except Exception:\n",
        "                            pass\n",
        "                        return Y\n",
        "                    step.transform = _wrapped_transform\n",
        "                new_steps.append((name, step))\n",
        "            p.steps = new_steps\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "_patch_priorsguard_cleaner(pipeline)\n",
        "\n",
        "# (diagnostica minima + layout pipeline)\n",
        "try:\n",
        "    import shared.common.constants as _cmod\n",
        "    print(\"constants module:\", getattr(_cmod, \"__file__\", \"<in-memory>\"))\n",
        "    print(\"SCHEMA_VERSION =\", getattr(_cmod, \"SCHEMA_VERSION\", None))\n",
        "    import notebooks.shared.common.constants as _lcmod  # legacy alias deve coincidere\n",
        "    assert _lcmod is _cmod\n",
        "    print(\"Legacy alias OK → notebooks.shared.common.constants → shared.common.constants\")\n",
        "except Exception as e:\n",
        "    print(\"⚠️ Legacy alias check:\", e)\n",
        "\n",
        "# Layout sintetico\n",
        "try:\n",
        "    print(\"=== PIPELINE LAYOUT ===\")\n",
        "    if isinstance(pipeline, Pipeline):\n",
        "        print(f\"Pipeline(steps={len(pipeline.steps)})\")\n",
        "        for i, (nm, st) in enumerate(pipeline.steps):\n",
        "            print(f\"  [{i}] {nm}: {st.__class__.__name__}\")\n",
        "    else:\n",
        "        print(type(pipeline).__name__)\n",
        "except Exception:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d55eb44-9ea4-4092-a3d1-7c910a6c2e76",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Input Schema & Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ae9985cd-a991-4471-8629-63b656695c92",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CELLA 02 eseguita. (_CELLA02_DONE = True )\n"
          ]
        }
      ],
      "source": [
        "# 02) Normalizzazione/validazione LEGGERA — no full validator, no side-effects\n",
        "from __future__ import annotations\n",
        "from typing import Dict, Any, Tuple, Optional, List\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from shared.common.constants import ASSET_ID, LOCATION\n",
        "from shared.common.utils import canonical_location, get_utc_now\n",
        "\n",
        "_CELLA02_START = True\n",
        "\n",
        "# --- Alias: manteniamo il CANONICO 'garage' (non 'has_garage') ---\n",
        "_KEY_ALIASES = {\n",
        "    \"sqm\":\"size_m2\",\"size\":\"size_m2\",\"m2\":\"size_m2\",\n",
        "    \"year\":\"year_built\",\"built_year\":\"year_built\",\n",
        "    \"balcony\":\"has_balcony\",\"garden\":\"has_garden\",\n",
        "    \"has_garage\":\"garage\",               # ← alias → canonico\n",
        "    \"air_quality\":\"air_quality_index\",\"noise\":\"noise_level\",\n",
        "    \"valuation\":\"valuation_k\",\"price_k\":\"valuation_k\",\n",
        "    \"n_rooms\":\"rooms\",\"room_count\":\"rooms\",\n",
        "    \"n_bathrooms\":\"bathrooms\",\"bathroom_count\":\"bathrooms\",\n",
        "    \"elevator\":\"has_elevator\",\"city_name\":\"city\",\n",
        "}\n",
        "\n",
        "# Derivate consentite (no leakage)\n",
        "_SAFE_DERIVED = {\n",
        "    \"age_years\",\"luxury_score\",\"env_score\",\"location\",\"city\",\n",
        "    \"is_top_floor\",\"listing_month\", ASSET_ID,\n",
        "}\n",
        "\n",
        "def _canonicalize_keys(rec: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    return {_KEY_ALIASES.get(k, k): v for k, v in rec.items()}\n",
        "\n",
        "def _autofill_safe(rec: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    r = _canonicalize_keys(dict(rec))\n",
        "\n",
        "    # age_years\n",
        "    if \"age_years\" not in r and r.get(\"year_built\") not in (None, \"\"):\n",
        "        try:\n",
        "            r[\"age_years\"] = max(0, datetime.utcnow().year - int(r[\"year_built\"]))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # luxury_score (has_garden/has_balcony/garage)\n",
        "    if \"luxury_score\" not in r:\n",
        "        g  = 1.0 if bool(r.get(\"has_garden\", 0)) else 0.0\n",
        "        b  = 1.0 if bool(r.get(\"has_balcony\", 0)) else 0.0\n",
        "        ga = 1.0 if bool(r.get(\"garage\", 0)) else 0.0\n",
        "        r[\"luxury_score\"] = (g + b + ga) / 3.0\n",
        "\n",
        "    # env_score\n",
        "    if \"env_score\" not in r:\n",
        "        try:\n",
        "            aq = float(r.get(\"air_quality_index\", 0.0))\n",
        "            nz = float(r.get(\"noise_level\", 0.0))\n",
        "            r[\"env_score\"] = float(np.clip((aq/100.0) * (1.0 - nz/100.0), 0.0, 1.0))\n",
        "        except Exception:\n",
        "            r[\"env_score\"] = None\n",
        "\n",
        "    # location normalize (passiamo la stringa) + fallback\n",
        "    if LOCATION in r and isinstance(r.get(LOCATION), str) and r[LOCATION].strip():\n",
        "        try:\n",
        "            _val = canonical_location(r[LOCATION])\n",
        "            if _val:  # evita di azzerare valori già buoni\n",
        "                r[LOCATION] = _val\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # se manca 'city', prova a derivarla da location\n",
        "    if not r.get(\"city\") and r.get(LOCATION):\n",
        "        try:\n",
        "            r[\"city\"] = str(r[LOCATION]).strip().title()\n",
        "        except Exception:\n",
        "            r[\"city\"] = None\n",
        "\n",
        "    # listing_month\n",
        "    if \"listing_month\" not in r or r.get(\"listing_month\") in (None, \"\", 0):\n",
        "        r[\"listing_month\"] = int(datetime.utcnow().month)\n",
        "\n",
        "    # is_top_floor\n",
        "    try:\n",
        "        if \"is_top_floor\" not in r and r.get(\"floor\") is not None and r.get(\"building_floors\") is not None:\n",
        "            r[\"is_top_floor\"] = int(r.get(\"floor\") == r.get(\"building_floors\"))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # default minimi e innocui (evitiamo 'view'/'heating' che scatenano errori)\n",
        "    r.setdefault(\"public_transport_nearby\", int(bool(r.get(\"public_transport_nearby\", 1))))\n",
        "    r.setdefault(\"garage\", int(bool(r.get(\"garage\", r.get(\"has_garage\", 0)))))\n",
        "\n",
        "    return r\n",
        "\n",
        "def validate_input_record(\n",
        "    record: Dict[str, Any],\n",
        "    all_expected: Optional[List[str]] = None,\n",
        "    *,\n",
        "    strict: bool = False,                 # compat, ignorato in LIGHT\n",
        "    drop_extras: bool = True,\n",
        "    allowed_features: Optional[List[str]] = None,\n",
        ") -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    LIGHT validation: alias + derivate sicure + normalizzazione location + filtro feature.\n",
        "    Nessuna chiamata a validator 'full' (niente stampe, niente side-effect).\n",
        "    \"\"\"\n",
        "    base = _autofill_safe(record)\n",
        "\n",
        "    expected = allowed_features or all_expected or list(globals().get(\"ALL_EXPECTED\", []))\n",
        "    allowed = set(expected) | _SAFE_DERIVED | {\"city\"}\n",
        "\n",
        "    if drop_extras:\n",
        "        for k in list(base.keys()):\n",
        "            if k not in allowed:\n",
        "                base.pop(k, None)\n",
        "\n",
        "    report = {\n",
        "        \"ok\": True,\n",
        "        \"errors\": [],\n",
        "        \"flags\": [\"light_validation\"],\n",
        "        \"normalized\": {\n",
        "            \"asset_type\": \"property\",\n",
        "            \"last_verified_ts\": get_utc_now().replace(microsecond=0).isoformat().replace(\"+00:00\",\"Z\"),\n",
        "        },\n",
        "    }\n",
        "    return base, report\n",
        "\n",
        "def detect_anomalies(record: Dict[str, Any]) -> Tuple[bool, Dict[str, Any]]:\n",
        "    \"\"\"Sempre non-bloccante in modalità LIGHT.\"\"\"\n",
        "    _, report = validate_input_record(record, strict=False)\n",
        "    return (False, report)\n",
        "\n",
        "_CELLA02_DONE = True\n",
        "print(\"CELLA 02 eseguita. (_CELLA02_DONE =\", _CELLA02_DONE, \")\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0341141f-3525-4d22-92f8-a68edc266946",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Sample + Predict (+CI) + Drift + Batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1d920b40-4628-4766-a1f0-231f3487d26d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Sample validated. asset_id=asset_infer_84534f2b  location=Milan\n",
            "ŷ_single = 635.51 k€  (±95.33 @ 80%)  [95.53 ms]\n",
            "\n",
            "Batch summary:\n",
            "            asset_id location  valuation_k   ci_low_k  ci_high_k\n",
            "     asset_batch_003 Florence   322.610753 274.219140 371.002366\n",
            "asset_infer_84534f2b    Milan   635.507347 540.181245 730.833449\n",
            "     asset_batch_002     Rome   361.777914 307.511227 416.044601\n",
            "     asset_batch_004    Turin   595.984870 506.587139 685.382600\n",
            "Note size=319 bytes | sha256=d0f2c6227180be57…\n"
          ]
        }
      ],
      "source": [
        "# R3) Sample → Single Predict → Batch (LIGHT validation + SAFE predict)\n",
        "from __future__ import annotations\n",
        "from uuid import uuid4\n",
        "from copy import deepcopy\n",
        "from typing import List, Optional, Dict, Any\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline  # usato in _predict_safe\n",
        "\n",
        "from shared.common.constants import ASSET_ID, LOCATION, SCHEMA_VERSION, NOTE_MAX_BYTES\n",
        "from shared.common.sanity_checks import leakage_gate\n",
        "from shared.common.utils import get_utc_now, sha256_hex, canonical_json_dumps\n",
        "\n",
        "# Reusa i cleaner della Cella 02 (_clean_missing_df è in globals)\n",
        "\n",
        "def _ensure_feature_frame(rec: Dict[str, Any], expected: List[str]) -> pd.DataFrame:\n",
        "    row = {k: rec.get(k, np.nan) for k in expected}\n",
        "    df = pd.DataFrame([row], columns=expected)\n",
        "    return _clean_missing_df(df)\n",
        "\n",
        "def _boolify_inplace(r: Dict[str, Any]) -> None:\n",
        "    candidates = [k for k in r.keys() if k.startswith(\"has_\")] + [\n",
        "        \"owner_occupied\", \"public_transport_nearby\",\n",
        "        \"garage\", \"parking_spot\", \"cellar\", \"attic\", \"concierge\",\n",
        "        \"is_top_floor\", \"is_ground_floor\", \"has_elevator\", \"has_balcony\", \"has_garden\",\n",
        "    ]\n",
        "    for k in candidates:\n",
        "        if k in r:\n",
        "            try:\n",
        "                r[k] = int(bool(r[k]))\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "def _simple_ci(y_hat_k: float) -> Dict[str, float]:\n",
        "    conf = 0.80\n",
        "    margin = float(max(5.0, 0.15 * float(y_hat_k)))\n",
        "    low = max(0.0, float(y_hat_k) - margin)\n",
        "    high = float(y_hat_k) + margin\n",
        "    return {\"confidence\": conf, \"ci_margin_k\": margin, \"confidence_low_k\": low, \"confidence_high_k\": high}\n",
        "\n",
        "def _predict_safe(pipeline_obj, X_df: pd.DataFrame) -> float:\n",
        "    \"\"\"Replica la robustezza dell'API: predict → last step → fallback senza far esplodere lo stack.\"\"\"\n",
        "    X_df = _clean_missing_df(X_df)\n",
        "    # 1) pipeline.predict\n",
        "    try:\n",
        "        y = pipeline_obj.predict(X_df)\n",
        "        return float(np.ravel(y)[0])\n",
        "    except RecursionError:\n",
        "        # 2) prova last-step estimator\n",
        "        try:\n",
        "            if isinstance(pipeline_obj, Pipeline):\n",
        "                last = pipeline_obj.steps[-1][1]\n",
        "                y = last.predict(X_df)\n",
        "                return float(np.ravel(y)[0])\n",
        "        except Exception:\n",
        "            pass\n",
        "        # 3) fallback minimale (evita crash)\n",
        "        s = X_df.get(\"size_m2\")\n",
        "        if s is not None and len(s) > 0 and pd.notna(s.iloc[0]):\n",
        "            return float(max(1.0, 0.8 * float(s.iloc[0])))\n",
        "        return 0.0\n",
        "\n",
        "def predict_asset(record: Dict[str, Any], *, asset_id: Optional[str] = None) -> Dict[str, Any]:\n",
        "    if \"ALL_EXPECTED\" not in globals():\n",
        "        raise RuntimeError(\"ALL_EXPECTED non definito (manca la cella 01).\")\n",
        "    if \"pipeline\" not in globals():\n",
        "        raise RuntimeError(\"pipeline non caricata (manca la cella 01).\")\n",
        "\n",
        "    # Validazione/normalizzazione input (LIGHT)\n",
        "    base, val_report = validate_input_record(\n",
        "        record,\n",
        "        allowed_features=ALL_EXPECTED,   # ora supportato\n",
        "        drop_extras=True,\n",
        "        strict=False,\n",
        "    )\n",
        "    _boolify_inplace(base)\n",
        "\n",
        "    aid = asset_id or base.get(ASSET_ID) or f\"asset_infer_{uuid4().hex[:8]}\"\n",
        "    base[ASSET_ID] = aid\n",
        "\n",
        "    X = _ensure_feature_frame(base, ALL_EXPECTED)\n",
        "\n",
        "    ok_leak, bad = leakage_gate(list(X.columns))\n",
        "    if not ok_leak and bad:\n",
        "        X = X.drop(columns=list(bad), errors=\"ignore\")\n",
        "\n",
        "    # --- SAFE predict (evita stack overflow)\n",
        "    y_hat = float(_predict_safe(pipeline, X))\n",
        "    if not np.isfinite(y_hat) or y_hat < 0:\n",
        "        y_hat = 0.0\n",
        "\n",
        "    ci = _simple_ci(y_hat)\n",
        "    out = {\n",
        "        \"schema_version\": SCHEMA_VERSION,\n",
        "        \"asset_type\": ASSET_TYPE,\n",
        "        \"asset_id\": aid,\n",
        "        \"timestamp\": get_utc_now().replace(microsecond=0).isoformat().replace(\"+00:00\", \"Z\"),\n",
        "        \"model_meta\": {\n",
        "            \"value_model_version\": MODEL_VERSION,\n",
        "            \"unit\": model_meta.get(\"unit\", \"k_eur\"),\n",
        "            \"feature_order_sha256\": model_meta.get(\"feature_order_sha256\"),\n",
        "            \"pipeline_sha256\": model_meta.get(\"pipeline_sha256\") or model_meta.get(\"model_hash\"),\n",
        "        },\n",
        "        \"input\": base,\n",
        "        \"metrics\": {\"valuation_k\": y_hat, **ci},\n",
        "        \"validation\": val_report,\n",
        "    }\n",
        "    return out\n",
        "\n",
        "# --- Sample singolo ----------------------------------------------------------\n",
        "sample_property_raw = {\n",
        "    \"location\": \"Milan\",\n",
        "    \"size_m2\": 120,\n",
        "    \"rooms\": 4,\n",
        "    \"bathrooms\": 2,\n",
        "    \"year_built\": 1999,\n",
        "    \"floor\": 2,\n",
        "    \"building_floors\": 6,\n",
        "    \"has_elevator\": 1,\n",
        "    \"has_garden\": 0,\n",
        "    \"has_balcony\": 1,\n",
        "    \"garage\": 1,               # canonico (poi alias → has_garage)\n",
        "    \"energy_class\": \"B\",\n",
        "    \"humidity_level\": 50.0,\n",
        "    \"temperature_avg\": 20.5,\n",
        "    \"noise_level\": 40,\n",
        "    \"air_quality_index\": 70,\n",
        "    \"owner_occupied\": 1,\n",
        "    \"public_transport_nearby\": 1,\n",
        "    \"distance_to_center_km\": 2.5,\n",
        "}\n",
        "\n",
        "sample_property, validation_report = validate_input_record(\n",
        "    sample_property_raw,\n",
        "    allowed_features=ALL_EXPECTED,\n",
        "    drop_extras=True,\n",
        ")\n",
        "\n",
        "for k in [k for k in sample_property if k.startswith(\"has_\")] + [\"owner_occupied\",\"public_transport_nearby\",\"garage\"]:\n",
        "    if k in sample_property:\n",
        "        sample_property[k] = int(bool(sample_property[k]))\n",
        "\n",
        "if not sample_property.get(ASSET_ID):\n",
        "    sample_property[ASSET_ID] = f\"asset_infer_{uuid4().hex[:8]}\"\n",
        "\n",
        "print(f\"✅ Sample validated. asset_id={sample_property.get(ASSET_ID)}  location={sample_property.get(LOCATION)}\")\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "single_output = predict_asset(sample_property, asset_id=sample_property.get(ASSET_ID))\n",
        "latency_ms_single = round((time.perf_counter() - t0) * 1000, 2)\n",
        "\n",
        "print(\n",
        "    f\"ŷ_single = {single_output['metrics']['valuation_k']:.2f} k€  \"\n",
        "    f\"(±{single_output['metrics']['ci_margin_k']:.2f} @ {int(single_output['metrics']['confidence']*100)}%)  \"\n",
        "    f\"[{latency_ms_single} ms]\"\n",
        ")\n",
        "\n",
        "# --- Batch -------------------------------------------------------------------\n",
        "batch_samples: List[dict] = [\n",
        "    deepcopy(sample_property),\n",
        "    {**sample_property, ASSET_ID: None, LOCATION: \"Rome\",     \"size_m2\":  90, \"energy_class\": \"C\"},\n",
        "    {**sample_property, ASSET_ID: None, LOCATION: \"Florence\", \"size_m2\":  70, \"has_garden\": 1, \"energy_class\": \"A\"},\n",
        "    {**sample_property, ASSET_ID: None, LOCATION: \"Turin\",    \"size_m2\": 150, \"energy_class\": \"D\"},\n",
        "]\n",
        "\n",
        "batch_outputs: List[dict] = []\n",
        "for i, raw in enumerate(batch_samples, start=1):\n",
        "    out = predict_asset(raw, asset_id=raw.get(ASSET_ID) or f\"asset_batch_{i:03}\")\n",
        "    batch_outputs.append(out)\n",
        "\n",
        "pd.DataFrame(\n",
        "    [{\"asset_id\": o[\"asset_id\"], \"location\": o[\"input\"].get(\"location\"), \"valuation_k\": o[\"metrics\"][\"valuation_k\"]}\n",
        "     for o in batch_outputs]\n",
        ")\n",
        "\n",
        "df_batch = pd.DataFrame(\n",
        "    [{\n",
        "        \"asset_id\": o[\"asset_id\"],\n",
        "        \"location\": o[\"input\"].get(\"location\"),\n",
        "        \"valuation_k\": o[\"metrics\"][\"valuation_k\"],\n",
        "        \"ci_low_k\": o[\"metrics\"][\"confidence_low_k\"],\n",
        "        \"ci_high_k\": o[\"metrics\"][\"confidence_high_k\"],\n",
        "    } for o in batch_outputs]\n",
        ").sort_values(\"location\")\n",
        "\n",
        "print(\"\\nBatch summary:\")\n",
        "print(df_batch.to_string(index=False))\n",
        "\n",
        "# --- Compact Note ------------------------------------------------------------\n",
        "def build_compact_note(out: dict) -> dict:\n",
        "    return {\n",
        "        \"schema_version\": \"v2\",\n",
        "        \"asset_id\": out[\"asset_id\"],\n",
        "        \"asset_type\": out[\"asset_type\"],\n",
        "        \"timestamp\": out[\"timestamp\"],\n",
        "        \"model\": {\n",
        "            \"version\": out[\"model_meta\"][\"value_model_version\"],\n",
        "            \"hash\": model_meta.get(\"pipeline_sha256\") or model_meta.get(\"model_hash\"),\n",
        "        },\n",
        "        \"metrics\": {\n",
        "            \"valuation_k\": out[\"metrics\"][\"valuation_k\"],\n",
        "            \"confidence\": out[\"metrics\"][\"confidence\"],\n",
        "            \"ci\": [out[\"metrics\"][\"confidence_low_k\"], out[\"metrics\"][\"confidence_high_k\"]],\n",
        "        },\n",
        "    }\n",
        "\n",
        "note = build_compact_note(single_output)\n",
        "note_bytes = canonical_json_dumps(note).encode(\"utf-8\")\n",
        "note_size = len(note_bytes)\n",
        "note_sha256 = sha256_hex(note_bytes)\n",
        "\n",
        "single_output.setdefault(\"publish\", {}).update({\n",
        "    \"status\": \"skipped\",\n",
        "    \"note_size\": note_size,\n",
        "    \"note_sha256\": note_sha256,\n",
        "    \"is_compacted\": True,\n",
        "    \"fallback_url_used\": False,\n",
        "})\n",
        "\n",
        "assert note_size <= NOTE_MAX_BYTES, f\"Nota troppo grande: {note_size} > {NOTE_MAX_BYTES}\"\n",
        "print(f\"Note size={note_size} bytes | sha256={note_sha256[:16]}…\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9b30533-6431-4327-8c9a-b781ecd1601f",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Logging JSONL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cb4b3446-a1ba-4b53-9cd2-83ae069e8b1f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appended 5 predictions → C:\\Users\\anven\\OneDrive\\Documenti\\GitHub\\axiomatic_oracle\\notebooks\\outputs\\logs\\predictions_log.jsonl\n",
            "Appended monitoring for 5 records → outputs\\logs\\monitoring_log.jsonl\n"
          ]
        }
      ],
      "source": [
        "# L1) JSONL Logging (atomic append) — predictions & monitoring\n",
        "from __future__ import annotations\n",
        "from pathlib import Path\n",
        "import os, json\n",
        "\n",
        "from shared.common.utils import canonical_json_dumps, get_utc_now\n",
        "\n",
        "MONITOR_LOG_PATH = Path(\"./outputs/logs/monitoring_log.jsonl\")\n",
        "LOG_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "MONITOR_LOG_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def append_jsonl(record: dict, path: Path) -> None:\n",
        "    \"\"\"Append atomico JSONL con timestamp UTC 'Z' + fsync (compat Windows).\"\"\"\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    payload = {**record, \"_logged_at\": get_utc_now().replace(microsecond=0).isoformat().replace(\"+00:00\", \"Z\")}\n",
        "    line = canonical_json_dumps(payload)\n",
        "    fd = os.open(str(path), os.O_WRONLY | os.O_CREAT | os.O_APPEND)\n",
        "    try:\n",
        "        with os.fdopen(fd, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(line + \"\\n\")\n",
        "            f.flush()\n",
        "            os.fsync(f.fileno())\n",
        "    except Exception:\n",
        "        try: os.close(fd)\n",
        "        except Exception: pass\n",
        "        raise\n",
        "\n",
        "def _to_monitoring(entry: dict) -> dict:\n",
        "    m  = entry.get(\"metrics\", {}) or {}\n",
        "    mm = entry.get(\"model_meta\", {}) or {}\n",
        "    return {\n",
        "        \"asset_id\": entry.get(\"asset_id\"),\n",
        "        \"model_version\": mm.get(\"value_model_version\", MODEL_VERSION),\n",
        "        \"model_class\": mm.get(\"value_model_name\"),\n",
        "        \"latency_ms\": m.get(\"latency_ms\"),\n",
        "        \"valuation_k\": m.get(\"valuation_k\") or m.get(\"valuation_base_k\"),\n",
        "        \"uncertainty_k\": m.get(\"uncertainty_k\") or m.get(\"uncertainty\"),\n",
        "        \"confidence_low_k\": m.get(\"confidence_low_k\"),\n",
        "        \"confidence_high_k\": m.get(\"confidence_high_k\"),\n",
        "        \"ci_method\": m.get(\"ci_method\"),\n",
        "        \"n_estimators\": m.get(\"n_estimators\"),\n",
        "        \"anomaly\": (entry.get(\"flags\") or {}).get(\"anomaly\"),\n",
        "        \"drift_detected\": (entry.get(\"flags\") or {}).get(\"drift_detected\"),\n",
        "    }\n",
        "\n",
        "# --- write predictions log ---\n",
        "n_batch = len(globals().get(\"batch_outputs\", []) or [])\n",
        "if \"single_output\" in globals():\n",
        "    append_jsonl(single_output, LOG_PATH)\n",
        "for o in (globals().get(\"batch_outputs\", []) or []):\n",
        "    append_jsonl(o, LOG_PATH)\n",
        "print(f\"Appended {int('single_output' in globals()) + n_batch} predictions → {LOG_PATH}\")\n",
        "\n",
        "# --- write monitoring log (derived) ---\n",
        "if \"single_output\" in globals():\n",
        "    append_jsonl(_to_monitoring(single_output), MONITOR_LOG_PATH)\n",
        "for o in (globals().get(\"batch_outputs\", []) or []):\n",
        "    append_jsonl(_to_monitoring(o), MONITOR_LOG_PATH)\n",
        "print(f\"Appended monitoring for {int('single_output' in globals()) + n_batch} records → {MONITOR_LOG_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b507a90c-a003-40e6-a97c-d4763895ba1c",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Sensitivity Check (vary size_m2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e1622a58-f3cd-47b3-8e5b-efe2cc2eadcd",
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sensitivity vs size_m2\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>size_m2</th>\n",
              "      <th>prediction_k</th>\n",
              "      <th>ci_low_k</th>\n",
              "      <th>ci_high_k</th>\n",
              "      <th>ci_margin_k</th>\n",
              "      <th>uncertainty_k</th>\n",
              "      <th>delta_vs_base_k</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>60</td>\n",
              "      <td>293.834095</td>\n",
              "      <td>249.758981</td>\n",
              "      <td>337.909209</td>\n",
              "      <td>44.075114</td>\n",
              "      <td>None</td>\n",
              "      <td>-341.673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>90</td>\n",
              "      <td>430.028203</td>\n",
              "      <td>365.523972</td>\n",
              "      <td>494.532433</td>\n",
              "      <td>64.504230</td>\n",
              "      <td>None</td>\n",
              "      <td>-205.479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>130</td>\n",
              "      <td>667.613635</td>\n",
              "      <td>567.471589</td>\n",
              "      <td>767.755680</td>\n",
              "      <td>100.142045</td>\n",
              "      <td>None</td>\n",
              "      <td>32.106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>170</td>\n",
              "      <td>897.896049</td>\n",
              "      <td>763.211642</td>\n",
              "      <td>1032.580457</td>\n",
              "      <td>134.684407</td>\n",
              "      <td>None</td>\n",
              "      <td>262.389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>210</td>\n",
              "      <td>972.688827</td>\n",
              "      <td>826.785503</td>\n",
              "      <td>1118.592151</td>\n",
              "      <td>145.903324</td>\n",
              "      <td>None</td>\n",
              "      <td>337.181</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   size_m2  prediction_k    ci_low_k    ci_high_k  ci_margin_k uncertainty_k  \\\n",
              "0       60    293.834095  249.758981   337.909209    44.075114          None   \n",
              "1       90    430.028203  365.523972   494.532433    64.504230          None   \n",
              "2      130    667.613635  567.471589   767.755680   100.142045          None   \n",
              "3      170    897.896049  763.211642  1032.580457   134.684407          None   \n",
              "4      210    972.688827  826.785503  1118.592151   145.903324          None   \n",
              "\n",
              "   delta_vs_base_k  \n",
              "0         -341.673  \n",
              "1         -205.479  \n",
              "2           32.106  \n",
              "3          262.389  \n",
              "4          337.181  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# L2) Sensitivity (“what-if”) su size_m2 — riusa predict_asset (no duplicazioni)\n",
        "import warnings\n",
        "import pandas as pd\n",
        "\n",
        "sizes = [60, 90, 130, 170, 210]\n",
        "rows  = []\n",
        "\n",
        "# baseline dal sample già validato (usa predict_asset per coerenza)\n",
        "_base = predict_asset(sample_property, asset_id=\"asset_sensitivity_base\")\n",
        "base_pred = float(_base[\"metrics\"][\"valuation_k\"])\n",
        "\n",
        "for s in sizes:\n",
        "    rec_raw = {**sample_property, \"size_m2\": s}\n",
        "    try:\n",
        "        out = predict_asset(rec_raw, asset_id=f\"asset_size_{s}\")\n",
        "        m = out[\"metrics\"]\n",
        "        # fallback robusti: se una chiave non c'è, usa None o un valore derivato\n",
        "        ci_margin = float(m.get(\"ci_margin_k\", max(5.0, 0.15 * float(m[\"valuation_k\"]))))\n",
        "        ci_low    = float(m.get(\"confidence_low_k\",  max(0.0, float(m[\"valuation_k\"]) - ci_margin)))\n",
        "        ci_high   = float(m.get(\"confidence_high_k\", float(m[\"valuation_k\"]) + ci_margin))\n",
        "        rows.append({\n",
        "            \"size_m2\": s,\n",
        "            \"prediction_k\": float(m[\"valuation_k\"]),\n",
        "            \"ci_low_k\": ci_low,\n",
        "            \"ci_high_k\": ci_high,\n",
        "            \"ci_margin_k\": ci_margin,\n",
        "            # 'uncertainty_k' può non esistere nella CI “light”: esponiamola se presente, altrimenti None\n",
        "            \"uncertainty_k\": m.get(\"uncertainty_k\"),\n",
        "            \"delta_vs_base_k\": round(float(m[\"valuation_k\"]) - base_pred, 3),\n",
        "        })\n",
        "    except Exception as e:\n",
        "        rows.append({\"size_m2\": s, \"prediction_k\": None, \"error\": str(e)})\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")\n",
        "\n",
        "df_sens = pd.DataFrame(rows)\n",
        "print(\"Sensitivity vs size_m2\")\n",
        "df_sens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c84f9f3e-aaad-4843-a735-1c388f17da55",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### API Checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "40996a99-ed49-473f-8ddb-74d5d36f932d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[API] ⚠️ Consistency check skipped: HTTPConnectionPool(host='127.0.0.1', port=8000): Max retries exceeded with url: /predict/property (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001BCF1D0FA90>: Failed to establish a new connection: [WinError 10061] Impossibile stabilire la connessione. Rifiuto persistente del computer di destinazione'))\n"
          ]
        }
      ],
      "source": [
        "# L3) API checks (consistency + optional publish) — unificata\n",
        "import os, json, time\n",
        "import requests\n",
        "from uuid import uuid4\n",
        "from shared.common.utils import NumpyJSONEncoder\n",
        "\n",
        "if COMPARE_WITH_API:\n",
        "    def _pick_pred_metrics(payload: dict):\n",
        "        \"\"\"Estrae predizione/CI da response v1/v2/flat.\"\"\"\n",
        "        if not isinstance(payload, dict):\n",
        "            return None, None, None, None\n",
        "        m = payload.get(\"metrics\", {}) if isinstance(payload.get(\"metrics\"), dict) else {}\n",
        "        pred = (m.get(\"valuation_k\") or m.get(\"valuation_base_k\")\n",
        "                or m.get(\"valuation\") or payload.get(\"valuation_k\")\n",
        "                or payload.get(\"prediction\"))\n",
        "        ci_low  = m.get(\"confidence_low_k\")  or payload.get(\"confidence_low_k\")\n",
        "        ci_high = m.get(\"confidence_high_k\") or payload.get(\"confidence_high_k\")\n",
        "        unc     = m.get(\"uncertainty_k\")     or m.get(\"uncertainty\") or payload.get(\"uncertainty_k\")\n",
        "        return pred, ci_low, ci_high, unc\n",
        "\n",
        "    def _model_version(payload: dict):\n",
        "        mm = (payload or {}).get(\"model_meta\", {}) if isinstance(payload, dict) else {}\n",
        "        return mm.get(\"value_model_version\") or mm.get(\"model_version\")\n",
        "\n",
        "    headers = {\"Content-Type\": \"application/json\", \"X-Idempotency-Key\": uuid4().hex}\n",
        "    token = os.getenv(\"AXM_TOKEN\")\n",
        "    if token:\n",
        "        headers[\"Authorization\"] = f\"Bearer {token}\"\n",
        "\n",
        "    url = f\"{API_BASE}/predict/{ASSET_TYPE}\"\n",
        "\n",
        "    # --- A) Consistency: confronta la singola locale vs API ---\n",
        "    try:\n",
        "        payload_json = json.loads(json.dumps(sample_property, cls=NumpyJSONEncoder, ensure_ascii=False))\n",
        "        t0 = time.perf_counter()\n",
        "        resp = requests.post(url, json=payload_json, headers=headers, timeout=10)\n",
        "        lat_ms = round((time.perf_counter() - t0) * 1000, 2)\n",
        "        if resp.ok:\n",
        "            api_json = resp.json()\n",
        "            api_pred, api_low, api_high, api_unc = _pick_pred_metrics(api_json)\n",
        "            if api_pred is None:\n",
        "                print(f\"[API] ❌ OK ma 'metrics.valuation_*' mancante.\")\n",
        "            else:\n",
        "                local_pred = float(single_output[\"metrics\"][\"valuation_k\"])\n",
        "                local_low  = float(single_output[\"metrics\"][\"confidence_low_k\"])\n",
        "                local_high = float(single_output[\"metrics\"][\"confidence_high_k\"])\n",
        "                delta = abs(float(api_pred) - local_pred)\n",
        "                pct   = (delta / max(1e-9, abs(local_pred))) * 100.0\n",
        "                ci_overlap = None\n",
        "                if api_low is not None and api_high is not None:\n",
        "                    try:\n",
        "                        ci_overlap = not (float(api_high) < local_low or float(api_low) > local_high)\n",
        "                    except Exception:\n",
        "                        ci_overlap = None\n",
        "                ver_note = \"\"\n",
        "                av, lv = _model_version(api_json), MODEL_VERSION\n",
        "                if av and av != lv:\n",
        "                    ver_note = f\" | ⚠️ model_version API={av} vs LOCAL={lv}\"\n",
        "                print(f\"[API] {lat_ms} ms | API={float(api_pred):.3f} k€ | LOCAL={local_pred:.3f} k€ | \"\n",
        "                      f\"Δ={delta:.4f} ({pct:.2f}%) | CI overlap: {ci_overlap if ci_overlap is not None else 'n/a'}{ver_note}\")\n",
        "        else:\n",
        "            print(f\"[API] ❌ {resp.status_code} | {resp.text[:200]}\")\n",
        "    except Exception as e:\n",
        "        print(f\"[API] ⚠️ Consistency check skipped: {e}\")\n",
        "\n",
        "    # --- B) Optional publish path (toggle via env PUBLISH=1) ---\n",
        "    PUBLISH = os.getenv(\"PUBLISH\", \"false\").lower() in {\"1\", \"true\", \"yes\", \"y\"}\n",
        "    if PUBLISH:\n",
        "        try:\n",
        "            payload_json = json.loads(json.dumps(sample_property, cls=NumpyJSONEncoder, ensure_ascii=False))\n",
        "            t0 = time.perf_counter()\n",
        "            resp = requests.post(url, params={\"publish\": \"true\"}, json=payload_json, headers=headers, timeout=15)\n",
        "            lat_ms = round((time.perf_counter() - t0) * 1000, 2)\n",
        "            if resp.ok:\n",
        "                api_json = resp.json()\n",
        "                api_pred, api_low, api_high, api_unc = _pick_pred_metrics(api_json)\n",
        "                print(f\"✅ API publish ok in {lat_ms} ms | pred={api_pred} k€ | unc={api_unc}\")\n",
        "            else:\n",
        "                print(f\"❌ API publish failed: {resp.status_code} | {resp.text[:200]}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ API publish exception: {e}\")\n",
        "else:\n",
        "    print(\"ℹ️ COMPARE_WITH_API disabled — skip API checks.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2eb46f5d-8713-442d-a46b-3da6b3f70278",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Artifact Audit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "703b2c48-e21a-441c-a574-e266a542cbb6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model SHA256: c0f7d4dbd41cd80282af4d4e4001ccd9b831db304e68d12cd9576ef98dcb5c65 (first16=c0f7d4dbd41cd802)\n",
            "Meta expects   : c0f7d4dbd41cd80282af4d4e4001ccd9b831db304e68d12cd9576ef98dcb5c65 (match: True)\n"
          ]
        }
      ],
      "source": [
        "# L4) Artifacts audit — file hash vs meta/manifest\n",
        "import hashlib, json\n",
        "from pathlib import Path\n",
        "\n",
        "def file_sha256(path: Path, chunk_size: int = 1 << 20) -> str:\n",
        "    h = hashlib.sha256()\n",
        "    with path.open(\"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(chunk_size), b\"\"): h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def manifest_expected_hash(manifest_path: Path) -> str | None:\n",
        "    if not manifest_path or not manifest_path.exists():\n",
        "        return None\n",
        "    try:\n",
        "        mf = json.loads(manifest_path.read_text(encoding=\"utf-8\"))\n",
        "        return (\n",
        "            (mf.get(\"paths\") or {}).get(\"pipeline_sha256\")\n",
        "            or (mf.get(\"model_meta\") or {}).get(\"model_hash\")\n",
        "            or (mf.get(\"artifacts\") or {}).get(\"pipeline_sha256\")\n",
        "            or (mf.get(\"artifacts\") or {}).get(\"model_sha256\")\n",
        "            or (mf.get(\"model\") or {}).get(\"sha256\")\n",
        "            or mf.get(\"pipeline_sha256\")\n",
        "        )\n",
        "    except Exception as e:\n",
        "        try: logger.info(\"Manifest unreadable for hash\", extra={\"error\": str(e)})\n",
        "        except Exception: pass\n",
        "        return None\n",
        "\n",
        "model_sha = file_sha256(PIPELINE_PATH)\n",
        "expected_sha_meta = (model_meta.get(\"model_hash\") or model_meta.get(\"pipeline_sha256\"))\n",
        "expected_sha_manifest = manifest_expected_hash(MANIFEST_PATH)\n",
        "\n",
        "print(f\"Model SHA256: {model_sha} (first16={model_sha[:16]})\")\n",
        "if expected_sha_meta:\n",
        "    print(f\"Meta expects   : {expected_sha_meta} (match: {expected_sha_meta == model_sha})\")\n",
        "if expected_sha_manifest:\n",
        "    ok = (expected_sha_manifest == model_sha)\n",
        "    print(f\"Manifest expects: {expected_sha_manifest} (match: {ok})\")\n",
        "    if not ok:\n",
        "        try:\n",
        "            logger.warning(\"Pipeline hash mismatch with manifest\",\n",
        "                           extra={\"expected\": expected_sha_manifest, \"actual\": model_sha})\n",
        "        except Exception:\n",
        "            pass"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
