name: CI/CD Pipeline

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  CI: "true"
  AWS_REGION: us-east-1
  ECR_REPO: 035754837020.dkr.ecr.us-east-1.amazonaws.com/ai-oracle-api
  IMAGE_TAG: latest
  DATA_PATH: data/df_cleaned_no_outliers.csv

jobs:
  generate-dataset:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install jupyter pandas pyarrow tqdm

      - name: Execute generate_dataset_01 notebook
        run: |
          jupyter nbconvert \
            --to notebook \
            --execute notebooks/generate_dataset_01.ipynb \
            --output executed_generate_dataset_01.ipynb \
            --ExecutePreprocessor.timeout=600

      - name: Check data output
        run: |
          ls -l data/
          test -f ${{env.DATA_PATH}}

      - name: Upload cleaned dataset
        uses: actions/upload-artifact@v4
        with:
          name: cleaned-dataset
          path: data/**

      - name: Upload data quality report
        uses: actions/upload-artifact@v4
        with:
          name: data-quality-report
          path: logs/quality_report.json

  train-and-evaluate:
    needs: generate-dataset
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Download cleaned dataset
        uses: actions/download-artifact@v4
        with:
          name: cleaned-dataset
          path: data

      - name: Download cleaned dataset
        uses: actions/download-artifact@v4
        with:
          name: cleaned-dataset
          path: data

      - name: Ensure expected CSV is present
        run: |
          echo "Listing dataset contents:"
          ls -lh data/
          test -f ${{env.DATA_PATH}}

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-cov shap

      - name: Convert notebook to script
        run: jupyter nbconvert --to script notebooks/train_model_03.ipynb

      - name: Train model & save outputs
        run: |
          python notebooks/train_model_03.py

      - name: Assert model quality & generate prediction intervals
        run: |
          python - <<'PYCODE'
          import json
          import pandas as pd
          from pathlib import Path
          from glob import glob

          # Cerca il metadata file partendo dalla root (models/)
          meta_files = glob("models/**/value_regressor_v2_meta.json", recursive=True)
          assert meta_files, "‚ùå Metadata file non trovato"

          meta_path = Path(meta_files[0])  # Usa il primo trovato (oppure scorri se ne hai pi√π)
          print(f"üìÑ Metadata file trovato: {meta_path}")

          with open(meta_path, encoding="utf-8") as f:
              meta = json.load(f)

          mae      = meta["metrics"]["mae_k"]
          coverage = meta["metrics"]["coverage_interval"]
          width    = meta["metrics"]["avg_interval_width_k"]
          r2       = meta["metrics"]["r2"]

          print(f"üìä MAE: {mae:.2f}k, R¬≤: {r2:.2f}, Coverage: {coverage:.2%}, Width: {width:.2f}k")

          assert mae < 100, "‚ùå MAE troppo alto"
          assert coverage >= 0.80, f"‚ùå Coverage troppo basso: {coverage:.2%}"
          print("‚úÖ Metriche accettabili")

          # Verifica se prediction_intervals.csv esiste
          pred_file = meta_path.parent / "prediction_intervals.csv"
          if not pred_file.exists():
              print("‚ö†Ô∏è prediction_intervals.csv non trovato nella cartella:", pred_file.parent)
          PYCODE

      - name: Upload model artifacts
        uses: actions/upload-artifact@v4
        with:
          name: model-artifacts
          path: |
            models/value_regressor_v2.joblib
            models/value_regressor_v2_meta.json
            models/prediction_intervals.csv

          name: Run unit tests & coverage
          run: |
            pytest --maxfail=1 --disable-warnings --cov=scripts --cov-fail-under=8

          name: Upload model artifacts
          uses: actions/upload-artifact@v4
          with:
            name: model-artifacts
            path: |
              models/final_model.pkl
              models/model_metadata.json
              models/prediction_intervals.csv
              models/monitoring_metadata.json
              models/shap_importance.csv

  build-and-push-image:
    needs: train-and-evaluate
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Log in to Amazon ECR
        uses: aws-actions/amazon-ecr-login@v1

      - name: Build and push Docker image
        run: |
          docker build -t $ECR_REPO:$IMAGE_TAG .
          docker push $ECR_REPO:$IMAGE_TAG

  deploy-to-prod:
    needs: build-and-push-image
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest

    steps:
      - name: Deploy to EC2 via SSH
        uses: appleboy/ssh-action@v1
        with:
          host: ${{ secrets.EC2_HOST }}
          username: ${{ secrets.EC2_USER }}
          key: ${{ secrets.EC2_SSH_KEY }}
          script: |
            aws ecr get-login-password --region ${{ env.AWS_REGION }} | docker login --username AWS --password-stdin ${{ env.ECR_REPO }}
            docker pull $ECR_REPO:$IMAGE_TAG
            docker stop ai-oracle-api || true
            docker rm ai-oracle-api || true
            docker run -d --name ai-oracle-api -p 80:8000 $ECR_REPO:$IMAGE_TAG
