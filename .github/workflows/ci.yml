name: CI/CD Pipeline

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  CI: "true"
  AWS_REGION: us-east-1
  ECR_REPO: 035754837020.dkr.ecr.us-east-1.amazonaws.com/ai-oracle-api
  IMAGE_TAG: latest

jobs:
  generate-dataset:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install jupyter pandas pyarrow tqdm
          pip install pytest pytest-cov shap

      - name: Execute generate_dataset_01 notebook
        run: |
          jupyter nbconvert \
            --to notebook \
            --execute notebooks/generate_dataset_01.ipynb \
            --output executed_generate_dataset_01.ipynb \
            --ExecutePreprocessor.timeout=600

      - name: Check data output
        run: |
          ls -l data/
          test -f data/df_cleaned_no_outliers.csv

      - name: Upload cleaned dataset
        uses: actions/upload-artifact@v4
        with:
          name: cleaned-dataset
          path: data/**

      - name: Upload data quality report
        uses: actions/upload-artifact@v4
        with:
          name: data-quality-report
          path: logs/quality_report.json

  train-and-evaluate:
    needs: generate-dataset
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Download cleaned dataset
        uses: actions/download-artifact@v4
        with:
          name: cleaned-dataset
          path: data

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Convert notebook to script
        run: jupyter nbconvert --to script notebooks/train_model_03.ipynb

      - name: Train model, generate intervals, monitoring & SHAP
        run: |
          python - <<'PYCODE'
          import json, joblib, logging
          from notebooks.train_model_03 import (
              final_model, X_train, X_test, y_train, y_test,
              mae, rmse, r2, coverage, avg_width,
              train_quantile_models, predict_with_intervals,
              generate_model_monitoring_metadata, generate_shap_explanations
          )
          logging.basicConfig(level=logging.INFO)

          assert mae < 100, f"MAE troppo alto: {mae}"
          assert coverage >= 0.85, f"Coverage interval troppo basso: {coverage:.2%}"

          joblib.dump(final_model, 'models/final_model.pkl')

          q_models = train_quantile_models(X_train, y_train, preprocessor=final_model.named_steps['preprocessor'], quantiles=[0.05,0.5,0.95])
          intervals = predict_with_intervals(q_models, X_test)
          intervals.to_csv('models/prediction_intervals.csv', index=False)

          monitoring = generate_model_monitoring_metadata(final_model, X_train, y_train, X_test, y_test)
          with open('models/monitoring_metadata.json','w') as f:
              json.dump(monitoring, f, indent=2)

          shap_imp = generate_shap_explanations(final_model, X_test, sample_size=100)
          shap_imp.to_csv('models/shap_importance.csv', index=False)

          metadata = {
            'mae': mae, 'rmse': rmse, 'r2': r2,
            'coverage': coverage, 'avg_interval_width': avg_width
          }
          with open('models/model_metadata.json','w') as f:
              json.dump(metadata, f, indent=2)

          logging.info("âœ… Training & evaluation complete")
          PYCODE

      - name: Run unit tests & coverage
        run: |
          pytest --maxfail=1 --disable-warnings --cov=scripts --cov-fail-under=80

      - name: Upload model artifacts
        uses: actions/upload-artifact@v4
        with:
          name: model-artifacts
          path: |
            models/final_model.pkl
            models/model_metadata.json
            models/prediction_intervals.csv
            models/monitoring_metadata.json
            models/shap_importance.csv

  build-and-push-image:
    needs: train-and-evaluate
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Log in to Amazon ECR
        uses: aws-actions/amazon-ecr-login@v1

      - name: Build and push Docker image
        run: |
          docker build -t $ECR_REPO:$IMAGE_TAG .
          docker push $ECR_REPO:$IMAGE_TAG

  deploy-to-prod:
    needs: build-and-push-image
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest

    steps:
      - name: Deploy to EC2 via SSH
        uses: appleboy/ssh-action@v1
        with:
          host: ${{ secrets.EC2_HOST }}
          username: ${{ secrets.EC2_USER }}
          key: ${{ secrets.EC2_SSH_KEY }}

      - name: Pull and restart container
        run: |
          docker login -u AWS -p $(aws ecr get-login-password --region ${{ env.AWS_REGION }}) ${{ env.ECR_REPO }}
          docker pull $ECR_REPO:$IMAGE_TAG
          docker stop oracle-api || true
          docker rm oracle-api || true
          docker run -d --name oracle-api -p 80:8000 $ECR_REPO:$IMAGE_TAG
